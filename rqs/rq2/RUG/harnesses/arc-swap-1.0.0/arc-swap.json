{"dependencies":{"<&'a Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<&'a T as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":[],"<*const T as as_raw::AsRaw<T>>::as_raw":[],"<*mut T as as_raw::AsRaw<T>>::as_raw":[],"<A as access::DynAccess<T>>::load":["access::DynGuard","std::alloc::Allocator","std::boxed::Box","std::marker::Sized"],"<ArcSwapAny<T, S> as access::Access<T>>::load":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<T, S> as std::convert::From<T>>::from":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<T, S> as std::default::Default>::default":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<T, S> as std::fmt::Debug>::fmt":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::result::Result","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<T, S> as std::fmt::Display>::fmt":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::result::Result","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<T, S> as std::ops::Drop>::drop":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<std::rc::Rc<T>, S> as access::Access<T>>::load":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<ArcSwapAny<std::sync::Arc<T>, S> as access::Access<T>>::load":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T, S> as std::convert::From<T>>::from":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T, S> as std::default::Default>::default":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T, S> as std::fmt::Debug>::fmt":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::fmt::Formatter","std::marker::Sized","std::option::Option","std::result::Result","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T, S> as std::fmt::Display>::fmt":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::fmt::Formatter","std::marker::Sized","std::option::Option","std::result::Result","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T, S> as std::ops::Deref>::deref":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<P as access::Access<T>>::load":[],"<access::Constant<T> as access::Access<T>>::load":["access::Constant","std::marker::Sized"],"<access::Constant<T> as std::clone::Clone>::clone":["access::Constant","std::marker::Sized"],"<access::Constant<T> as std::cmp::Eq>::assert_receiver_is_total_eq":["access::Constant","std::marker::Sized"],"<access::Constant<T> as std::cmp::Ord>::cmp":["access::Constant","std::cmp::Ordering","std::marker::Sized"],"<access::Constant<T> as std::cmp::PartialEq>::eq":["access::Constant","std::marker::Sized"],"<access::Constant<T> as std::cmp::PartialOrd>::partial_cmp":["access::Constant","std::marker::Sized","std::option::Option"],"<access::Constant<T> as std::fmt::Debug>::fmt":["access::Constant","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<access::Constant<T> as std::hash::Hash>::hash":["access::Constant","std::hash::Hasher","std::marker::Sized"],"<access::ConstantDeref<T> as std::clone::Clone>::clone":["access::ConstantDeref","std::marker::Sized"],"<access::ConstantDeref<T> as std::cmp::Eq>::assert_receiver_is_total_eq":["access::ConstantDeref","std::marker::Sized"],"<access::ConstantDeref<T> as std::cmp::Ord>::cmp":["access::ConstantDeref","std::cmp::Ordering","std::marker::Sized"],"<access::ConstantDeref<T> as std::cmp::PartialEq>::eq":["access::ConstantDeref","std::marker::Sized"],"<access::ConstantDeref<T> as std::cmp::PartialOrd>::partial_cmp":["access::ConstantDeref","std::marker::Sized","std::option::Option"],"<access::ConstantDeref<T> as std::fmt::Debug>::fmt":["access::ConstantDeref","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<access::ConstantDeref<T> as std::hash::Hash>::hash":["access::ConstantDeref","std::hash::Hasher","std::marker::Sized"],"<access::ConstantDeref<T> as std::ops::Deref>::deref":["access::ConstantDeref","std::marker::Sized"],"<access::DirectDeref<T, S> as std::fmt::Debug>::fmt":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","access::DirectDeref","ref_cnt::RefCnt","std::clone::Clone","std::fmt::Formatter","std::marker::Sized","std::option::Option","std::result::Result","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<access::DirectDeref<std::rc::Rc<T>, S> as std::ops::Deref>::deref":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","access::DirectDeref","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<access::DirectDeref<std::sync::Arc<T>, S> as std::ops::Deref>::deref":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","access::DirectDeref","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"<access::DynGuard<T> as std::ops::Deref>::deref":["access::DynGuard","std::alloc::Allocator","std::boxed::Box","std::marker::Sized"],"<access::Map<A, T, F> as access::Access<R>>::load":["access::Map","std::marker::PhantomData","std::marker::Sized"],"<access::Map<A, T, F> as std::clone::Clone>::clone":["access::Map","std::marker::PhantomData","std::marker::Sized"],"<access::Map<A, T, F> as std::fmt::Debug>::fmt":["access::Map","std::fmt::Formatter","std::marker::PhantomData","std::marker::Sized","std::result::Result"],"<access::MapGuard<G, T> as std::clone::Clone>::clone":["access::MapGuard","std::marker::Sized"],"<access::MapGuard<G, T> as std::fmt::Debug>::fmt":["access::MapGuard","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<access::MapGuard<G, T> as std::ops::Deref>::deref":["access::MapGuard","std::marker::Sized"],"<cache::Cache<A, T> as cache::Access<<T as std::ops::Deref>::Target>>::load":["cache::Cache","std::marker::Sized"],"<cache::Cache<A, T> as std::clone::Clone>::clone":["cache::Cache","std::marker::Sized"],"<cache::Cache<A, T> as std::convert::From<A>>::from":["cache::Cache","std::marker::Sized"],"<cache::Cache<A, T> as std::fmt::Debug>::fmt":["cache::Cache","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<cache::MapCache<A, T, F> as cache::Access<U>>::load":["cache::Cache","cache::MapCache","std::marker::Sized"],"<cache::MapCache<A, T, F> as std::clone::Clone>::clone":["cache::Cache","cache::MapCache","std::marker::Sized"],"<cache::MapCache<A, T, F> as std::fmt::Debug>::fmt":["cache::Cache","cache::MapCache","std::fmt::Formatter","std::marker::Sized","std::result::Result"],"<debt::Debt as std::default::Default>::default":["debt::Debt","std::sync::atomic::AtomicUsize"],"<debt::DebtHead as std::ops::Drop>::drop":["debt::DebtHead","std::cell::Cell"],"<debt::Node as std::default::Default>::default":["debt::Debt","debt::Node","debt::Slots","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicBool","std::sync::atomic::AtomicUsize"],"<debt::Slots as std::default::Default>::default":["debt::Debt","debt::Slots","std::sync::atomic::AtomicUsize"],"<gen_lock::GenLock<'_> as std::ops::Drop>::drop":["gen_lock::GenLock","std::sync::atomic::AtomicUsize"],"<gen_lock::Global as gen_lock::LockStorage>::choose_shard":["gen_lock::Global"],"<gen_lock::Global as gen_lock::LockStorage>::gen_idx":["gen_lock::Global","std::sync::atomic::AtomicUsize"],"<gen_lock::Global as gen_lock::LockStorage>::shards":["gen_lock::Global","gen_lock::Shard","std::sync::atomic::AtomicUsize"],"<gen_lock::Global as std::clone::Clone>::clone":["gen_lock::Global"],"<gen_lock::Global as std::default::Default>::default":["gen_lock::Global"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::choose_shard":["gen_lock::PrivateUnsharded","std::sync::atomic::AtomicUsize"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::gen_idx":["gen_lock::PrivateUnsharded","std::sync::atomic::AtomicUsize"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::shards":["gen_lock::PrivateUnsharded","std::sync::atomic::AtomicUsize"],"<gen_lock::PrivateUnsharded as std::default::Default>::default":["gen_lock::PrivateUnsharded","std::sync::atomic::AtomicUsize"],"<gen_lock::Shard as std::borrow::Borrow<[std::sync::atomic::AtomicUsize; _]>>::borrow":["gen_lock::Shard","std::sync::atomic::AtomicUsize"],"<gen_lock::Shard as std::default::Default>::default":["gen_lock::Shard","std::sync::atomic::AtomicUsize"],"<std::option::Option<T> as ref_cnt::RefCnt>::as_ptr":["std::marker::Sized","std::option::Option"],"<std::option::Option<T> as ref_cnt::RefCnt>::from_ptr":["std::marker::Sized","std::option::Option"],"<std::option::Option<T> as ref_cnt::RefCnt>::into_ptr":["std::marker::Sized","std::option::Option"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::as_ptr":["std::rc::Rc"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::from_ptr":["std::rc::Rc"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::into_ptr":["std::rc::Rc"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::as_ptr":["std::sync::Arc"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::from_ptr":["std::sync::Arc"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::into_ptr":["std::sync::Arc"],"<strategy::gen_lock::GenLockStrategy<L> as std::clone::Clone>::clone":["std::marker::Sized","strategy::gen_lock::GenLockStrategy"],"<strategy::gen_lock::GenLockStrategy<L> as std::default::Default>::default":["std::marker::Sized","strategy::gen_lock::GenLockStrategy"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::CaS<T>>::compare_and_swap":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::gen_lock::GenLockStrategy","strategy::sealed::InnerStrategy"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::load":["std::marker::Sized","std::sync::atomic::AtomicPtr","strategy::gen_lock::GenLockStrategy"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["std::marker::Sized","strategy::gen_lock::GenLockStrategy"],"<strategy::hybrid::HybridProtection<T> as std::borrow::Borrow<T>>::borrow":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"<strategy::hybrid::HybridProtection<T> as std::ops::Drop>::drop":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::from_inner":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::into_inner":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"<strategy::hybrid::HybridStrategy<F> as std::clone::Clone>::clone":["std::marker::Sized","strategy::hybrid::HybridStrategy"],"<strategy::hybrid::HybridStrategy<F> as std::default::Default>::default":["std::marker::Sized","strategy::hybrid::HybridStrategy"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::load":["std::marker::Sized","std::sync::atomic::AtomicPtr","strategy::hybrid::HybridStrategy"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["std::marker::Sized","strategy::hybrid::HybridStrategy"],"<strategy::hybrid::HybridStrategy<strategy::gen_lock::GenLockStrategy<L>> as strategy::sealed::CaS<T>>::compare_and_swap":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::hybrid::HybridStrategy","strategy::sealed::InnerStrategy"],"ArcSwapAny":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::compare_and_swap":["<S as strategy::CaS<T>>::S","<S as strategy::Strategy<T>>::S","ArcSwapAny","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::CaS","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::into_inner":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::load":["<S as strategy::Strategy<T>>::S","ArcSwapAny","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::load_full":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::map":["<P as access::Access<T>>::P","<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Access","access::Constant","access::Map","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::ops::Fn","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::new":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::default::Default","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::rcu":["<S as strategy::CaS<T>>::S","<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::convert::Into","std::marker::PhantomData","std::marker::Sized","std::ops::FnMut","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::CaS","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::store":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::swap":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<T, S>::with_strategy":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::empty":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::default::Default","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::from_pointee":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::convert::Into","std::default::Default","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"ArcSwapAny::<std::sync::Arc<T>, S>::from_pointee":["<S as strategy::Strategy<T>>::S","ArcSwapAny","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::default::Default","std::marker::PhantomData","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"Guard":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"Guard::<T, S>::from_inner":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"Guard::<T, S>::into_inner":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"access::Access::load":[],"access::Constant":["access::Constant","std::marker::Sized"],"access::ConstantDeref":["access::ConstantDeref","std::marker::Sized"],"access::DirectDeref":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","access::DirectDeref","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"access::DynAccess::load":["access::DynGuard","std::alloc::Allocator","std::boxed::Box","std::marker::Sized"],"access::DynGuard":["access::DynGuard","std::alloc::Allocator","std::boxed::Box","std::marker::Sized"],"access::Map":["access::Map","std::marker::PhantomData","std::marker::Sized"],"access::Map::<A, T, F>::new":["access::Map","std::marker::PhantomData","std::marker::Sized","std::ops::Fn"],"access::MapGuard":["access::MapGuard","std::marker::Sized"],"as_raw::AsRaw::as_raw":[],"cache::Access::load":[],"cache::Cache":["cache::Cache","std::marker::Sized"],"cache::Cache::<A, T>::arc_swap":["cache::Cache","std::marker::Sized"],"cache::Cache::<A, T>::load":["cache::Cache","std::marker::Sized"],"cache::Cache::<A, T>::load_no_revalidate":["cache::Cache","std::marker::Sized"],"cache::Cache::<A, T>::map":["cache::Cache","cache::MapCache","std::marker::Sized","std::ops::FnMut"],"cache::Cache::<A, T>::new":["cache::Cache","std::marker::Sized"],"cache::Cache::<A, T>::revalidate":["cache::Cache","std::marker::Sized"],"cache::MapCache":["cache::Cache","cache::MapCache","std::marker::Sized"],"debt::Debt":["debt::Debt","std::sync::atomic::AtomicUsize"],"debt::Debt::new":["std::marker::Sized","std::option::Option"],"debt::Debt::pay":["debt::Debt","ref_cnt::RefCnt","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicUsize"],"debt::Debt::pay_all":["ref_cnt::RefCnt","std::marker::Sized","std::option::Option"],"debt::DebtHead":["debt::DebtHead","std::cell::Cell"],"debt::Node":["debt::Debt","debt::Node","debt::Slots","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicBool","std::sync::atomic::AtomicUsize"],"debt::Node::get":["debt::Debt","debt::Node","debt::Slots","std::marker::Sized","std::option::Option","std::sync::atomic::AtomicBool","std::sync::atomic::AtomicUsize"],"debt::Slots":["debt::Debt","debt::Slots","std::sync::atomic::AtomicUsize"],"debt::THREAD_HEAD::__getit":["std::marker::Sized","std::option::Option"],"debt::THREAD_HEAD::__init":["debt::DebtHead","std::cell::Cell"],"debt::traverse":["std::marker::Sized","std::ops::FnMut","std::option::Option"],"gen_lock::GenLock":["gen_lock::GenLock","std::sync::atomic::AtomicUsize"],"gen_lock::GenLock::<'a>::new":["gen_lock::GenLock","gen_lock::Global","gen_lock::LockStorage","std::marker::Sized","std::sync::atomic::AtomicUsize"],"gen_lock::Global":["gen_lock::Global"],"gen_lock::LockStorage::choose_shard":[],"gen_lock::LockStorage::gen_idx":["std::sync::atomic::AtomicUsize"],"gen_lock::LockStorage::shards":[],"gen_lock::PrivateUnsharded":["gen_lock::PrivateUnsharded","std::sync::atomic::AtomicUsize"],"gen_lock::Shard":["gen_lock::Shard","std::sync::atomic::AtomicUsize"],"gen_lock::THREAD_SHARD::__getit":["std::marker::Sized","std::option::Option"],"gen_lock::THREAD_SHARD::__init":["std::cell::Cell"],"gen_lock::snapshot":["std::sync::atomic::AtomicUsize"],"gen_lock::wait_for_readers":["gen_lock::Global","gen_lock::LockStorage","std::marker::Sized"],"ptr_eq":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","strategy::Strategy","strategy::sealed::InnerStrategy"],"ref_cnt::RefCnt::as_ptr":[],"ref_cnt::RefCnt::dec":[],"ref_cnt::RefCnt::from_ptr":[],"ref_cnt::RefCnt::inc":[],"ref_cnt::RefCnt::into_ptr":[],"strategy::gen_lock::GenLockStrategy":["std::marker::Sized","strategy::gen_lock::GenLockStrategy"],"strategy::hybrid::HybridProtection":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"strategy::hybrid::HybridProtection::<T>::attempt":["std::marker::Sized","std::option::Option","std::sync::atomic::AtomicPtr"],"strategy::hybrid::HybridProtection::<T>::new":["access::Constant","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::mem::ManuallyDrop","std::option::Option","strategy::hybrid::HybridProtection"],"strategy::hybrid::HybridStrategy":["std::marker::Sized","strategy::hybrid::HybridStrategy"],"strategy::rw_lock::<impl strategy::sealed::CaS<T> for std::sync::RwLock<()>>::compare_and_swap":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::load":["std::marker::Sized","std::sync::RwLock","std::sync::atomic::AtomicPtr"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::wait_for_readers":["std::sync::RwLock"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::from_inner":[],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::into_inner":[],"strategy::sealed::CaS::compare_and_swap":["<S as strategy::Strategy<T>>::S","Guard","access::Constant","as_raw::AsRaw","ref_cnt::RefCnt","std::clone::Clone","std::marker::Sized","std::option::Option","std::sync::RwLock","std::sync::atomic::AtomicPtr","strategy::Strategy","strategy::sealed::InnerStrategy"],"strategy::sealed::InnerStrategy::load":["std::marker::Sized","std::sync::atomic::AtomicPtr"],"strategy::sealed::InnerStrategy::wait_for_readers":[],"strategy::sealed::Protected::from_inner":[],"strategy::sealed::Protected::into_inner":[]},"glob_path_import":{},"self_to_fn":{"<A as access::DynAccess<T>>::A":["impl<T, A> DynAccess<T> for A\nwhere\n    A: Access<T>,\n    A::Guard: 'static,\n{\n    fn load(&self) -> DynGuard<T> {\n        DynGuard(Box::new(Access::load(self)))\n    }\n}"],"<P as access::Access<T>>::P":["impl<T, A: Access<T>, P: Deref<Target = A>> Access<T> for P {\n    type Guard = A::Guard;\n    fn load(&self) -> Self::Guard {\n        self.deref().load()\n    }\n}"],"<S as strategy::CaS<T>>::S":["impl<T: RefCnt, S: sealed::CaS<T>> CaS<T> for S {}"],"<S as strategy::Strategy<T>>::S":["impl<T: RefCnt, S: sealed::InnerStrategy<T>> Strategy<T> for S {}"],"ArcSwapAny":["impl<T, S: Strategy<Arc<T>>> Access<T> for ArcSwapAny<Arc<T>, S> {\n    type Guard = DirectDeref<Arc<T>, S>;\n    fn load(&self) -> Self::Guard {\n        DirectDeref(self.load())\n    }\n}","impl<T, S: Strategy<Arc<T>>> ArcSwapAny<Arc<T>, S> {\n    /// A convenience constructor directly from the pointed-to value.\n    ///\n    /// Direct equivalent for `ArcSwap::new(Arc::new(val))`.\n    pub fn from_pointee(val: T) -> Self\n    where\n        S: Default,\n    {\n        Self::from(Arc::new(val))\n    }\n}","impl<T, S: Strategy<Option<Arc<T>>>> ArcSwapAny<Option<Arc<T>>, S> {\n    /// A convenience constructor directly from a pointed-to value.\n    ///\n    /// This just allocates the `Arc` under the hood.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use arc_swap::ArcSwapOption;\n    ///\n    /// let empty: ArcSwapOption<usize> = ArcSwapOption::from_pointee(None);\n    /// assert!(empty.load().is_none());\n    /// let non_empty: ArcSwapOption<usize> = ArcSwapOption::from_pointee(42);\n    /// assert_eq!(42, **non_empty.load().as_ref().unwrap());\n    /// ```\n    pub fn from_pointee<V: Into<Option<T>>>(val: V) -> Self\n    where\n        S: Default,\n    {\n        Self::new(val.into().map(Arc::new))\n    }\n\n    /// A convenience constructor for an empty value.\n    ///\n    /// This is equivalent to `ArcSwapOption::new(None)`.\n    pub fn empty() -> Self\n    where\n        S: Default,\n    {\n        Self::new(None)\n    }\n}","impl<T, S: Strategy<Rc<T>>> Access<T> for ArcSwapAny<Rc<T>, S> {\n    type Guard = DirectDeref<Rc<T>, S>;\n    fn load(&self) -> Self::Guard {\n        DirectDeref(self.load())\n    }\n}","impl<T, S: Strategy<T>> Debug for ArcSwapAny<T, S>\nwhere\n    T: Debug + RefCnt,\n{\n    fn fmt(&self, formatter: &mut Formatter) -> FmtResult {\n        formatter\n            .debug_tuple(\"ArcSwapAny\")\n            .field(&self.load())\n            .finish()\n    }\n}","impl<T, S: Strategy<T>> Display for ArcSwapAny<T, S>\nwhere\n    T: Display + RefCnt,\n{\n    fn fmt(&self, formatter: &mut Formatter) -> FmtResult {\n        self.load().fmt(formatter)\n    }\n}","impl<T: RefCnt + Default, S: Default + Strategy<T>> Default for ArcSwapAny<T, S> {\n    fn default() -> Self {\n        Self::new(T::default())\n    }\n}","impl<T: RefCnt, S: Default + Strategy<T>> From<T> for ArcSwapAny<T, S> {\n    fn from(val: T) -> Self {\n        Self::with_strategy(val, S::default())\n    }\n}","impl<T: RefCnt, S: Strategy<T>> Access<T> for ArcSwapAny<T, S> {\n    type Guard = Guard<T, S>;\n\n    fn load(&self) -> Self::Guard {\n        self.load()\n    }\n}","impl<T: RefCnt, S: Strategy<T>> ArcSwapAny<T, S> {\n    /// Constructs a new storage.\n    pub fn new(val: T) -> Self\n    where\n        S: Default,\n    {\n        Self::from(val)\n    }\n\n    /// Constructs a new storage while customizing the protection strategy.\n    pub fn with_strategy(val: T, strategy: S) -> Self {\n        // The AtomicPtr requires *mut in its interface. We are more like *const, so we cast it.\n        // However, we always go back to *const right away when we get the pointer on the other\n        // side, so it should be fine.\n        let ptr = T::into_ptr(val);\n        Self {\n            ptr: AtomicPtr::new(ptr),\n            _phantom_arc: PhantomData,\n            strategy,\n        }\n    }\n\n    /// Extracts the value inside.\n    pub fn into_inner(mut self) -> T {\n        let ptr = *self.ptr.get_mut();\n        // To pay all the debts\n        unsafe { self.strategy.wait_for_readers(ptr) };\n        mem::forget(self);\n        unsafe { T::from_ptr(ptr) }\n    }\n\n    /// Loads the value.\n    ///\n    /// This makes another copy of the held pointer and returns it, atomically (it is\n    /// safe even when other thread stores into the same instance at the same time).\n    ///\n    /// The method is lock-free and wait-free, but usually more expensive than\n    /// [`load`](#method.load).\n    pub fn load_full(&self) -> T {\n        Guard::into_inner(self.load())\n    }\n\n    /// Provides a temporary borrow of the object inside.\n    ///\n    /// This returns a proxy object allowing access to the thing held inside. However, there's\n    /// only limited amount of possible cheap proxies in existence for each thread ‒ if more are\n    /// created, it falls back to equivalent of [`load_full`](#method.load_full) internally.\n    ///\n    /// This is therefore a good choice to use for eg. searching a data structure or juggling the\n    /// pointers around a bit, but not as something to store in larger amounts. The rule of thumb\n    /// is this is suited for local variables on stack, but not in long-living data structures.\n    ///\n    /// # Consistency\n    ///\n    /// In case multiple related operations are to be done on the loaded value, it is generally\n    /// recommended to call `load` just once and keep the result over calling it multiple times.\n    /// First, keeping it is usually faster. But more importantly, the value can change between the\n    /// calls to load, returning different objects, which could lead to logical inconsistency.\n    /// Keeping the result makes sure the same object is used.\n    ///\n    /// ```rust\n    /// # use arc_swap::ArcSwap;\n    /// struct Point {\n    ///     x: usize,\n    ///     y: usize,\n    /// }\n    ///\n    /// fn print_broken(p: &ArcSwap<Point>) {\n    ///     // This is broken, because the x and y may come from different points,\n    ///     // combining into an invalid point that never existed.\n    ///     println!(\"X: {}\", p.load().x);\n    ///     // If someone changes the content now, between these two loads, we\n    ///     // have a problem\n    ///     println!(\"Y: {}\", p.load().y);\n    /// }\n    ///\n    /// fn print_correct(p: &ArcSwap<Point>) {\n    ///     // Here we take a snapshot of one specific point so both x and y come\n    ///     // from the same one.\n    ///     let point = p.load();\n    ///     println!(\"X: {}\", point.x);\n    ///     println!(\"Y: {}\", point.y);\n    /// }\n    /// # let p = ArcSwap::from_pointee(Point { x: 10, y: 20 });\n    /// # print_correct(&p);\n    /// # print_broken(&p);\n    /// ```\n    #[inline]\n    pub fn load(&self) -> Guard<T, S> {\n        let protected = unsafe { self.strategy.load(&self.ptr) };\n        Guard { inner: protected }\n    }\n\n    /// Replaces the value inside this instance.\n    ///\n    /// Further loads will yield the new value. Uses [`swap`](#method.swap) internally.\n    pub fn store(&self, val: T) {\n        drop(self.swap(val));\n    }\n\n    /// Exchanges the value inside this instance.\n    ///\n    /// Note that this method is *not* lock-free with the [`DefaultStrategy`].\n    pub fn swap(&self, new: T) -> T {\n        let new = T::into_ptr(new);\n        // AcqRel needed to publish the target of the new pointer and get the target of the old\n        // one.\n        //\n        // SeqCst to synchronize the time lines with the group counters.\n        let old = self.ptr.swap(new, Ordering::SeqCst);\n        unsafe {\n            self.strategy.wait_for_readers(old);\n            T::from_ptr(old)\n        }\n    }\n\n    /// Swaps the stored Arc if it equals to `current`.\n    ///\n    /// If the current value of the `ArcSwapAny` equals to `current`, the `new` is stored inside.\n    /// If not, nothing happens.\n    ///\n    /// The previous value (no matter if the swap happened or not) is returned. Therefore, if the\n    /// returned value is equal to `current`, the swap happened. You want to do a pointer-based\n    /// comparison to determine it.\n    ///\n    /// In other words, if the caller „guesses“ the value of current correctly, it acts like\n    /// [`swap`](#method.swap), otherwise it acts like [`load_full`](#method.load_full) (including\n    /// the limitations).\n    ///\n    /// The `current` can be specified as `&Arc`, [`Guard`](struct.Guard.html),\n    /// [`&Guards`](struct.Guards.html) or as a raw pointer.\n    pub fn compare_and_swap<C>(&self, current: C, new: T) -> Guard<T, S>\n    where\n        C: AsRaw<T::Base>,\n        S: CaS<T>,\n    {\n        let protected = unsafe { self.strategy.compare_and_swap(&self.ptr, current, new) };\n        Guard { inner: protected }\n        /*\n         */\n    }\n\n    /// Read-Copy-Update of the pointer inside.\n    ///\n    /// This is useful in read-heavy situations with several threads that sometimes update the data\n    /// pointed to. The readers can just repeatedly use [`load`](#method.load) without any locking.\n    /// The writer uses this method to perform the update.\n    ///\n    /// In case there's only one thread that does updates or in case the next version is\n    /// independent of the previous one, simple [`swap`](#method.swap) or [`store`](#method.store)\n    /// is enough. Otherwise, it may be needed to retry the update operation if some other thread\n    /// made an update in between. This is what this method does.\n    ///\n    /// # Examples\n    ///\n    /// This will *not* work as expected, because between loading and storing, some other thread\n    /// might have updated the value.\n    ///\n    /// ```rust\n    /// # use std::sync::Arc;\n    /// #\n    /// # use arc_swap::ArcSwap;\n    /// # use crossbeam_utils::thread;\n    /// #\n    /// let cnt = ArcSwap::from_pointee(0);\n    /// thread::scope(|scope| {\n    ///     for _ in 0..10 {\n    ///         scope.spawn(|_| {\n    ///            let inner = cnt.load_full();\n    ///             // Another thread might have stored some other number than what we have\n    ///             // between the load and store.\n    ///             cnt.store(Arc::new(*inner + 1));\n    ///         });\n    ///     }\n    /// }).unwrap();\n    /// // This will likely fail:\n    /// // assert_eq!(10, *cnt.load_full());\n    /// ```\n    ///\n    /// This will, but it can call the closure multiple times to retry:\n    ///\n    /// ```rust\n    /// # use arc_swap::ArcSwap;\n    /// # use crossbeam_utils::thread;\n    /// #\n    /// let cnt = ArcSwap::from_pointee(0);\n    /// thread::scope(|scope| {\n    ///     for _ in 0..10 {\n    ///         scope.spawn(|_| cnt.rcu(|inner| **inner + 1));\n    ///     }\n    /// }).unwrap();\n    /// assert_eq!(10, *cnt.load_full());\n    /// ```\n    ///\n    /// Due to the retries, you might want to perform all the expensive operations *before* the\n    /// rcu. As an example, if there's a cache of some computations as a map, and the map is cheap\n    /// to clone but the computations are not, you could do something like this:\n    ///\n    /// ```rust\n    /// # use std::collections::HashMap;\n    /// #\n    /// # use arc_swap::ArcSwap;\n    /// # use once_cell::sync::Lazy;\n    /// #\n    /// fn expensive_computation(x: usize) -> usize {\n    ///     x * 2 // Let's pretend multiplication is *really expensive expensive*\n    /// }\n    ///\n    /// type Cache = HashMap<usize, usize>;\n    ///\n    /// static CACHE: Lazy<ArcSwap<Cache>> = Lazy::new(|| ArcSwap::default());\n    ///\n    /// fn cached_computation(x: usize) -> usize {\n    ///     let cache = CACHE.load();\n    ///     if let Some(result) = cache.get(&x) {\n    ///         return *result;\n    ///     }\n    ///     // Not in cache. Compute and store.\n    ///     // The expensive computation goes outside, so it is not retried.\n    ///     let result = expensive_computation(x);\n    ///     CACHE.rcu(|cache| {\n    ///         // The cheaper clone of the cache can be retried if need be.\n    ///         let mut cache = HashMap::clone(&cache);\n    ///         cache.insert(x, result);\n    ///         cache\n    ///     });\n    ///     result\n    /// }\n    ///\n    /// assert_eq!(42, cached_computation(21));\n    /// assert_eq!(42, cached_computation(21));\n    /// ```\n    ///\n    /// # The cost of cloning\n    ///\n    /// Depending on the size of cache above, the cloning might not be as cheap. You can however\n    /// use persistent data structures ‒ each modification creates a new data structure, but it\n    /// shares most of the data with the old one (which is usually accomplished by using `Arc`s\n    /// inside to share the unchanged values). Something like\n    /// [`rpds`](https://crates.io/crates/rpds) or [`im`](https://crates.io/crates/im) might do\n    /// what you need.\n    pub fn rcu<R, F>(&self, mut f: F) -> T\n    where\n        F: FnMut(&T) -> R,\n        R: Into<T>,\n        S: CaS<T>,\n    {\n        let mut cur = self.load();\n        loop {\n            let new = f(&cur).into();\n            let prev = self.compare_and_swap(&*cur, new);\n            let swapped = ptr_eq(&*cur, &*prev);\n            if swapped {\n                return Guard::into_inner(prev);\n            } else {\n                cur = prev;\n            }\n        }\n    }\n\n    /// Provides an access to an up to date projection of the carried data.\n    ///\n    /// # Motivation\n    ///\n    /// Sometimes, an application consists of components. Each component has its own configuration\n    /// structure. The whole configuration contains all the smaller config parts.\n    ///\n    /// For the sake of separation and abstraction, it is not desirable to pass the whole\n    /// configuration to each of the components. This allows the component to take only access to\n    /// its own part.\n    ///\n    /// # Lifetimes & flexibility\n    ///\n    /// This method is not the most flexible way, as the returned type borrows into the `ArcSwap`.\n    /// To provide access into eg. `Arc<ArcSwap<T>>`, you can create the [`Map`] type directly. See\n    /// the [`access`] module.\n    ///\n    /// # Performance\n    ///\n    /// As the provided function is called on each load from the shared storage, it should\n    /// generally be cheap. It is expected this will usually be just referencing of a field inside\n    /// the structure.\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use std::sync::Arc;\n    ///\n    /// use arc_swap::ArcSwap;\n    /// use arc_swap::access::Access;\n    ///\n    /// struct Cfg {\n    ///     value: usize,\n    /// }\n    ///\n    /// fn print_many_times<V: Access<usize>>(value: V) {\n    ///     for _ in 0..25 {\n    ///         let value = value.load();\n    ///         println!(\"{}\", *value);\n    ///     }\n    /// }\n    ///\n    /// let shared = ArcSwap::from_pointee(Cfg { value: 0 });\n    /// let mapped = shared.map(|c: &Cfg| &c.value);\n    /// crossbeam_utils::thread::scope(|s| {\n    ///     // Will print some zeroes and some twos\n    ///     s.spawn(|_| print_many_times(mapped));\n    ///     s.spawn(|_| shared.store(Arc::new(Cfg { value: 2 })));\n    /// }).expect(\"Something panicked in a thread\");\n    /// ```\n    pub fn map<I, R, F>(&self, f: F) -> Map<&Self, I, F>\n    where\n        F: Fn(&I) -> &R,\n        Self: Access<I>,\n    {\n        Map::new(self, f)\n    }\n}","impl<T: RefCnt, S: Strategy<T>> Drop for ArcSwapAny<T, S> {\n    fn drop(&mut self) {\n        let ptr = *self.ptr.get_mut();\n        unsafe {\n            // To pay any possible debts\n            self.strategy.wait_for_readers(ptr);\n            // We are getting rid of the one stored ref count\n            T::dec(ptr);\n        }\n    }\n}"],"Guard":["impl<'a, T: RefCnt, S: Strategy<T>> Deref for Guard<T, S> {\n    type Target = T;\n    #[inline]\n    fn deref(&self) -> &T {\n        self.inner.borrow()\n    }\n}","impl<'a, T: RefCnt, S: Strategy<T>> Guard<T, S> {\n    /// Converts it into the held value.\n    ///\n    /// This, on occasion, may be a tiny bit faster than cloning the Arc or whatever is being held\n    /// inside.\n    // Associated function on purpose, because of deref\n    #[allow(clippy::wrong_self_convention)]\n    #[inline]\n    pub fn into_inner(lease: Self) -> T {\n        lease.inner.into_inner()\n    }\n\n    /// Create a guard for a given value `inner`.\n    ///\n    /// This can be useful on occasion to pass a specific object to code that expects or\n    /// wants to store a Guard.\n    ///\n    /// # Example\n    ///\n    /// ```rust\n    /// # use arc_swap::{ArcSwap, DefaultStrategy, Guard};\n    /// # use std::sync::Arc;\n    /// # let p = ArcSwap::from_pointee(42);\n    /// // Create two guards pointing to the same object\n    /// let g1 = p.load();\n    /// let g2 = Guard::<_, DefaultStrategy>::from_inner(Arc::clone(&*g1));\n    /// # drop(g2);\n    /// ```\n    pub fn from_inner(inner: T) -> Self {\n        Guard {\n            inner: S::Protected::from_inner(inner),\n        }\n    }\n}","impl<'a, T: RefCnt> AsRaw<T::Base> for Guard<T> {\n    fn as_raw(&self) -> *mut T::Base {\n        T::as_ptr(&self)\n    }\n}","impl<'a, T: RefCnt> Sealed for Guard<T> {}","impl<T: Debug + RefCnt, S: Strategy<T>> Debug for Guard<T, S> {\n    fn fmt(&self, formatter: &mut Formatter) -> FmtResult {\n        self.deref().fmt(formatter)\n    }\n}","impl<T: Default + RefCnt, S: Strategy<T>> Default for Guard<T, S> {\n    fn default() -> Self {\n        Self::from(T::default())\n    }\n}","impl<T: Display + RefCnt, S: Strategy<T>> Display for Guard<T, S> {\n    fn fmt(&self, formatter: &mut Formatter) -> FmtResult {\n        self.deref().fmt(formatter)\n    }\n}","impl<T: RefCnt, S: Strategy<T>> From<T> for Guard<T, S> {\n    fn from(inner: T) -> Self {\n        Self::from_inner(inner)\n    }\n}"],"access::Constant":["Clone","Copy","Debug","Eq","Hash","Ord","PartialEq","PartialOrd","impl<T: Clone> Access<T> for Constant<T> {\n    type Guard = ConstantDeref<T>;\n    fn load(&self) -> Self::Guard {\n        ConstantDeref(self.0.clone())\n    }\n}"],"access::ConstantDeref":["Clone","Copy","Debug","Eq","Hash","Ord","PartialEq","PartialOrd","impl<T> Deref for ConstantDeref<T> {\n    type Target = T;\n    fn deref(&self) -> &T {\n        &self.0\n    }\n}"],"access::DirectDeref":["Debug","impl<T, S: Strategy<Arc<T>>> Deref for DirectDeref<Arc<T>, S> {\n    type Target = T;\n    fn deref(&self) -> &T {\n        self.0.deref().deref()\n    }\n}","impl<T, S: Strategy<Rc<T>>> Deref for DirectDeref<Rc<T>, S> {\n    type Target = T;\n    fn deref(&self) -> &T {\n        self.0.deref().deref()\n    }\n}"],"access::DynGuard":["impl<T: ?Sized> Deref for DynGuard<T> {\n    type Target = T;\n    fn deref(&self) -> &T {\n        &self.0\n    }\n}"],"access::Map":["Clone","Copy","Debug","impl<A, T, F, R> Access<R> for Map<A, T, F>\nwhere\n    A: Access<T>,\n    F: Fn(&T) -> &R,\n{\n    type Guard = MapGuard<A::Guard, R>;\n    fn load(&self) -> Self::Guard {\n        let guard = self.access.load();\n        let value: *const _ = (self.projection)(&guard);\n        MapGuard {\n            _guard: guard,\n            value,\n        }\n    }\n}","impl<A, T, F> Map<A, T, F> {\n    /// Creates a new instance.\n    ///\n    /// # Parameters\n    ///\n    /// * `access`: Access to the bigger structure. This is usually something like `Arc<ArcSwap>`\n    ///   or `&ArcSwap`. It is technically possible to use any other [`Access`] here, though, for\n    ///   example to sub-delegate into even smaller structure from a [`Map`] (or generic\n    ///   [`Access`]).\n    /// * `projection`: A function (or closure) responsible to providing a reference into the\n    ///   bigger bigger structure, selecting just subset of it. In general, it is expected to be\n    ///   *cheap* (like only taking reference).\n    pub fn new<R>(access: A, projection: F) -> Self\n    where\n        F: Fn(&T) -> &R,\n    {\n        Map {\n            access,\n            projection,\n            _t: PhantomData,\n        }\n    }\n}"],"access::MapGuard":["Clone","Copy","Debug","impl<G, T> Deref for MapGuard<G, T> {\n    type Target = T;\n    fn deref(&self) -> &T {\n        // Why this is safe:\n        // * The pointer is originally converted from a reference. It's not null, it's aligned,\n        //   it's the right type, etc.\n        // * The pointee couldn't have gone away ‒ the guard keeps the original reference alive, so\n        //   must the new one still be alive too. Moving the guard is fine, we assume the RefCnt is\n        //   Pin (because it's Arc or Rc or something like that ‒ when that one moves, the data it\n        //   points to stay at the same place).\n        unsafe { &*self.value }\n    }\n}","unsafe impl<G, T> Send for MapGuard<G, T>\nwhere\n    G: Send,\n    for<'a> &'a T: Send,\n{\n}","unsafe impl<G, T> Sync for MapGuard<G, T>\nwhere\n    G: Sync,\n    for<'a> &'a T: Sync,\n{\n}"],"cache::Cache":["Clone","Debug","impl<A, T, S> Access<T::Target> for Cache<A, T>\nwhere\n    A: Deref<Target = ArcSwapAny<T, S>>,\n    T: Deref<Target = <T as RefCnt>::Base> + RefCnt,\n    S: Strategy<T>,\n{\n    fn load(&mut self) -> &T::Target {\n        self.load().deref()\n    }\n}","impl<A, T, S> Cache<A, T>\nwhere\n    A: Deref<Target = ArcSwapAny<T, S>>,\n    T: RefCnt,\n    S: Strategy<T>,\n{\n    /// Creates a new caching handle.\n    ///\n    /// The parameter is something dereferencing into an [`ArcSwapAny`] (eg. either to [`ArcSwap`]\n    /// or [`ArcSwapOption`]). That can be [`ArcSwapAny`] itself, but that's not very useful. But\n    /// it also can be a reference to it or `Arc`, which makes it possible to share the\n    /// [`ArcSwapAny`] with multiple caches or access it in non-cached way too.\n    ///\n    /// [`ArcSwapOption`]: crate::ArcSwapOption\n    /// [`ArcSwap`]: crate::ArcSwap\n    pub fn new(arc_swap: A) -> Self {\n        let cached = arc_swap.load_full();\n        Self { arc_swap, cached }\n    }\n\n    /// Gives access to the (possibly shared) cached [`ArcSwapAny`].\n    pub fn arc_swap(&self) -> &A::Target {\n        &self.arc_swap\n    }\n\n    /// Loads the currently held value.\n    ///\n    /// This first checks if the cached value is up to date. This check is very cheap.\n    ///\n    /// If it is up to date, the cached value is simply returned without additional costs. If it is\n    /// outdated, a load is done on the underlying shared storage. The newly loaded value is then\n    /// stored in the cache and returned.\n    #[inline]\n    pub fn load(&mut self) -> &T {\n        self.revalidate();\n        self.load_no_revalidate()\n    }\n\n    #[inline]\n    fn load_no_revalidate(&self) -> &T {\n        &self.cached\n    }\n\n    #[inline]\n    fn revalidate(&mut self) {\n        let cached_ptr = RefCnt::as_ptr(&self.cached);\n        // Node: Relaxed here is fine. We do not synchronize any data through this, we already have\n        // it synchronized in self.cache. We just want to check if it changed, if it did, the\n        // load_full will be responsible for any synchronization needed.\n        let shared_ptr = self.arc_swap.ptr.load(Ordering::Relaxed);\n        if cached_ptr != shared_ptr {\n            self.cached = self.arc_swap.load_full();\n        }\n    }\n\n    /// Turns this cache into a cache with a projection inside the cached value.\n    ///\n    /// You'd use this in case when some part of code needs access to fresh values of `U`, however\n    /// a bigger structure containing `U` is provided by this cache. The possibility of giving the\n    /// whole structure to the part of the code falls short in terms of reusability (the part of\n    /// the code could be used within multiple contexts, each with a bigger different structure\n    /// containing `U`) and code separation (the code shouldn't needs to know about the big\n    /// structure).\n    ///\n    /// # Warning\n    ///\n    /// As the provided `f` is called inside every [`load`][Access::load], this one should be\n    /// cheap. Most often it is expected to be just a closure taking reference of some inner field.\n    ///\n    /// For the same reasons, it should not have side effects and should never panic (these will\n    /// not break Rust's safety rules, but might produce behaviour you don't expect).\n    ///\n    /// # Examples\n    ///\n    /// ```rust\n    /// use arc_swap::ArcSwap;\n    /// use arc_swap::cache::{Access, Cache};\n    ///\n    /// struct InnerCfg {\n    ///     answer: usize,\n    /// }\n    ///\n    /// struct FullCfg {\n    ///     inner: InnerCfg,\n    /// }\n    ///\n    /// fn use_inner<A: Access<InnerCfg>>(cache: &mut A) {\n    ///     let value = cache.load();\n    ///     println!(\"The answer is: {}\", value.answer);\n    /// }\n    ///\n    /// let full_cfg = ArcSwap::from_pointee(FullCfg {\n    ///     inner: InnerCfg {\n    ///         answer: 42,\n    ///     }\n    /// });\n    /// let cache = Cache::new(&full_cfg);\n    /// use_inner(&mut cache.map(|full| &full.inner));\n    ///\n    /// let inner_cfg = ArcSwap::from_pointee(InnerCfg { answer: 24 });\n    /// let mut inner_cache = Cache::new(&inner_cfg);\n    /// use_inner(&mut inner_cache);\n    /// ```\n    pub fn map<F, U>(self, f: F) -> MapCache<A, T, F>\n    where\n        F: FnMut(&T) -> &U,\n    {\n        MapCache {\n            inner: self,\n            projection: f,\n        }\n    }\n}","impl<A, T, S> From<A> for Cache<A, T>\nwhere\n    A: Deref<Target = ArcSwapAny<T, S>>,\n    T: RefCnt,\n    S: Strategy<T>,\n{\n    fn from(arc_swap: A) -> Self {\n        Self::new(arc_swap)\n    }\n}"],"cache::MapCache":["Clone","Debug","impl<A, T, S, F, U> Access<U> for MapCache<A, T, F>\nwhere\n    A: Deref<Target = ArcSwapAny<T, S>>,\n    T: RefCnt,\n    S: Strategy<T>,\n    F: FnMut(&T) -> &U,\n{\n    fn load(&mut self) -> &U {\n        (self.projection)(self.inner.load())\n    }\n}"],"debt::Debt":["impl Debt {\n    /// Creates a new debt.\n    ///\n    /// This stores the debt of the given pointer (untyped, casted into an usize) and returns a\n    /// reference to that slot, or gives up with `None` if all the slots are currently full.\n    ///\n    /// This is technically lock-free on the first call in a given thread and wait-free on all the\n    /// other accesses.\n    #[allow(clippy::new_ret_no_self)]\n    #[inline]\n    pub(crate) fn new(ptr: usize) -> Option<&'static Self> {\n        THREAD_HEAD\n            .try_with(|head| {\n                let node = match head.node.get() {\n                    // Already have my own node (most likely)?\n                    Some(node) => node,\n                    // No node yet, called for the first time in this thread. Set one up.\n                    None => {\n                        let new_node = Node::get();\n                        head.node.set(Some(new_node));\n                        new_node\n                    }\n                };\n                // Check it is in use by *us*\n                debug_assert!(node.in_use.load(Ordering::Relaxed));\n                // Trick with offsets: we rotate through the slots (save the value from last time)\n                // so successive leases are likely to succeed on the first attempt (or soon after)\n                // instead of going through the list of already held ones.\n                let offset = head.offset.get();\n                let len = node.slots.0.len();\n                for i in 0..len {\n                    let i = (i + offset) % len;\n                    // Note: the indexing check is almost certainly optimised out because the len\n                    // is used above. And using .get_unchecked was actually *slower*.\n                    let got_it = node.slots.0[i]\n                        .0\n                        // Try to acquire the slot. Relaxed if it doesn't work is fine, as we don't\n                        // synchronize by it.\n                        .compare_exchange(NO_DEBT, ptr, Ordering::SeqCst, Ordering::Relaxed)\n                        .is_ok();\n                    if got_it {\n                        head.offset.set(i + 1);\n                        return Some(&node.slots.0[i]);\n                    }\n                }\n                None\n            })\n            .ok()\n            .and_then(|new| new)\n    }\n\n    /// Tries to pay the given debt.\n    ///\n    /// If the debt is still there, for the given pointer, it is paid and `true` is returned. If it\n    /// is empty or if there's some other pointer, it is not paid and `false` is returned, meaning\n    /// the debt was paid previously by someone else.\n    ///\n    /// # Notes\n    ///\n    /// * It is possible that someone paid the debt and then someone else put a debt for the same\n    ///   pointer in there. This is fine, as we'll just pay the debt for that someone else.\n    /// * This relies on the fact that the same pointer must point to the same object and\n    ///   specifically to the same type ‒ the caller provides the type, it's destructor, etc.\n    /// * It also relies on the fact the same thing is not stuffed both inside an `Arc` and `Rc` or\n    ///   something like that, but that sounds like a reasonable assumption. Someone storing it\n    ///   through `ArcSwap<T>` and someone else with `ArcSwapOption<T>` will work.\n    #[inline]\n    pub(crate) fn pay<T: RefCnt>(&self, ptr: *const T::Base) -> bool {\n        self.0\n            // If we don't change anything because there's something else, Relaxed is fine.\n            //\n            // The Release works as kind of Mutex. We make sure nothing from the debt-protected\n            // sections leaks below this point.\n            .compare_exchange(ptr as usize, NO_DEBT, Ordering::Release, Ordering::Relaxed)\n            .is_ok()\n    }\n\n    /// Pays all the debts on the given pointer.\n    pub(crate) fn pay_all<T: RefCnt>(ptr: *const T::Base) {\n        let val = unsafe { T::from_ptr(ptr) };\n        T::inc(&val);\n        traverse::<(), _>(|node| {\n            for slot in &node.slots.0 {\n                if slot\n                    .0\n                    .compare_exchange(ptr as usize, NO_DEBT, Ordering::AcqRel, Ordering::Relaxed)\n                    .is_ok()\n                {\n                    T::inc(&val);\n                }\n            }\n            None\n        });\n    }\n}","impl Default for Debt {\n    fn default() -> Self {\n        Debt(AtomicUsize::new(NO_DEBT))\n    }\n}"],"debt::DebtHead":["impl Drop for DebtHead {\n    fn drop(&mut self) {\n        if let Some(node) = self.node.get() {\n            // Nothing synchronized by this atomic.\n            assert!(node.in_use.swap(false, Ordering::Relaxed));\n        }\n    }\n}"],"debt::Node":["impl Default for Node {\n    fn default() -> Self {\n        Node {\n            next: None,\n            in_use: AtomicBool::new(true),\n            slots: Default::default(),\n        }\n    }\n}","impl Node {\n    fn get() -> &'static Self {\n        // Try to find an unused one in the chain and reuse it.\n        traverse(|node| {\n            // Try to claim this node. Nothing is synchronized through this atomic, we only\n            // track if someone claims ownership of it.\n            if !node.in_use.compare_and_swap(false, true, Ordering::Relaxed) {\n                Some(node)\n            } else {\n                None\n            }\n        })\n        // If that didn't work, create a new one and prepend to the list.\n        .unwrap_or_else(|| {\n            let node = Box::leak(Box::new(Node::default()));\n            // Not shared between threads yet, so ordinary write would be fine too.\n            node.in_use.store(true, Ordering::Relaxed);\n            // We don't want to read any data in addition to the head, Relaxed is fine\n            // here.\n            //\n            // We do need to release the data to others, but for that, we acquire in the\n            // compare_exchange below.\n            let mut head = DEBT_HEAD.load(Ordering::Relaxed);\n            loop {\n                node.next = unsafe { head.as_ref() };\n                if let Err(old) = DEBT_HEAD.compare_exchange_weak(\n                    head,\n                    node,\n                    // We need to release *the whole chain* here. For that, we need to\n                    // acquire it first.\n                    Ordering::AcqRel,\n                    Ordering::Relaxed, // Nothing changed, go next round of the loop.\n                ) {\n                    head = old;\n                } else {\n                    return node;\n                }\n            }\n        })\n    }\n}"],"debt::Slots":["Default"],"gen_lock::GenLock":["impl Drop for GenLock<'_> {\n    fn drop(&mut self) {\n        // Release, so the dangerous section stays in. Acquire to chain the operations.\n        // Do not drop the inner (maybe we should do into_raw for proper measures?)\n        self.slot.fetch_sub(1, Ordering::AcqRel);\n    }\n}","impl<'a> GenLock<'a> {\n    pub(crate) fn new<S: LockStorage + 'a>(storage: &'a S) -> Self {\n        let shard = storage.choose_shard();\n        let gen = storage.gen_idx().load(Ordering::Relaxed) % GEN_CNT;\n        // TODO: Is this still needed? Is the other SeqCst needed, in the writer? Is *there* any?\n        // Or should it be Release in there and SeqCst barrier as part of wait_for_readers?\n        // SeqCst: Acquire, so the dangerous section stays in. SeqCst to sync timelines with the\n        // swap on the ptr in writer thread.\n        let slot = &storage.shards().as_ref()[shard].borrow()[gen];\n        let old = slot.fetch_add(1, Ordering::SeqCst);\n        // The trick is taken from Arc.\n        if old > MAX_GUARDS {\n            process::abort();\n        }\n\n        Self { slot }\n    }\n}"],"gen_lock::Global":["Clone","Copy","Default","unsafe impl LockStorage for Global {\n    type Shard = Shard;\n    type Shards = Shards;\n\n    #[inline]\n    fn gen_idx(&self) -> &AtomicUsize {\n        &GEN_IDX\n    }\n\n    #[inline]\n    fn shards(&self) -> &Shards {\n        &SHARDS\n    }\n\n    #[inline]\n    fn choose_shard(&self) -> usize {\n        THREAD_SHARD\n            .try_with(|ts| {\n                let mut val = ts.get();\n                if val >= SHARD_CNT {\n                    val = THREAD_ID_GEN.fetch_add(1, Ordering::Relaxed) % SHARD_CNT;\n                    ts.set(val);\n                }\n                val\n            })\n            .unwrap_or(0)\n    }\n}"],"gen_lock::PrivateUnsharded":["Default","unsafe impl LockStorage for PrivateUnsharded {\n    type Shard = [AtomicUsize; GEN_CNT];\n    type Shards = [Self::Shard; 1];\n\n    #[inline]\n    fn gen_idx(&self) -> &AtomicUsize {\n        &self.gen_idx\n    }\n\n    #[inline]\n    fn shards(&self) -> &[Self::Shard; 1] {\n        &self.shard\n    }\n\n    #[inline]\n    fn choose_shard(&self) -> usize {\n        0\n    }\n}"],"gen_lock::Shard":["Default","impl Borrow<[AtomicUsize; GEN_CNT]> for Shard {\n    #[inline]\n    fn borrow(&self) -> &[AtomicUsize; GEN_CNT] {\n        &self.0\n    }\n}"],"std::option::Option":["unsafe impl<T: RefCnt> RefCnt for Option<T> {\n    type Base = T::Base;\n    fn into_ptr(me: Option<T>) -> *mut T::Base {\n        me.map(T::into_ptr).unwrap_or_else(ptr::null_mut)\n    }\n    fn as_ptr(me: &Option<T>) -> *mut T::Base {\n        me.as_ref().map(T::as_ptr).unwrap_or_else(ptr::null_mut)\n    }\n    unsafe fn from_ptr(ptr: *const T::Base) -> Option<T> {\n        if ptr.is_null() {\n            None\n        } else {\n            Some(T::from_ptr(ptr))\n        }\n    }\n}"],"std::rc::Rc":["unsafe impl<T> RefCnt for Rc<T> {\n    type Base = T;\n    fn into_ptr(me: Rc<T>) -> *mut T {\n        Rc::into_raw(me) as *mut T\n    }\n    fn as_ptr(me: &Rc<T>) -> *mut T {\n        me as &T as *const T as *mut T\n    }\n    unsafe fn from_ptr(ptr: *const T) -> Rc<T> {\n        Rc::from_raw(ptr)\n    }\n}"],"std::sync::Arc":["unsafe impl<T> RefCnt for Arc<T> {\n    type Base = T;\n    fn into_ptr(me: Arc<T>) -> *mut T {\n        Arc::into_raw(me) as *mut T\n    }\n    fn as_ptr(me: &Arc<T>) -> *mut T {\n        me as &T as *const T as *mut T\n    }\n    unsafe fn from_ptr(ptr: *const T) -> Arc<T> {\n        Arc::from_raw(ptr)\n    }\n}"],"std::sync::RwLock":["impl<T: RefCnt> CaS<T> for RwLock<()> {\n    unsafe fn compare_and_swap<C: AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected {\n        let _lock = self.write();\n        let cur = current.as_raw() as *mut T::Base;\n        let new = T::into_ptr(new);\n        let swapped = storage.compare_exchange(cur, new, Ordering::AcqRel, Ordering::Relaxed);\n        let old = match swapped {\n            Ok(old) => old,\n            Err(old) => old,\n        };\n        let old = T::from_ptr(old as *const T::Base);\n        if swapped.is_err() {\n            // If the new didn't go in, we need to destroy it and increment count in the old that\n            // we just duplicated\n            T::inc(&old);\n            drop(T::from_ptr(new));\n        }\n        drop(current);\n        old\n    }\n}","impl<T: RefCnt> InnerStrategy<T> for RwLock<()> {\n    type Protected = T;\n    unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> T {\n        let _guard = self.read().expect(\"We don't panic in here\");\n        let ptr = storage.load(Ordering::Acquire);\n        let ptr = T::from_ptr(ptr as *const T::Base);\n        T::inc(&ptr);\n\n        ptr\n    }\n\n    unsafe fn wait_for_readers(&self, _: *const T::Base) {\n        // By acquiring the write lock, we make sure there are no read locks present across it.\n        drop(self.write().expect(\"We don't panic in here\"));\n    }\n}"],"strategy::gen_lock::GenLockStrategy":["Clone","Copy","Default","impl<T: RefCnt, L: LockStorage> CaS<T> for GenLockStrategy<L> {\n    unsafe fn compare_and_swap<C: crate::as_raw::AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected {\n        let cur_ptr = current.as_raw();\n        let new = T::into_ptr(new);\n\n        // As noted above, this method has either semantics of load or of store. We don't know\n        // which ones upfront, so we need to implement safety measures for both.\n        let lock = GenLock::new(&self.0);\n\n        let previous_ptr = storage.compare_and_swap(cur_ptr, new, Ordering::SeqCst);\n        let swapped = ptr::eq(cur_ptr, previous_ptr);\n\n        // Drop it here, because:\n        // * We can't drop it before the compare_and_swap ‒ in such case, it could get recycled,\n        //   put into the pointer by another thread with a different value and create a fake\n        //   success (ABA).\n        // * We drop it before waiting for readers, because it could have been a Guard with a\n        //   generation lock. In such case, the caller doesn't have it any more and can't check if\n        //   it succeeded, but that's OK.\n        drop(current);\n\n        let previous = T::from_ptr(previous_ptr);\n\n        if swapped {\n            drop(lock);\n            gen_lock::wait_for_readers(&self.0);\n        } else {\n            // We didn't swap. Therefore, we need to bump the count on the old one and release the\n            // new one (blackhole it).\n            T::inc(&previous);\n            T::dec(new);\n        }\n\n        previous\n    }\n}","impl<T: RefCnt, L: LockStorage> InnerStrategy<T> for GenLockStrategy<L> {\n    type Protected = T;\n\n    unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> Self::Protected {\n        let lock = GenLock::new(&self.0);\n\n        let ptr = storage.load(Ordering::Acquire);\n        let result = T::from_ptr(ptr);\n        T::inc(&result);\n\n        drop(lock);\n\n        result\n    }\n\n    unsafe fn wait_for_readers(&self, _: *const T::Base) {\n        gen_lock::wait_for_readers(&self.0);\n    }\n}"],"strategy::hybrid::HybridProtection":["impl<T: RefCnt> Borrow<T> for HybridProtection<T> {\n    #[inline]\n    fn borrow(&self) -> &T {\n        &self.ptr\n    }\n}","impl<T: RefCnt> Drop for HybridProtection<T> {\n    #[inline]\n    fn drop(&mut self) {\n        match self.debt.take() {\n            // We have our own copy of Arc, so we don't need a protection. Do nothing (but release\n            // the Arc below).\n            None => (),\n            // If we owed something, just return the debt. We don't have a pointer owned, so\n            // nothing to release.\n            Some(debt) => {\n                let ptr = T::as_ptr(&self.ptr);\n                if debt.pay::<T>(ptr) {\n                    return;\n                }\n                // But if the debt was already paid for us, we need to release the pointer, as we\n                // were effectively already in the Unprotected mode.\n            }\n        }\n        // Equivalent to T::dec(ptr)\n        unsafe { ManuallyDrop::drop(&mut self.ptr) };\n    }\n}","impl<T: RefCnt> HybridProtection<T> {\n    #[inline]\n    unsafe fn new(ptr: *const T::Base, debt: Option<&'static Debt>) -> Self {\n        Self {\n            debt,\n            ptr: ManuallyDrop::new(T::from_ptr(ptr)),\n        }\n    }\n    #[inline]\n    fn attempt(storage: &AtomicPtr<T::Base>) -> Option<Self> {\n        // Relaxed is good enough here, see the Acquire below\n        let ptr = storage.load(Ordering::Relaxed);\n        // Try to get a debt slot. If not possible, fail.\n        let debt = Debt::new(ptr as usize)?;\n\n        let confirm = storage.load(Ordering::Acquire);\n        if ptr == confirm {\n            // Successfully got a debt\n            Some(unsafe { Self::new(ptr, Some(debt)) })\n        } else if debt.pay::<T>(ptr) {\n            // It changed in the meantime, we return the debt (that is on the outdated pointer,\n            // possibly destroyed) and fail.\n            None\n        } else {\n            // It changed in the meantime, but the debt for the previous pointer was already paid\n            // for by someone else, so we are fine using it.\n            Some(unsafe { Self::new(ptr, None) })\n        }\n    }\n}","impl<T: RefCnt> Protected<T> for HybridProtection<T> {\n    #[inline]\n    fn from_inner(ptr: T) -> Self {\n        Self {\n            debt: None,\n            ptr: ManuallyDrop::new(ptr),\n        }\n    }\n\n    #[inline]\n    fn into_inner(mut self) -> T {\n        // Drop any debt and release any lock held by the given guard and return a\n        // full-featured value that even can outlive the ArcSwap it originated from.\n        match self.debt.take() {\n            None => (), // We have a fully loaded ref-counted pointer.\n            Some(debt) => {\n                let ptr = T::inc(&self.ptr);\n                if !debt.pay::<T>(ptr) {\n                    unsafe { T::dec(ptr) };\n                }\n            }\n        }\n\n        // The ptr::read & forget is something like a cheating move. We can't move it out, because\n        // we have a destructor and Rust doesn't allow us to do that.\n        let inner = unsafe { ptr::read(self.ptr.deref()) };\n        mem::forget(self);\n        inner\n    }\n}"],"strategy::hybrid::HybridStrategy":["Clone","Default","impl<T, F> InnerStrategy<T> for HybridStrategy<F>\nwhere\n    T: RefCnt,\n    F: InnerStrategy<T, Protected = T>,\n{\n    type Protected = HybridProtection<T>;\n    unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> Self::Protected {\n        HybridProtection::attempt(storage).unwrap_or_else(|| {\n            let loaded = self.fallback.load(storage);\n            HybridProtection {\n                debt: None,\n                ptr: ManuallyDrop::new(loaded),\n            }\n        })\n    }\n    unsafe fn wait_for_readers(&self, old: *const T::Base) {\n        self.fallback.wait_for_readers(old);\n        Debt::pay_all::<T>(old);\n    }\n}","impl<T: RefCnt, L: LockStorage> CaS<T> for HybridStrategy<GenLockStrategy<L>> {\n    unsafe fn compare_and_swap<C: crate::as_raw::AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected {\n        let cur_ptr = current.as_raw();\n        let new = T::into_ptr(new);\n\n        // As noted above, this method has either semantics of load or of store. We don't know\n        // which ones upfront, so we need to implement safety measures for both.\n        let gen = GenLock::new(&self.fallback.0);\n\n        let previous_ptr = storage.compare_and_swap(cur_ptr, new, Ordering::SeqCst);\n        let swapped = ptr::eq(cur_ptr, previous_ptr);\n\n        // Drop it here, because:\n        // * We can't drop it before the compare_and_swap ‒ in such case, it could get recycled,\n        //   put into the pointer by another thread with a different value and create a fake\n        //   success (ABA).\n        // * We drop it before waiting for readers, because it could have been a Guard with a\n        //   generation lock. In such case, the caller doesn't have it any more and can't check if\n        //   it succeeded, but that's OK.\n        drop(current);\n\n        let debt = if swapped {\n            // New went in, previous out, but their ref counts are correct. So nothing to do here.\n            None\n        } else {\n            // Previous is a new copy of what is inside (and it stays there as well), so bump its\n            // ref count. New is thrown away so dec its ref count (but do it outside of the\n            // gen-lock).\n            //\n            // We try to do that by registering a debt and only if that fails by actually bumping\n            // the ref.\n            let debt = Debt::new(previous_ptr as usize);\n            if debt.is_none() {\n                let previous = T::from_ptr(previous_ptr);\n                T::inc(&previous);\n                T::into_ptr(previous);\n            }\n            debt\n        };\n\n        drop(gen);\n\n        if swapped {\n            // We swapped. Before releasing the (possibly only) ref count of previous to user, wait\n            // for all readers to make sure there are no more untracked copies of it.\n            //\n            // Why is rustc confused about self.wait_for_readers???\n            InnerStrategy::<T>::wait_for_readers(self, previous_ptr);\n        } else {\n            // We didn't swap, so new is black-holed.\n            T::dec(new);\n        }\n\n        HybridProtection::new(previous_ptr, debt)\n    }\n}"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::T":["impl<T: RefCnt> Protected<T> for T {\n    #[inline]\n    fn from_inner(ptr: T) -> Self {\n        ptr\n    }\n\n    #[inline]\n    fn into_inner(self) -> T {\n        self\n    }\n}"]},"single_path_import":{"as_raw::AsRaw":"AsRaw","cache::Cache":"Cache","ref_cnt::RefCnt":"RefCnt","strategy::DefaultStrategy":"DefaultStrategy","strategy::IndependentStrategy":"IndependentStrategy"},"srcs":{"<&'a Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["fn as_raw(&self) -> *mut T::Base{\n        T::as_ptr(&self)\n    }","Real(LocalPath(\"src/as_raw.rs\"))"],"<&'a T as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["fn as_raw(&self) -> *mut T::Base{\n        T::as_ptr(self)\n    }","Real(LocalPath(\"src/as_raw.rs\"))"],"<*const T as as_raw::AsRaw<T>>::as_raw":["fn as_raw(&self) -> *mut T{\n        *self as *mut T\n    }","Real(LocalPath(\"src/as_raw.rs\"))"],"<*mut T as as_raw::AsRaw<T>>::as_raw":["fn as_raw(&self) -> *mut T{\n        *self\n    }","Real(LocalPath(\"src/as_raw.rs\"))"],"<A as access::DynAccess<T>>::load":["fn load(&self) -> DynGuard<T>{\n        DynGuard(Box::new(Access::load(self)))\n    }","Real(LocalPath(\"src/access.rs\"))"],"<ArcSwapAny<T, S> as access::Access<T>>::load":["fn load(&self) -> Self::Guard{\n        self.load()\n    }","Real(LocalPath(\"src/access.rs\"))"],"<ArcSwapAny<T, S> as std::convert::From<T>>::from":["fn from(val: T) -> Self{\n        Self::with_strategy(val, S::default())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ArcSwapAny<T, S> as std::default::Default>::default":["fn default() -> Self{\n        Self::new(T::default())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ArcSwapAny<T, S> as std::fmt::Debug>::fmt":["fn fmt(&self, formatter: &mut Formatter) -> FmtResult{\n        formatter\n            .debug_tuple(\"ArcSwapAny\")\n            .field(&self.load())\n            .finish()\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ArcSwapAny<T, S> as std::fmt::Display>::fmt":["fn fmt(&self, formatter: &mut Formatter) -> FmtResult{\n        self.load().fmt(formatter)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ArcSwapAny<T, S> as std::ops::Drop>::drop":["fn drop(&mut self){\n        let ptr = *self.ptr.get_mut();\n        unsafe {\n            // To pay any possible debts\n            self.strategy.wait_for_readers(ptr);\n            // We are getting rid of the one stored ref count\n            T::dec(ptr);\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ArcSwapAny<std::rc::Rc<T>, S> as access::Access<T>>::load":["fn load(&self) -> Self::Guard{\n        DirectDeref(self.load())\n    }","Real(LocalPath(\"src/access.rs\"))"],"<ArcSwapAny<std::sync::Arc<T>, S> as access::Access<T>>::load":["fn load(&self) -> Self::Guard{\n        DirectDeref(self.load())\n    }","Real(LocalPath(\"src/access.rs\"))"],"<Guard<T, S> as std::convert::From<T>>::from":["fn from(inner: T) -> Self{\n        Self::from_inner(inner)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Guard<T, S> as std::default::Default>::default":["fn default() -> Self{\n        Self::from(T::default())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Guard<T, S> as std::fmt::Debug>::fmt":["fn fmt(&self, formatter: &mut Formatter) -> FmtResult{\n        self.deref().fmt(formatter)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Guard<T, S> as std::fmt::Display>::fmt":["fn fmt(&self, formatter: &mut Formatter) -> FmtResult{\n        self.deref().fmt(formatter)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Guard<T, S> as std::ops::Deref>::deref":["#[inline]\nfn deref(&self) -> &T{\n        self.inner.borrow()\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["fn as_raw(&self) -> *mut T::Base{\n        T::as_ptr(&self)\n    }","Real(LocalPath(\"src/as_raw.rs\"))"],"<P as access::Access<T>>::load":["fn load(&self) -> Self::Guard{\n        self.deref().load()\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::Constant<T> as access::Access<T>>::load":["fn load(&self) -> Self::Guard{\n        ConstantDeref(self.0.clone())\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::ConstantDeref<T> as std::ops::Deref>::deref":["fn deref(&self) -> &T{\n        &self.0\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::DirectDeref<std::rc::Rc<T>, S> as std::ops::Deref>::deref":["fn deref(&self) -> &T{\n        self.0.deref().deref()\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::DirectDeref<std::sync::Arc<T>, S> as std::ops::Deref>::deref":["fn deref(&self) -> &T{\n        self.0.deref().deref()\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::DynGuard<T> as std::ops::Deref>::deref":["fn deref(&self) -> &T{\n        &self.0\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::Map<A, T, F> as access::Access<R>>::load":["fn load(&self) -> Self::Guard{\n        let guard = self.access.load();\n        let value: *const _ = (self.projection)(&guard);\n        MapGuard {\n            _guard: guard,\n            value,\n        }\n    }","Real(LocalPath(\"src/access.rs\"))"],"<access::MapGuard<G, T> as std::ops::Deref>::deref":["fn deref(&self) -> &T{\n        // Why this is safe:\n        // * The pointer is originally converted from a reference. It's not null, it's aligned,\n        //   it's the right type, etc.\n        // * The pointee couldn't have gone away ‒ the guard keeps the original reference alive, so\n        //   must the new one still be alive too. Moving the guard is fine, we assume the RefCnt is\n        //   Pin (because it's Arc or Rc or something like that ‒ when that one moves, the data it\n        //   points to stay at the same place).\n        unsafe { &*self.value }\n    }","Real(LocalPath(\"src/access.rs\"))"],"<cache::Cache<A, T> as cache::Access<<T as std::ops::Deref>::Target>>::load":["fn load(&mut self) -> &T::Target{\n        self.load().deref()\n    }","Real(LocalPath(\"src/cache.rs\"))"],"<cache::Cache<A, T> as std::convert::From<A>>::from":["fn from(arc_swap: A) -> Self{\n        Self::new(arc_swap)\n    }","Real(LocalPath(\"src/cache.rs\"))"],"<cache::MapCache<A, T, F> as cache::Access<U>>::load":["fn load(&mut self) -> &U{\n        (self.projection)(self.inner.load())\n    }","Real(LocalPath(\"src/cache.rs\"))"],"<debt::Debt as std::default::Default>::default":["fn default() -> Self{\n        Debt(AtomicUsize::new(NO_DEBT))\n    }","Real(LocalPath(\"src/debt.rs\"))"],"<debt::DebtHead as std::ops::Drop>::drop":["fn drop(&mut self){\n        if let Some(node) = self.node.get() {\n            // Nothing synchronized by this atomic.\n            assert!(node.in_use.swap(false, Ordering::Relaxed));\n        }\n    }","Real(LocalPath(\"src/debt.rs\"))"],"<debt::Node as std::default::Default>::default":["fn default() -> Self{\n        Node {\n            next: None,\n            in_use: AtomicBool::new(true),\n            slots: Default::default(),\n        }\n    }","Real(LocalPath(\"src/debt.rs\"))"],"<gen_lock::GenLock<'_> as std::ops::Drop>::drop":["fn drop(&mut self){\n        // Release, so the dangerous section stays in. Acquire to chain the operations.\n        // Do not drop the inner (maybe we should do into_raw for proper measures?)\n        self.slot.fetch_sub(1, Ordering::AcqRel);\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::Global as gen_lock::LockStorage>::choose_shard":["#[inline]\nfn choose_shard(&self) -> usize{\n        THREAD_SHARD\n            .try_with(|ts| {\n                let mut val = ts.get();\n                if val >= SHARD_CNT {\n                    val = THREAD_ID_GEN.fetch_add(1, Ordering::Relaxed) % SHARD_CNT;\n                    ts.set(val);\n                }\n                val\n            })\n            .unwrap_or(0)\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::Global as gen_lock::LockStorage>::gen_idx":["#[inline]\nfn gen_idx(&self) -> &AtomicUsize{\n        &GEN_IDX\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::Global as gen_lock::LockStorage>::shards":["#[inline]\nfn shards(&self) -> &Shards{\n        &SHARDS\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::choose_shard":["#[inline]\nfn choose_shard(&self) -> usize{\n        0\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::gen_idx":["#[inline]\nfn gen_idx(&self) -> &AtomicUsize{\n        &self.gen_idx\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::shards":["#[inline]\nfn shards(&self) -> &[Self::Shard; 1]{\n        &self.shard\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<gen_lock::Shard as std::borrow::Borrow<[std::sync::atomic::AtomicUsize; _]>>::borrow":["#[inline]\nfn borrow(&self) -> &[AtomicUsize; GEN_CNT]{\n        &self.0\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"<std::option::Option<T> as ref_cnt::RefCnt>::as_ptr":["fn as_ptr(me: &Option<T>) -> *mut T::Base{\n        me.as_ref().map(T::as_ptr).unwrap_or_else(ptr::null_mut)\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::option::Option<T> as ref_cnt::RefCnt>::from_ptr":["unsafe fn from_ptr(ptr: *const T::Base) -> Option<T>{\n        if ptr.is_null() {\n            None\n        } else {\n            Some(T::from_ptr(ptr))\n        }\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::option::Option<T> as ref_cnt::RefCnt>::into_ptr":["fn into_ptr(me: Option<T>) -> *mut T::Base{\n        me.map(T::into_ptr).unwrap_or_else(ptr::null_mut)\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::as_ptr":["fn as_ptr(me: &Rc<T>) -> *mut T{\n        me as &T as *const T as *mut T\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::from_ptr":["unsafe fn from_ptr(ptr: *const T) -> Rc<T>{\n        Rc::from_raw(ptr)\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::into_ptr":["fn into_ptr(me: Rc<T>) -> *mut T{\n        Rc::into_raw(me) as *mut T\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::as_ptr":["fn as_ptr(me: &Arc<T>) -> *mut T{\n        me as &T as *const T as *mut T\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::from_ptr":["unsafe fn from_ptr(ptr: *const T) -> Arc<T>{\n        Arc::from_raw(ptr)\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::into_ptr":["fn into_ptr(me: Arc<T>) -> *mut T{\n        Arc::into_raw(me) as *mut T\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::CaS<T>>::compare_and_swap":["unsafe fn compare_and_swap<C: crate::as_raw::AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected{\n        let cur_ptr = current.as_raw();\n        let new = T::into_ptr(new);\n\n        // As noted above, this method has either semantics of load or of store. We don't know\n        // which ones upfront, so we need to implement safety measures for both.\n        let lock = GenLock::new(&self.0);\n\n        let previous_ptr = storage.compare_and_swap(cur_ptr, new, Ordering::SeqCst);\n        let swapped = ptr::eq(cur_ptr, previous_ptr);\n\n        // Drop it here, because:\n        // * We can't drop it before the compare_and_swap ‒ in such case, it could get recycled,\n        //   put into the pointer by another thread with a different value and create a fake\n        //   success (ABA).\n        // * We drop it before waiting for readers, because it could have been a Guard with a\n        //   generation lock. In such case, the caller doesn't have it any more and can't check if\n        //   it succeeded, but that's OK.\n        drop(current);\n\n        let previous = T::from_ptr(previous_ptr);\n\n        if swapped {\n            drop(lock);\n            gen_lock::wait_for_readers(&self.0);\n        } else {\n            // We didn't swap. Therefore, we need to bump the count on the old one and release the\n            // new one (blackhole it).\n            T::inc(&previous);\n            T::dec(new);\n        }\n\n        previous\n    }","Real(LocalPath(\"src/strategy/gen_lock.rs\"))"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::load":["unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> Self::Protected{\n        let lock = GenLock::new(&self.0);\n\n        let ptr = storage.load(Ordering::Acquire);\n        let result = T::from_ptr(ptr);\n        T::inc(&result);\n\n        drop(lock);\n\n        result\n    }","Real(LocalPath(\"src/strategy/gen_lock.rs\"))"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["unsafe fn wait_for_readers(&self, _: *const T::Base){\n        gen_lock::wait_for_readers(&self.0);\n    }","Real(LocalPath(\"src/strategy/gen_lock.rs\"))"],"<strategy::hybrid::HybridProtection<T> as std::borrow::Borrow<T>>::borrow":["#[inline]\nfn borrow(&self) -> &T{\n        &self.ptr\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridProtection<T> as std::ops::Drop>::drop":["#[inline]\nfn drop(&mut self){\n        match self.debt.take() {\n            // We have our own copy of Arc, so we don't need a protection. Do nothing (but release\n            // the Arc below).\n            None => (),\n            // If we owed something, just return the debt. We don't have a pointer owned, so\n            // nothing to release.\n            Some(debt) => {\n                let ptr = T::as_ptr(&self.ptr);\n                if debt.pay::<T>(ptr) {\n                    return;\n                }\n                // But if the debt was already paid for us, we need to release the pointer, as we\n                // were effectively already in the Unprotected mode.\n            }\n        }\n        // Equivalent to T::dec(ptr)\n        unsafe { ManuallyDrop::drop(&mut self.ptr) };\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::from_inner":["#[inline]\nfn from_inner(ptr: T) -> Self{\n        Self {\n            debt: None,\n            ptr: ManuallyDrop::new(ptr),\n        }\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::into_inner":["#[inline]\nfn into_inner(mut self) -> T{\n        // Drop any debt and release any lock held by the given guard and return a\n        // full-featured value that even can outlive the ArcSwap it originated from.\n        match self.debt.take() {\n            None => (), // We have a fully loaded ref-counted pointer.\n            Some(debt) => {\n                let ptr = T::inc(&self.ptr);\n                if !debt.pay::<T>(ptr) {\n                    unsafe { T::dec(ptr) };\n                }\n            }\n        }\n\n        // The ptr::read & forget is something like a cheating move. We can't move it out, because\n        // we have a destructor and Rust doesn't allow us to do that.\n        let inner = unsafe { ptr::read(self.ptr.deref()) };\n        mem::forget(self);\n        inner\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::load":["unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> Self::Protected{\n        HybridProtection::attempt(storage).unwrap_or_else(|| {\n            let loaded = self.fallback.load(storage);\n            HybridProtection {\n                debt: None,\n                ptr: ManuallyDrop::new(loaded),\n            }\n        })\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["unsafe fn wait_for_readers(&self, old: *const T::Base){\n        self.fallback.wait_for_readers(old);\n        Debt::pay_all::<T>(old);\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"<strategy::hybrid::HybridStrategy<strategy::gen_lock::GenLockStrategy<L>> as strategy::sealed::CaS<T>>::compare_and_swap":["unsafe fn compare_and_swap<C: crate::as_raw::AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected{\n        let cur_ptr = current.as_raw();\n        let new = T::into_ptr(new);\n\n        // As noted above, this method has either semantics of load or of store. We don't know\n        // which ones upfront, so we need to implement safety measures for both.\n        let gen = GenLock::new(&self.fallback.0);\n\n        let previous_ptr = storage.compare_and_swap(cur_ptr, new, Ordering::SeqCst);\n        let swapped = ptr::eq(cur_ptr, previous_ptr);\n\n        // Drop it here, because:\n        // * We can't drop it before the compare_and_swap ‒ in such case, it could get recycled,\n        //   put into the pointer by another thread with a different value and create a fake\n        //   success (ABA).\n        // * We drop it before waiting for readers, because it could have been a Guard with a\n        //   generation lock. In such case, the caller doesn't have it any more and can't check if\n        //   it succeeded, but that's OK.\n        drop(current);\n\n        let debt = if swapped {\n            // New went in, previous out, but their ref counts are correct. So nothing to do here.\n            None\n        } else {\n            // Previous is a new copy of what is inside (and it stays there as well), so bump its\n            // ref count. New is thrown away so dec its ref count (but do it outside of the\n            // gen-lock).\n            //\n            // We try to do that by registering a debt and only if that fails by actually bumping\n            // the ref.\n            let debt = Debt::new(previous_ptr as usize);\n            if debt.is_none() {\n                let previous = T::from_ptr(previous_ptr);\n                T::inc(&previous);\n                T::into_ptr(previous);\n            }\n            debt\n        };\n\n        drop(gen);\n\n        if swapped {\n            // We swapped. Before releasing the (possibly only) ref count of previous to user, wait\n            // for all readers to make sure there are no more untracked copies of it.\n            //\n            // Why is rustc confused about self.wait_for_readers???\n            InnerStrategy::<T>::wait_for_readers(self, previous_ptr);\n        } else {\n            // We didn't swap, so new is black-holed.\n            T::dec(new);\n        }\n\n        HybridProtection::new(previous_ptr, debt)\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"ArcSwapAny":["/// An atomic storage for a reference counted smart pointer like [`Arc`] or `Option<Arc>`.\n///\n/// This is a storage where a smart pointer may live. It can be read and written atomically from\n/// several threads, but doesn't act like a pointer itself.\n///\n/// One can be created [`from`] an [`Arc`]. To get the pointer back, use the\n/// [`load`](#method.load).\n///\n/// # Note\n///\n/// This is the common generic implementation. This allows sharing the same code for storing\n/// both `Arc` and `Option<Arc>` (and possibly other similar types).\n///\n/// In your code, you most probably want to interact with it through the\n/// [`ArcSwap`](type.ArcSwap.html) and [`ArcSwapOption`](type.ArcSwapOption.html) aliases. However,\n/// the methods they share are described here and are applicable to both of them. That's why the\n/// examples here use `ArcSwap` ‒ but they could as well be written with `ArcSwapOption` or\n/// `ArcSwapAny`.\n///\n/// # Type parameters\n///\n/// * `T`: The smart pointer to be kept inside. This crate provides implementation for `Arc<_>` and\n///   `Option<Arc<_>>` (`Rc` too, but that one is not practically useful). But third party could\n///   provide implementations of the [`RefCnt`] trait and plug in others.\n/// * `S`: Chooses the [strategy] used to protect the data inside. They come with various\n///   performance trade offs, the default [`DefaultStrategy`] is good rule of thumb for most use\n///   cases.\n///\n/// # Examples\n///\n/// ```rust\n/// # use std::sync::Arc;\n/// # use arc_swap::ArcSwap;\n/// let arc = Arc::new(42);\n/// let arc_swap = ArcSwap::from(arc);\n/// assert_eq!(42, **arc_swap.load());\n/// // It can be read multiple times\n/// assert_eq!(42, **arc_swap.load());\n///\n/// // Put a new one in there\n/// let new_arc = Arc::new(0);\n/// assert_eq!(42, *arc_swap.swap(new_arc));\n/// assert_eq!(0, **arc_swap.load());\n/// ```\n///\n/// [`Arc`]: https://doc.rust-lang.org/std/sync/struct.Arc.html\n/// [`from`]: https://doc.rust-lang.org/nightly/std/convert/trait.From.html#tymethod.from\n/// [`RefCnt`]: trait.RefCnt.html\npub struct ArcSwapAny<T: RefCnt, S: Strategy<T> = DefaultStrategy> {\n    // Notes: AtomicPtr needs Sized\n    /// The actual pointer, extracted from the Arc.\n    ptr: AtomicPtr<T::Base>,\n\n    /// We are basically an Arc in disguise. Inherit parameters from Arc by pretending to contain\n    /// it.\n    _phantom_arc: PhantomData<T>,\n\n    /// Strategy to protect the data.\n    strategy: S,\n}","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::compare_and_swap":["/// Swaps the stored Arc if it equals to `current`.\n///\n/// If the current value of the `ArcSwapAny` equals to `current`, the `new` is stored inside.\n/// If not, nothing happens.\n///\n/// The previous value (no matter if the swap happened or not) is returned. Therefore, if the\n/// returned value is equal to `current`, the swap happened. You want to do a pointer-based\n/// comparison to determine it.\n///\n/// In other words, if the caller „guesses“ the value of current correctly, it acts like\n/// [`swap`](#method.swap), otherwise it acts like [`load_full`](#method.load_full) (including\n/// the limitations).\n///\n/// The `current` can be specified as `&Arc`, [`Guard`](struct.Guard.html),\n/// [`&Guards`](struct.Guards.html) or as a raw pointer.\npub fn compare_and_swap<C>(&self, current: C, new: T) -> Guard<T, S>\n    where\n        C: AsRaw<T::Base>,\n        S: CaS<T>,{\n        let protected = unsafe { self.strategy.compare_and_swap(&self.ptr, current, new) };\n        Guard { inner: protected }\n        /*\n         */\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::into_inner":["/// Extracts the value inside.\npub fn into_inner(mut self) -> T{\n        let ptr = *self.ptr.get_mut();\n        // To pay all the debts\n        unsafe { self.strategy.wait_for_readers(ptr) };\n        mem::forget(self);\n        unsafe { T::from_ptr(ptr) }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::load":["/// Provides a temporary borrow of the object inside.\n///\n/// This returns a proxy object allowing access to the thing held inside. However, there's\n/// only limited amount of possible cheap proxies in existence for each thread ‒ if more are\n/// created, it falls back to equivalent of [`load_full`](#method.load_full) internally.\n///\n/// This is therefore a good choice to use for eg. searching a data structure or juggling the\n/// pointers around a bit, but not as something to store in larger amounts. The rule of thumb\n/// is this is suited for local variables on stack, but not in long-living data structures.\n///\n/// # Consistency\n///\n/// In case multiple related operations are to be done on the loaded value, it is generally\n/// recommended to call `load` just once and keep the result over calling it multiple times.\n/// First, keeping it is usually faster. But more importantly, the value can change between the\n/// calls to load, returning different objects, which could lead to logical inconsistency.\n/// Keeping the result makes sure the same object is used.\n///\n/// ```rust\n/// # use arc_swap::ArcSwap;\n/// struct Point {\n///     x: usize,\n///     y: usize,\n/// }\n///\n/// fn print_broken(p: &ArcSwap<Point>) {\n///     // This is broken, because the x and y may come from different points,\n///     // combining into an invalid point that never existed.\n///     println!(\"X: {}\", p.load().x);\n///     // If someone changes the content now, between these two loads, we\n///     // have a problem\n///     println!(\"Y: {}\", p.load().y);\n/// }\n///\n/// fn print_correct(p: &ArcSwap<Point>) {\n///     // Here we take a snapshot of one specific point so both x and y come\n///     // from the same one.\n///     let point = p.load();\n///     println!(\"X: {}\", point.x);\n///     println!(\"Y: {}\", point.y);\n/// }\n/// # let p = ArcSwap::from_pointee(Point { x: 10, y: 20 });\n/// # print_correct(&p);\n/// # print_broken(&p);\n/// ```\n#[inline]\npub fn load(&self) -> Guard<T, S>{\n        let protected = unsafe { self.strategy.load(&self.ptr) };\n        Guard { inner: protected }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::load_full":["/// Loads the value.\n///\n/// This makes another copy of the held pointer and returns it, atomically (it is\n/// safe even when other thread stores into the same instance at the same time).\n///\n/// The method is lock-free and wait-free, but usually more expensive than\n/// [`load`](#method.load).\npub fn load_full(&self) -> T{\n        Guard::into_inner(self.load())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::map":["/// Provides an access to an up to date projection of the carried data.\n///\n/// # Motivation\n///\n/// Sometimes, an application consists of components. Each component has its own configuration\n/// structure. The whole configuration contains all the smaller config parts.\n///\n/// For the sake of separation and abstraction, it is not desirable to pass the whole\n/// configuration to each of the components. This allows the component to take only access to\n/// its own part.\n///\n/// # Lifetimes & flexibility\n///\n/// This method is not the most flexible way, as the returned type borrows into the `ArcSwap`.\n/// To provide access into eg. `Arc<ArcSwap<T>>`, you can create the [`Map`] type directly. See\n/// the [`access`] module.\n///\n/// # Performance\n///\n/// As the provided function is called on each load from the shared storage, it should\n/// generally be cheap. It is expected this will usually be just referencing of a field inside\n/// the structure.\n///\n/// # Examples\n///\n/// ```rust\n/// use std::sync::Arc;\n///\n/// use arc_swap::ArcSwap;\n/// use arc_swap::access::Access;\n///\n/// struct Cfg {\n///     value: usize,\n/// }\n///\n/// fn print_many_times<V: Access<usize>>(value: V) {\n///     for _ in 0..25 {\n///         let value = value.load();\n///         println!(\"{}\", *value);\n///     }\n/// }\n///\n/// let shared = ArcSwap::from_pointee(Cfg { value: 0 });\n/// let mapped = shared.map(|c: &Cfg| &c.value);\n/// crossbeam_utils::thread::scope(|s| {\n///     // Will print some zeroes and some twos\n///     s.spawn(|_| print_many_times(mapped));\n///     s.spawn(|_| shared.store(Arc::new(Cfg { value: 2 })));\n/// }).expect(\"Something panicked in a thread\");\n/// ```\npub fn map<I, R, F>(&self, f: F) -> Map<&Self, I, F>\n    where\n        F: Fn(&I) -> &R,\n        Self: Access<I>,{\n        Map::new(self, f)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::new":["/// Constructs a new storage.\npub fn new(val: T) -> Self\n    where\n        S: Default,{\n        Self::from(val)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::rcu":["/// Read-Copy-Update of the pointer inside.\n///\n/// This is useful in read-heavy situations with several threads that sometimes update the data\n/// pointed to. The readers can just repeatedly use [`load`](#method.load) without any locking.\n/// The writer uses this method to perform the update.\n///\n/// In case there's only one thread that does updates or in case the next version is\n/// independent of the previous one, simple [`swap`](#method.swap) or [`store`](#method.store)\n/// is enough. Otherwise, it may be needed to retry the update operation if some other thread\n/// made an update in between. This is what this method does.\n///\n/// # Examples\n///\n/// This will *not* work as expected, because between loading and storing, some other thread\n/// might have updated the value.\n///\n/// ```rust\n/// # use std::sync::Arc;\n/// #\n/// # use arc_swap::ArcSwap;\n/// # use crossbeam_utils::thread;\n/// #\n/// let cnt = ArcSwap::from_pointee(0);\n/// thread::scope(|scope| {\n///     for _ in 0..10 {\n///         scope.spawn(|_| {\n///            let inner = cnt.load_full();\n///             // Another thread might have stored some other number than what we have\n///             // between the load and store.\n///             cnt.store(Arc::new(*inner + 1));\n///         });\n///     }\n/// }).unwrap();\n/// // This will likely fail:\n/// // assert_eq!(10, *cnt.load_full());\n/// ```\n///\n/// This will, but it can call the closure multiple times to retry:\n///\n/// ```rust\n/// # use arc_swap::ArcSwap;\n/// # use crossbeam_utils::thread;\n/// #\n/// let cnt = ArcSwap::from_pointee(0);\n/// thread::scope(|scope| {\n///     for _ in 0..10 {\n///         scope.spawn(|_| cnt.rcu(|inner| **inner + 1));\n///     }\n/// }).unwrap();\n/// assert_eq!(10, *cnt.load_full());\n/// ```\n///\n/// Due to the retries, you might want to perform all the expensive operations *before* the\n/// rcu. As an example, if there's a cache of some computations as a map, and the map is cheap\n/// to clone but the computations are not, you could do something like this:\n///\n/// ```rust\n/// # use std::collections::HashMap;\n/// #\n/// # use arc_swap::ArcSwap;\n/// # use once_cell::sync::Lazy;\n/// #\n/// fn expensive_computation(x: usize) -> usize {\n///     x * 2 // Let's pretend multiplication is *really expensive expensive*\n/// }\n///\n/// type Cache = HashMap<usize, usize>;\n///\n/// static CACHE: Lazy<ArcSwap<Cache>> = Lazy::new(|| ArcSwap::default());\n///\n/// fn cached_computation(x: usize) -> usize {\n///     let cache = CACHE.load();\n///     if let Some(result) = cache.get(&x) {\n///         return *result;\n///     }\n///     // Not in cache. Compute and store.\n///     // The expensive computation goes outside, so it is not retried.\n///     let result = expensive_computation(x);\n///     CACHE.rcu(|cache| {\n///         // The cheaper clone of the cache can be retried if need be.\n///         let mut cache = HashMap::clone(&cache);\n///         cache.insert(x, result);\n///         cache\n///     });\n///     result\n/// }\n///\n/// assert_eq!(42, cached_computation(21));\n/// assert_eq!(42, cached_computation(21));\n/// ```\n///\n/// # The cost of cloning\n///\n/// Depending on the size of cache above, the cloning might not be as cheap. You can however\n/// use persistent data structures ‒ each modification creates a new data structure, but it\n/// shares most of the data with the old one (which is usually accomplished by using `Arc`s\n/// inside to share the unchanged values). Something like\n/// [`rpds`](https://crates.io/crates/rpds) or [`im`](https://crates.io/crates/im) might do\n/// what you need.\npub fn rcu<R, F>(&self, mut f: F) -> T\n    where\n        F: FnMut(&T) -> R,\n        R: Into<T>,\n        S: CaS<T>,{\n        let mut cur = self.load();\n        loop {\n            let new = f(&cur).into();\n            let prev = self.compare_and_swap(&*cur, new);\n            let swapped = ptr_eq(&*cur, &*prev);\n            if swapped {\n                return Guard::into_inner(prev);\n            } else {\n                cur = prev;\n            }\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::store":["/// Replaces the value inside this instance.\n///\n/// Further loads will yield the new value. Uses [`swap`](#method.swap) internally.\npub fn store(&self, val: T){\n        drop(self.swap(val));\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::swap":["/// Exchanges the value inside this instance.\n///\n/// Note that this method is *not* lock-free with the [`DefaultStrategy`].\npub fn swap(&self, new: T) -> T{\n        let new = T::into_ptr(new);\n        // AcqRel needed to publish the target of the new pointer and get the target of the old\n        // one.\n        //\n        // SeqCst to synchronize the time lines with the group counters.\n        let old = self.ptr.swap(new, Ordering::SeqCst);\n        unsafe {\n            self.strategy.wait_for_readers(old);\n            T::from_ptr(old)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<T, S>::with_strategy":["/// Constructs a new storage while customizing the protection strategy.\npub fn with_strategy(val: T, strategy: S) -> Self{\n        // The AtomicPtr requires *mut in its interface. We are more like *const, so we cast it.\n        // However, we always go back to *const right away when we get the pointer on the other\n        // side, so it should be fine.\n        let ptr = T::into_ptr(val);\n        Self {\n            ptr: AtomicPtr::new(ptr),\n            _phantom_arc: PhantomData,\n            strategy,\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::empty":["/// A convenience constructor for an empty value.\n///\n/// This is equivalent to `ArcSwapOption::new(None)`.\npub fn empty() -> Self\n    where\n        S: Default,{\n        Self::new(None)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::from_pointee":["/// A convenience constructor directly from a pointed-to value.\n///\n/// This just allocates the `Arc` under the hood.\n///\n/// # Examples\n///\n/// ```rust\n/// use arc_swap::ArcSwapOption;\n///\n/// let empty: ArcSwapOption<usize> = ArcSwapOption::from_pointee(None);\n/// assert!(empty.load().is_none());\n/// let non_empty: ArcSwapOption<usize> = ArcSwapOption::from_pointee(42);\n/// assert_eq!(42, **non_empty.load().as_ref().unwrap());\n/// ```\npub fn from_pointee<V: Into<Option<T>>>(val: V) -> Self\n    where\n        S: Default,{\n        Self::new(val.into().map(Arc::new))\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ArcSwapAny::<std::sync::Arc<T>, S>::from_pointee":["/// A convenience constructor directly from the pointed-to value.\n///\n/// Direct equivalent for `ArcSwap::new(Arc::new(val))`.\npub fn from_pointee(val: T) -> Self\n    where\n        S: Default,{\n        Self::from(Arc::new(val))\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Guard":["/// A temporary storage of the pointer.\n///\n/// This guard object is returned from most loading methods (with the notable exception of\n/// [`load_full`](struct.ArcSwapAny.html#method.load_full)). It dereferences to the smart pointer\n/// loaded, so most operations are to be done using that.\npub struct Guard<T: RefCnt, S: Strategy<T> = DefaultStrategy> {\n    inner: S::Protected,\n}","Real(LocalPath(\"src/lib.rs\"))"],"Guard::<T, S>::from_inner":["/// Create a guard for a given value `inner`.\n///\n/// This can be useful on occasion to pass a specific object to code that expects or\n/// wants to store a Guard.\n///\n/// # Example\n///\n/// ```rust\n/// # use arc_swap::{ArcSwap, DefaultStrategy, Guard};\n/// # use std::sync::Arc;\n/// # let p = ArcSwap::from_pointee(42);\n/// // Create two guards pointing to the same object\n/// let g1 = p.load();\n/// let g2 = Guard::<_, DefaultStrategy>::from_inner(Arc::clone(&*g1));\n/// # drop(g2);\n/// ```\npub fn from_inner(inner: T) -> Self{\n        Guard {\n            inner: S::Protected::from_inner(inner),\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Guard::<T, S>::into_inner":["/// Converts it into the held value.\n///\n/// This, on occasion, may be a tiny bit faster than cloning the Arc or whatever is being held\n/// inside.\n#[allow(clippy::wrong_self_convention)]\n#[inline]\npub fn into_inner(lease: Self) -> T{\n        lease.inner.into_inner()\n    }","Real(LocalPath(\"src/lib.rs\"))"],"access::Access":["/// Abstracts over ways code can get access to a value of type `T`.\n///\n/// This is the trait that parts of code will use when accessing a subpart of the big data\n/// structure. See the [module documentation](index.html) for details.\npub trait Access<T> {\n    /// A guard object containing the value and keeping it alive.\n    ///\n    /// For technical reasons, the library doesn't allow direct access into the stored value. A\n    /// temporary guard object must be loaded, that keeps the actual value alive for the time of\n    /// use.\n    type Guard: Deref<Target = T>;\n\n    /// The loading method.\n    ///\n    /// This returns the guard that holds the actual value. Should be called anew each time a fresh\n    /// value is needed.\n    fn load(&self) -> Self::Guard;\n}","Real(LocalPath(\"src/access.rs\"))"],"access::Constant":["/// Access to an constant.\n///\n/// This wraps a constant value to provide [`Access`] to it. It is constant in the sense that,\n/// unlike [`ArcSwapAny`] and [`Map`], the loaded value will always stay the same (there's no\n/// remote `store`).\n///\n/// The purpose is mostly testing and plugging a parameter that works generically from code that\n/// doesn't need the updating functionality.\npub struct Constant<T>(pub T);","Real(LocalPath(\"src/access.rs\"))"],"access::ConstantDeref":["#[doc(hidden)]\npub struct ConstantDeref<T>(T);","Real(LocalPath(\"src/access.rs\"))"],"access::DirectDeref":["#[doc(hidden)]\npub struct DirectDeref<T: RefCnt, S: Strategy<T>>(Guard<T, S>);","Real(LocalPath(\"src/access.rs\"))"],"access::DynAccess":["/// An object-safe version of the [`Access`] trait.\n///\n/// This can be used instead of the [`Access`] trait in case a type erasure is desired. This has\n/// the effect of performance hit (due to boxing of the result and due to dynamic dispatch), but\n/// makes certain code simpler and possibly makes the executable smaller.\n///\n/// This is automatically implemented for everything that implements [`Access`].\n///\n/// # Examples\n///\n/// ```rust\n/// use std::thread;\n///\n/// use arc_swap::access::{Constant, DynAccess};\n///\n/// fn do_something(value: Box<dyn DynAccess<usize> + Send>) {\n///     thread::spawn(move || {\n///         let v = value.load();\n///         println!(\"{}\", *v);\n///     });\n/// }\n///\n/// do_something(Box::new(Constant(42)));\n/// ```\npub trait DynAccess<T> {\n    /// The equivalent of [`Access::load`].\n    fn load(&self) -> DynGuard<T>;\n}","Real(LocalPath(\"src/access.rs\"))"],"access::DynGuard":["#[doc(hidden)]\npub struct DynGuard<T: ?Sized>(Box<dyn Deref<Target = T>>);","Real(LocalPath(\"src/access.rs\"))"],"access::Map":["/// An adaptor to provide access to a part of larger structure.\n///\n/// This is the *active* part of this module. Use the [module documentation](index.html) for the\n/// details.\npub struct Map<A, T, F> {\n    access: A,\n    projection: F,\n    _t: PhantomData<fn() -> T>,\n}","Real(LocalPath(\"src/access.rs\"))"],"access::Map::<A, T, F>::new":["/// Creates a new instance.\n///\n/// # Parameters\n///\n/// * `access`: Access to the bigger structure. This is usually something like `Arc<ArcSwap>`\n///   or `&ArcSwap`. It is technically possible to use any other [`Access`] here, though, for\n///   example to sub-delegate into even smaller structure from a [`Map`] (or generic\n///   [`Access`]).\n/// * `projection`: A function (or closure) responsible to providing a reference into the\n///   bigger bigger structure, selecting just subset of it. In general, it is expected to be\n///   *cheap* (like only taking reference).\npub fn new<R>(access: A, projection: F) -> Self\n    where\n        F: Fn(&T) -> &R,{\n        Map {\n            access,\n            projection,\n            _t: PhantomData,\n        }\n    }","Real(LocalPath(\"src/access.rs\"))"],"access::MapGuard":["#[doc(hidden)]\npub struct MapGuard<G, T> {\n    _guard: G,\n    value: *const T,\n}","Real(LocalPath(\"src/access.rs\"))"],"as_raw::AsRaw":["/// A trait describing things that can be turned into a raw pointer.\n///\n/// This is just an abstraction of things that can be passed to the\n/// [`compare_and_swap`](struct.ArcSwapAny.html#method.compare_and_swap).\n///\n/// # Examples\n///\n/// ```\n/// use std::ptr;\n/// use std::sync::Arc;\n///\n/// use arc_swap::ArcSwapOption;\n///\n/// let a = Arc::new(42);\n/// let shared = ArcSwapOption::from(Some(Arc::clone(&a)));\n///\n/// shared.compare_and_swap(&a, Some(Arc::clone(&a)));\n/// shared.compare_and_swap(&None::<Arc<_>>, Some(Arc::clone(&a)));\n/// shared.compare_and_swap(shared.load(), Some(Arc::clone(&a)));\n/// shared.compare_and_swap(&shared.load(), Some(Arc::clone(&a)));\n/// shared.compare_and_swap(ptr::null(), Some(Arc::clone(&a)));\n/// ```\npub trait AsRaw<T>: Sealed {\n    /// Converts the value into a raw pointer.\n    fn as_raw(&self) -> *mut T;\n}","Real(LocalPath(\"src/as_raw.rs\"))"],"as_raw::sealed::Sealed":["pub trait Sealed {}","Real(LocalPath(\"src/as_raw.rs\"))"],"cache::Access":["/// Generalization of caches providing access to `T`.\n///\n/// This abstracts over all kinds of caches that can provide a cheap access to values of type `T`.\n/// This is useful in cases where some code doesn't care if the `T` is the whole structure or just\n/// a part of it.\n///\n/// See the example at [`Cache::map`].\npub trait Access<T> {\n    /// Loads the value from cache.\n    ///\n    /// This revalidates the value in the cache, then provides the access to the cached value.\n    fn load(&mut self) -> &T;\n}","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache":["/// Caching handle for [`ArcSwapAny`][ArcSwapAny].\n///\n/// Instead of loading the [`Arc`][Arc] on every request from the shared storage, this keeps\n/// another copy inside itself. Upon request it only cheaply revalidates it is up to\n/// date. If it is, access is significantly faster. If it is stale, the [load_full] is done and the\n/// cache value is replaced. Under a read-heavy loads, the measured speedup are 10-25 times,\n/// depending on the architecture.\n///\n/// There are, however, downsides:\n///\n/// * The handle needs to be kept around by the caller (usually, one per thread). This is fine if\n///   there's one global `ArcSwapAny`, but starts being tricky with eg. data structures build from\n///   them.\n/// * As it keeps a copy of the [Arc] inside the cache, the old value may be kept alive for longer\n///   period of time ‒ it is replaced by the new value on [load][Cache::load]. You may not want to\n///   use this if dropping the old value in timely manner is important (possibly because of\n///   releasing large amount of RAM or because of closing file handles).\n///\n/// # Examples\n///\n/// ```rust\n/// # fn do_something<V>(_v: V) { }\n/// use std::sync::Arc;\n///\n/// use arc_swap::{ArcSwap, Cache};\n///\n/// let shared = Arc::new(ArcSwap::from_pointee(42));\n/// // Start 10 worker threads...\n/// for _ in 0..10 {\n///     let mut cache = Cache::new(Arc::clone(&shared));\n///     std::thread::spawn(move || {\n///         // Keep loading it like mad..\n///         loop {\n///             let value = cache.load();\n///             do_something(value);\n///         }\n///     });\n/// }\n/// shared.store(Arc::new(12));\n/// ```\n///\n/// [Arc]: std::sync::Arc\n/// [load_full]: ArcSwapAny::load_full\npub struct Cache<A, T> {\n    arc_swap: A,\n    cached: T,\n}","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::arc_swap":["/// Gives access to the (possibly shared) cached [`ArcSwapAny`].\npub fn arc_swap(&self) -> &A::Target{\n        &self.arc_swap\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::load":["/// Loads the currently held value.\n///\n/// This first checks if the cached value is up to date. This check is very cheap.\n///\n/// If it is up to date, the cached value is simply returned without additional costs. If it is\n/// outdated, a load is done on the underlying shared storage. The newly loaded value is then\n/// stored in the cache and returned.\n#[inline]\npub fn load(&mut self) -> &T{\n        self.revalidate();\n        self.load_no_revalidate()\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::load_no_revalidate":["#[inline]\nfn load_no_revalidate(&self) -> &T{\n        &self.cached\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::map":["/// Turns this cache into a cache with a projection inside the cached value.\n///\n/// You'd use this in case when some part of code needs access to fresh values of `U`, however\n/// a bigger structure containing `U` is provided by this cache. The possibility of giving the\n/// whole structure to the part of the code falls short in terms of reusability (the part of\n/// the code could be used within multiple contexts, each with a bigger different structure\n/// containing `U`) and code separation (the code shouldn't needs to know about the big\n/// structure).\n///\n/// # Warning\n///\n/// As the provided `f` is called inside every [`load`][Access::load], this one should be\n/// cheap. Most often it is expected to be just a closure taking reference of some inner field.\n///\n/// For the same reasons, it should not have side effects and should never panic (these will\n/// not break Rust's safety rules, but might produce behaviour you don't expect).\n///\n/// # Examples\n///\n/// ```rust\n/// use arc_swap::ArcSwap;\n/// use arc_swap::cache::{Access, Cache};\n///\n/// struct InnerCfg {\n///     answer: usize,\n/// }\n///\n/// struct FullCfg {\n///     inner: InnerCfg,\n/// }\n///\n/// fn use_inner<A: Access<InnerCfg>>(cache: &mut A) {\n///     let value = cache.load();\n///     println!(\"The answer is: {}\", value.answer);\n/// }\n///\n/// let full_cfg = ArcSwap::from_pointee(FullCfg {\n///     inner: InnerCfg {\n///         answer: 42,\n///     }\n/// });\n/// let cache = Cache::new(&full_cfg);\n/// use_inner(&mut cache.map(|full| &full.inner));\n///\n/// let inner_cfg = ArcSwap::from_pointee(InnerCfg { answer: 24 });\n/// let mut inner_cache = Cache::new(&inner_cfg);\n/// use_inner(&mut inner_cache);\n/// ```\npub fn map<F, U>(self, f: F) -> MapCache<A, T, F>\n    where\n        F: FnMut(&T) -> &U,{\n        MapCache {\n            inner: self,\n            projection: f,\n        }\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::new":["/// Creates a new caching handle.\n///\n/// The parameter is something dereferencing into an [`ArcSwapAny`] (eg. either to [`ArcSwap`]\n/// or [`ArcSwapOption`]). That can be [`ArcSwapAny`] itself, but that's not very useful. But\n/// it also can be a reference to it or `Arc`, which makes it possible to share the\n/// [`ArcSwapAny`] with multiple caches or access it in non-cached way too.\n///\n/// [`ArcSwapOption`]: crate::ArcSwapOption\n/// [`ArcSwap`]: crate::ArcSwap\npub fn new(arc_swap: A) -> Self{\n        let cached = arc_swap.load_full();\n        Self { arc_swap, cached }\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::Cache::<A, T>::revalidate":["#[inline]\nfn revalidate(&mut self){\n        let cached_ptr = RefCnt::as_ptr(&self.cached);\n        // Node: Relaxed here is fine. We do not synchronize any data through this, we already have\n        // it synchronized in self.cache. We just want to check if it changed, if it did, the\n        // load_full will be responsible for any synchronization needed.\n        let shared_ptr = self.arc_swap.ptr.load(Ordering::Relaxed);\n        if cached_ptr != shared_ptr {\n            self.cached = self.arc_swap.load_full();\n        }\n    }","Real(LocalPath(\"src/cache.rs\"))"],"cache::MapCache":["/// An implementation of a cache with a projection into the accessed value.\n///\n/// This is the implementation structure for [`Cache::map`]. It can't be created directly and it\n/// should be used through the [`Access`] trait.\npub struct MapCache<A, T, F> {\n    inner: Cache<A, T>,\n    projection: F,\n}","Real(LocalPath(\"src/cache.rs\"))"],"debt::Debt":["/// One debt slot.\npub(crate) struct Debt(AtomicUsize);","Real(LocalPath(\"src/debt.rs\"))"],"debt::Debt::new":["/// Creates a new debt.\n///\n/// This stores the debt of the given pointer (untyped, casted into an usize) and returns a\n/// reference to that slot, or gives up with `None` if all the slots are currently full.\n///\n/// This is technically lock-free on the first call in a given thread and wait-free on all the\n/// other accesses.\n#[allow(clippy::new_ret_no_self)]\n#[inline]\npub(crate) fn new(ptr: usize) -> Option<&'static Self>{\n        THREAD_HEAD\n            .try_with(|head| {\n                let node = match head.node.get() {\n                    // Already have my own node (most likely)?\n                    Some(node) => node,\n                    // No node yet, called for the first time in this thread. Set one up.\n                    None => {\n                        let new_node = Node::get();\n                        head.node.set(Some(new_node));\n                        new_node\n                    }\n                };\n                // Check it is in use by *us*\n                debug_assert!(node.in_use.load(Ordering::Relaxed));\n                // Trick with offsets: we rotate through the slots (save the value from last time)\n                // so successive leases are likely to succeed on the first attempt (or soon after)\n                // instead of going through the list of already held ones.\n                let offset = head.offset.get();\n                let len = node.slots.0.len();\n                for i in 0..len {\n                    let i = (i + offset) % len;\n                    // Note: the indexing check is almost certainly optimised out because the len\n                    // is used above. And using .get_unchecked was actually *slower*.\n                    let got_it = node.slots.0[i]\n                        .0\n                        // Try to acquire the slot. Relaxed if it doesn't work is fine, as we don't\n                        // synchronize by it.\n                        .compare_exchange(NO_DEBT, ptr, Ordering::SeqCst, Ordering::Relaxed)\n                        .is_ok();\n                    if got_it {\n                        head.offset.set(i + 1);\n                        return Some(&node.slots.0[i]);\n                    }\n                }\n                None\n            })\n            .ok()\n            .and_then(|new| new)\n    }","Real(LocalPath(\"src/debt.rs\"))"],"debt::Debt::pay":["/// Tries to pay the given debt.\n///\n/// If the debt is still there, for the given pointer, it is paid and `true` is returned. If it\n/// is empty or if there's some other pointer, it is not paid and `false` is returned, meaning\n/// the debt was paid previously by someone else.\n///\n/// # Notes\n///\n/// * It is possible that someone paid the debt and then someone else put a debt for the same\n///   pointer in there. This is fine, as we'll just pay the debt for that someone else.\n/// * This relies on the fact that the same pointer must point to the same object and\n///   specifically to the same type ‒ the caller provides the type, it's destructor, etc.\n/// * It also relies on the fact the same thing is not stuffed both inside an `Arc` and `Rc` or\n///   something like that, but that sounds like a reasonable assumption. Someone storing it\n///   through `ArcSwap<T>` and someone else with `ArcSwapOption<T>` will work.\n#[inline]\npub(crate) fn pay<T: RefCnt>(&self, ptr: *const T::Base) -> bool{\n        self.0\n            // If we don't change anything because there's something else, Relaxed is fine.\n            //\n            // The Release works as kind of Mutex. We make sure nothing from the debt-protected\n            // sections leaks below this point.\n            .compare_exchange(ptr as usize, NO_DEBT, Ordering::Release, Ordering::Relaxed)\n            .is_ok()\n    }","Real(LocalPath(\"src/debt.rs\"))"],"debt::Debt::pay_all":["/// Pays all the debts on the given pointer.\npub(crate) fn pay_all<T: RefCnt>(ptr: *const T::Base){\n        let val = unsafe { T::from_ptr(ptr) };\n        T::inc(&val);\n        traverse::<(), _>(|node| {\n            for slot in &node.slots.0 {\n                if slot\n                    .0\n                    .compare_exchange(ptr as usize, NO_DEBT, Ordering::AcqRel, Ordering::Relaxed)\n                    .is_ok()\n                {\n                    T::inc(&val);\n                }\n            }\n            None\n        });\n    }","Real(LocalPath(\"src/debt.rs\"))"],"debt::DebtHead":["/// A wrapper around a node pointer, to un-claim the node on thread shutdown.\nstruct DebtHead {\n    // Node for this thread.\n    node: Cell<Option<&'static Node>>,\n    // The next slot in round-robin rotation. Heuristically tries to balance the load across them\n    // instead of having all of them stuffed towards the start of the array which gets\n    // unsuccessfully iterated through every time.\n    offset: Cell<usize>,\n}","Real(LocalPath(\"src/debt.rs\"))"],"debt::Node":["/// One thread-local node for debts.\n#[repr(C)]\nstruct Node {\n    slots: Slots,\n    next: Option<&'static Node>,\n    in_use: AtomicBool,\n}","Real(LocalPath(\"src/debt.rs\"))"],"debt::Node::get":["fn get() -> &'static Self{\n        // Try to find an unused one in the chain and reuse it.\n        traverse(|node| {\n            // Try to claim this node. Nothing is synchronized through this atomic, we only\n            // track if someone claims ownership of it.\n            if !node.in_use.compare_and_swap(false, true, Ordering::Relaxed) {\n                Some(node)\n            } else {\n                None\n            }\n        })\n        // If that didn't work, create a new one and prepend to the list.\n        .unwrap_or_else(|| {\n            let node = Box::leak(Box::new(Node::default()));\n            // Not shared between threads yet, so ordinary write would be fine too.\n            node.in_use.store(true, Ordering::Relaxed);\n            // We don't want to read any data in addition to the head, Relaxed is fine\n            // here.\n            //\n            // We do need to release the data to others, but for that, we acquire in the\n            // compare_exchange below.\n            let mut head = DEBT_HEAD.load(Ordering::Relaxed);\n            loop {\n                node.next = unsafe { head.as_ref() };\n                if let Err(old) = DEBT_HEAD.compare_exchange_weak(\n                    head,\n                    node,\n                    // We need to release *the whole chain* here. For that, we need to\n                    // acquire it first.\n                    Ordering::AcqRel,\n                    Ordering::Relaxed, // Nothing changed, go next round of the loop.\n                ) {\n                    head = old;\n                } else {\n                    return node;\n                }\n            }\n        })\n    }","Real(LocalPath(\"src/debt.rs\"))"],"debt::Slots":["#[repr(align(64))]\nstruct Slots([Debt; DEBT_SLOT_CNT]);","Real(LocalPath(\"src/debt.rs\"))"],"debt::traverse":["/// Goes through the debt linked list.\n///\n/// This traverses the linked list, calling the closure on each node. If the closure returns\n/// `Some`, it terminates with that value early, otherwise it runs to the end.\nfn traverse<R, F: FnMut(&'static Node) -> Option<R>>(mut f: F) -> Option<R>{\n    // Acquire ‒ we want to make sure we read the correct version of data at the end of the\n    // pointer. Any write to the DEBT_HEAD is with Release.\n    //\n    // Note that the other pointers in the chain never change and are *ordinary* pointers. The\n    // whole linked list is synchronized through the head.\n    let mut current = unsafe { DEBT_HEAD.load(Ordering::Acquire).as_ref() };\n    while let Some(node) = current {\n        let result = f(node);\n        if result.is_some() {\n            return result;\n        }\n        current = node.next;\n    }\n    None\n}","Real(LocalPath(\"src/debt.rs\"))"],"gen_lock::GenLock":["pub(crate) struct GenLock<'a> {\n    slot: &'a AtomicUsize,\n}","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::GenLock::<'a>::new":["pub(crate) fn new<S: LockStorage + 'a>(storage: &'a S) -> Self{\n        let shard = storage.choose_shard();\n        let gen = storage.gen_idx().load(Ordering::Relaxed) % GEN_CNT;\n        // TODO: Is this still needed? Is the other SeqCst needed, in the writer? Is *there* any?\n        // Or should it be Release in there and SeqCst barrier as part of wait_for_readers?\n        // SeqCst: Acquire, so the dangerous section stays in. SeqCst to sync timelines with the\n        // swap on the ptr in writer thread.\n        let slot = &storage.shards().as_ref()[shard].borrow()[gen];\n        let old = slot.fetch_add(1, Ordering::SeqCst);\n        // The trick is taken from Arc.\n        if old > MAX_GUARDS {\n            process::abort();\n        }\n\n        Self { slot }\n    }","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::Global":["/// The default, global lock.\n///\n/// The lock is stored out-of-band, globally. This means that one `ArcSwap` with this lock storage\n/// is only one machine word large, but a lock on one instance blocks the other, independent ones.\n///\n/// It has several shards so threads are less likely to collide (HW-contend) on them.\npub struct Global;","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::LockStorage":["/// Abstraction of the place where generation locks are stored.\n///\n/// The trait is unsafe because if the trait messes up with the values stored in there in any way\n/// (or makes the values available to something else that messes them up), this can cause UB and\n/// daemons and discomfort to users and such. The library expects it is the only one storing values\n/// there. In other words, it is expected the trait is only a dumb storage and doesn't actively do\n/// anything.\npub unsafe trait LockStorage: Default {\n    /// Type of one shard.\n    type Shard: Borrow<[AtomicUsize; GEN_CNT]>;\n\n    /// The type for keeping several shards.\n    ///\n    /// In general, it is expected to be a fixed-size array, but different implementations can have\n    /// different sizes.\n    type Shards: AsRef<[Self::Shard]>;\n\n    /// Access to the generation index.\n    ///\n    /// Must return the same instance of the `AtomicUsize` for the lifetime of the storage, must\n    /// start at `0` and the trait itself must not modify it.\n    fn gen_idx(&self) -> &AtomicUsize;\n\n    /// Access to the shards storage.\n    ///\n    /// Must return the same instance of the shards for the lifetime of the storage. Must start\n    /// zeroed-out and the trait itself must not modify it.\n    fn shards(&self) -> &Self::Shards;\n\n    /// Pick one shard of the all selected.\n    ///\n    /// Returns the index of one of the shards. The choice can be arbitrary, but it should be fast\n    /// and avoid collisions.\n    fn choose_shard(&self) -> usize;\n}","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::PrivateUnsharded":["/// A single „shard“ that is stored inline, inside the corresponding `ArcSwap`. Therefore, locks on\n/// each instance won't influence any other instances. On the other hand, the `ArcSwap` itself gets\n/// bigger and doesn't have multiple shards, so concurrent uses might contend each other a bit.\n///\n/// Note that there`s a type alias [`IndependentArcSwap`](../type.IndependentArcSwap.html) that can\n/// be used instead.\npub struct PrivateUnsharded {\n    gen_idx: AtomicUsize,\n    shard: [[AtomicUsize; GEN_CNT]; 1],\n}","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::Shard":["/// A single shard.\n///\n/// This is one copy of place where the library keeps tracks of generation locks. It consists of a\n/// pair of counters and allows double-buffering readers (therefore, even if there's a never-ending\n/// stream of readers coming in, writer will get through eventually).\n///\n/// To avoid contention and sharing of the counters between readers, we don't have one pair of\n/// generation counters, but several. The reader picks one shard and uses that, while the writer\n/// looks through all of them. This is still not perfect (two threads may choose the same ID), but\n/// it helps.\n///\n/// Each [`LockStorage`](trait.LockStorage.html) must provide a (non-empty) array of these.\n#[repr(align(64))]\npub struct Shard(pub(crate) [AtomicUsize; GEN_CNT]);","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::snapshot":["fn snapshot(shard: &[AtomicUsize; GEN_CNT]) -> [usize; GEN_CNT]{\n    [\n        shard[0].load(Ordering::Acquire),\n        shard[1].load(Ordering::Acquire),\n    ]\n}","Real(LocalPath(\"src/gen_lock.rs\"))"],"gen_lock::wait_for_readers":["pub(crate) fn wait_for_readers<S: LockStorage>(storage: &S){\n    let mut seen_group = [false; GEN_CNT];\n    let mut iter = 0usize;\n    let gen_idk = storage.gen_idx();\n    let shards = storage.shards().as_ref();\n\n    loop {\n        // Note that we don't need the snapshot to be consistent. We just need to see both\n        // halves being zero, not necessarily at the same time.\n        let gen = gen_idk.load(Ordering::Relaxed);\n        let groups = shards.iter().fold([0, 0], |[a1, a2], s| {\n            let [v1, v2] = snapshot(s.borrow());\n            [a1 + v1, a2 + v2]\n        });\n        // Should we increment the generation? Is the next one empty?\n        let next_gen = gen.wrapping_add(1);\n        if groups[next_gen % GEN_CNT] == 0 {\n            // Replace it only if someone else didn't do it in the meantime\n            gen_idk.compare_and_swap(gen, next_gen, Ordering::Relaxed);\n        }\n        for i in 0..GEN_CNT {\n            seen_group[i] = seen_group[i] || (groups[i] == 0);\n        }\n\n        if seen_group.iter().all(|seen| *seen) {\n            break;\n        }\n\n        iter = iter.wrapping_add(1);\n        if cfg!(not(miri)) {\n            if iter % YIELD_EVERY == 0 {\n                thread::yield_now();\n            } else {\n                atomic::spin_loop_hint();\n            }\n        }\n    }\n}","Real(LocalPath(\"src/gen_lock.rs\"))"],"ptr_eq":["/// Comparison of two pointer-like things.\n#[allow(clippy::needless_pass_by_value)]\nfn ptr_eq<Base, A, B>(a: A, b: B) -> bool\nwhere\n    A: AsRaw<Base>,\n    B: AsRaw<Base>,{\n    let a = a.as_raw();\n    let b = b.as_raw();\n    ptr::eq(a, b)\n}","Real(LocalPath(\"src/lib.rs\"))"],"ref_cnt::RefCnt":["/// A trait describing smart reference counted pointers.\n///\n/// Note that in a way [`Option<Arc<T>>`][Option] is also a smart reference counted pointer, just\n/// one that can hold NULL.\n///\n/// The trait is unsafe, because a wrong implementation will break the [ArcSwapAny]\n/// implementation and lead to UB.\n///\n/// This is not actually expected for downstream crate to implement, this is just means to reuse\n/// code for [Arc] and [`Option<Arc>`][Option] variants. However, it is theoretically possible (if\n/// you have your own [Arc] implementation).\n///\n/// It is also implemented for [Rc], but that is not considered very useful (because the\n/// [ArcSwapAny] is not `Send` or `Sync`, therefore there's very little advantage for it to be\n/// atomic).\n///\n/// # Safety\n///\n/// Aside from the obvious properties (like that incrementing and decrementing a reference count\n/// cancel each out and that having less references tracked than how many things actually point to\n/// the value is fine as long as the count doesn't drop to 0), it also must satisfy that if two\n/// pointers have the same value, they point to the same object. This is specifically not true for\n/// ZSTs, but it is true for `Arc`s of ZSTs, because they have the reference counts just after the\n/// value. It would be fine to point to a type-erased version of the same object, though (if one\n/// could use this trait with unsized types in the first place).\n///\n/// Furthermore, the type should be Pin (eg. if the type is cloned or moved, it should still\n/// point/deref to the same place in memory).\n///\n/// [Arc]: std::sync::Arc\n/// [Rc]: std::rc::Rc\n/// [ArcSwapAny]: crate::ArcSwapAny\npub unsafe trait RefCnt: Clone {\n    /// The base type the pointer points to.\n    type Base;\n\n    /// Converts the smart pointer into a raw pointer, without affecting the reference count.\n    ///\n    /// This can be seen as kind of freezing the pointer ‒ it'll be later converted back using\n    /// [`from_ptr`](#method.from_ptr).\n    ///\n    /// The pointer must point to the value stored (and the value must be the same as one returned\n    /// by [`as_ptr`](#method.as_ptr).\n    fn into_ptr(me: Self) -> *mut Self::Base;\n\n    /// Provides a view into the smart pointer as a raw pointer.\n    ///\n    /// This must not affect the reference count ‒ the pointer is only borrowed.\n    fn as_ptr(me: &Self) -> *mut Self::Base;\n\n    /// Converts a raw pointer back into the smart pointer, without affecting the reference count.\n    ///\n    /// This is only called on values previously returned by [`into_ptr`](#method.into_ptr).\n    /// However, it is not guaranteed to be 1:1 relation ‒ `from_ptr` may be called more times than\n    /// `into_ptr` temporarily provided the reference count never drops under 1 during that time\n    /// (the implementation sometimes owes a reference). These extra pointers will either be\n    /// converted back using `into_ptr` or forgotten.\n    ///\n    /// # Safety\n    ///\n    /// This must not be called by code outside of this crate.\n    unsafe fn from_ptr(ptr: *const Self::Base) -> Self;\n\n    /// Increments the reference count by one.\n    ///\n    /// Return the pointer to the inner thing as a side effect.\n    fn inc(me: &Self) -> *mut Self::Base {\n        Self::into_ptr(Self::clone(me))\n    }\n\n    /// Decrements the reference count by one.\n    ///\n    /// Note this is called on a raw pointer (one previously returned by\n    /// [`into_ptr`](#method.into_ptr). This may lead to dropping of the reference count to 0 and\n    /// destruction of the internal pointer.\n    ///\n    /// # Safety\n    ///\n    /// This must not be called by code outside of this crate.\n    unsafe fn dec(ptr: *const Self::Base) {\n        drop(Self::from_ptr(ptr));\n    }\n}","Real(LocalPath(\"src/ref_cnt.rs\"))"],"ref_cnt::RefCnt::dec":["/// Decrements the reference count by one.\n///\n/// Note this is called on a raw pointer (one previously returned by\n/// [`into_ptr`](#method.into_ptr). This may lead to dropping of the reference count to 0 and\n/// destruction of the internal pointer.\n///\n/// # Safety\n///\n/// This must not be called by code outside of this crate.\nunsafe fn dec(ptr: *const Self::Base){\n        drop(Self::from_ptr(ptr));\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"ref_cnt::RefCnt::inc":["/// Increments the reference count by one.\n///\n/// Return the pointer to the inner thing as a side effect.\nfn inc(me: &Self) -> *mut Self::Base{\n        Self::into_ptr(Self::clone(me))\n    }","Real(LocalPath(\"src/ref_cnt.rs\"))"],"strategy::CaS":["/// An extension of the [`Strategy`], allowing for compare and swap operation.\n///\n/// The compare and swap operation is \"advanced\" and not all strategies need to support them.\n/// Therefore, it is a separate trait.\n///\n/// Similarly, it is not yet made publicly usable or implementable and works only as a bound.\npub trait CaS<T: RefCnt>: sealed::CaS<T> {}","Real(LocalPath(\"src/strategy/mod.rs\"))"],"strategy::Strategy":["/// A strategy for protecting the reference counted pointer `T`.\n///\n/// This chooses the algorithm for how the reference counts are protected. Note that the user of\n/// the crate can't implement the trait and can't access any method; this is hopefully temporary\n/// measure to make sure the interface is not part of the stability guarantees of the crate. Once\n/// enough experience is gained with implementing various strategies, it will be un-sealed and\n/// users will be able to provide their own implementation.\n///\n/// For now, the trait works only as a bound to talk about the types that represent strategies.\npub trait Strategy<T: RefCnt>: sealed::InnerStrategy<T> {}","Real(LocalPath(\"src/strategy/mod.rs\"))"],"strategy::gen_lock::GenLockStrategy":["pub struct GenLockStrategy<L>(pub(crate) L);","Real(LocalPath(\"src/strategy/gen_lock.rs\"))"],"strategy::hybrid::HybridProtection":["pub struct HybridProtection<T: RefCnt> {\n    debt: Option<&'static Debt>,\n    ptr: ManuallyDrop<T>,\n}","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"strategy::hybrid::HybridProtection::<T>::attempt":["#[inline]\nfn attempt(storage: &AtomicPtr<T::Base>) -> Option<Self>{\n        // Relaxed is good enough here, see the Acquire below\n        let ptr = storage.load(Ordering::Relaxed);\n        // Try to get a debt slot. If not possible, fail.\n        let debt = Debt::new(ptr as usize)?;\n\n        let confirm = storage.load(Ordering::Acquire);\n        if ptr == confirm {\n            // Successfully got a debt\n            Some(unsafe { Self::new(ptr, Some(debt)) })\n        } else if debt.pay::<T>(ptr) {\n            // It changed in the meantime, we return the debt (that is on the outdated pointer,\n            // possibly destroyed) and fail.\n            None\n        } else {\n            // It changed in the meantime, but the debt for the previous pointer was already paid\n            // for by someone else, so we are fine using it.\n            Some(unsafe { Self::new(ptr, None) })\n        }\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"strategy::hybrid::HybridProtection::<T>::new":["#[inline]\nunsafe fn new(ptr: *const T::Base, debt: Option<&'static Debt>) -> Self{\n        Self {\n            debt,\n            ptr: ManuallyDrop::new(T::from_ptr(ptr)),\n        }\n    }","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"strategy::hybrid::HybridStrategy":["pub struct HybridStrategy<F> {\n    fallback: F,\n}","Real(LocalPath(\"src/strategy/hybrid.rs\"))"],"strategy::rw_lock::<impl strategy::sealed::CaS<T> for std::sync::RwLock<()>>::compare_and_swap":["unsafe fn compare_and_swap<C: AsRaw<T::Base>>(\n        &self,\n        storage: &AtomicPtr<T::Base>,\n        current: C,\n        new: T,\n    ) -> Self::Protected{\n        let _lock = self.write();\n        let cur = current.as_raw() as *mut T::Base;\n        let new = T::into_ptr(new);\n        let swapped = storage.compare_exchange(cur, new, Ordering::AcqRel, Ordering::Relaxed);\n        let old = match swapped {\n            Ok(old) => old,\n            Err(old) => old,\n        };\n        let old = T::from_ptr(old as *const T::Base);\n        if swapped.is_err() {\n            // If the new didn't go in, we need to destroy it and increment count in the old that\n            // we just duplicated\n            T::inc(&old);\n            drop(T::from_ptr(new));\n        }\n        drop(current);\n        old\n    }","Real(LocalPath(\"src/strategy/rw_lock.rs\"))"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::load":["unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> T{\n        let _guard = self.read().expect(\"We don't panic in here\");\n        let ptr = storage.load(Ordering::Acquire);\n        let ptr = T::from_ptr(ptr as *const T::Base);\n        T::inc(&ptr);\n\n        ptr\n    }","Real(LocalPath(\"src/strategy/rw_lock.rs\"))"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::wait_for_readers":["unsafe fn wait_for_readers(&self, _: *const T::Base){\n        // By acquiring the write lock, we make sure there are no read locks present across it.\n        drop(self.write().expect(\"We don't panic in here\"));\n    }","Real(LocalPath(\"src/strategy/rw_lock.rs\"))"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::from_inner":["#[inline]\nfn from_inner(ptr: T) -> Self{\n        ptr\n    }","Real(LocalPath(\"src/strategy/rw_lock.rs\"))"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::into_inner":["#[inline]\nfn into_inner(self) -> T{\n        self\n    }","Real(LocalPath(\"src/strategy/rw_lock.rs\"))"],"strategy::sealed::CaS":["pub trait CaS<T: RefCnt>: InnerStrategy<T> {\n        unsafe fn compare_and_swap<C: AsRaw<T::Base>>(\n            &self,\n            storage: &AtomicPtr<T::Base>,\n            current: C,\n            new: T,\n        ) -> Self::Protected;\n    }","Real(LocalPath(\"src/strategy/mod.rs\"))"],"strategy::sealed::InnerStrategy":["pub trait InnerStrategy<T: RefCnt> {\n        // Drop „unlocks“\n        type Protected: Protected<T>;\n        unsafe fn load(&self, storage: &AtomicPtr<T::Base>) -> Self::Protected;\n        unsafe fn wait_for_readers(&self, old: *const T::Base);\n    }","Real(LocalPath(\"src/strategy/mod.rs\"))"],"strategy::sealed::Protected":["pub trait Protected<T>: Borrow<T> {\n        fn into_inner(self) -> T;\n        fn from_inner(ptr: T) -> Self;\n    }","Real(LocalPath(\"src/strategy/mod.rs\"))"]},"struct_constructor":{"&'static debt::Debt":["new"],"&'static debt::DebtHead":["__getit"],"&'static debt::Node":["get"],"&<A as std::ops::Deref>::Target":["arc_swap"],"&<Self as gen_lock::LockStorage>::Shards":["shards"],"&<T as std::ops::Deref>::Target":["load"],"&[<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::Shard; 1]":["shards"],"&[gen_lock::Shard; _]":["shards"],"&[std::sync::atomic::AtomicUsize; _]":["borrow"],"&std::sync::atomic::AtomicUsize":["gen_idx"],"*mut <Self as ref_cnt::RefCnt>::Base":["as_ptr","inc","into_ptr"],"*mut <T as ref_cnt::RefCnt>::Base":["as_ptr","as_raw","into_ptr"],"<ArcSwapAny<T, S> as access::Access<T>>::Guard":["load"],"<ArcSwapAny<std::rc::Rc<T>, S> as access::Access<T>>::Guard":["load"],"<ArcSwapAny<std::sync::Arc<T>, S> as access::Access<T>>::Guard":["load"],"<P as access::Access<T>>::Guard":["load"],"<Self as access::Access<T>>::Guard":["load"],"<Self as strategy::sealed::InnerStrategy<T>>::Protected":["compare_and_swap","load"],"<access::Constant<T> as access::Access<T>>::Guard":["load"],"<access::Map<A, T, F> as access::Access<R>>::Guard":["load"],"<std::sync::RwLock<()> as strategy::sealed::InnerStrategy<T>>::Protected":["compare_and_swap"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::Protected":["compare_and_swap","load"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::Protected":["load"],"<strategy::hybrid::HybridStrategy<strategy::gen_lock::GenLockStrategy<L>> as strategy::sealed::InnerStrategy<T>>::Protected":["compare_and_swap"],"ArcSwapAny":["default","empty","from","from_pointee","new","with_strategy"],"Guard":["compare_and_swap","default","from","from_inner","load"],"[usize; _]":["snapshot"],"access::Constant":["clone"],"access::ConstantDeref":["clone"],"access::DynGuard":["load"],"access::Map":["clone","map","new"],"access::MapGuard":["clone"],"bool":["eq","pay","ptr_eq"],"cache::Cache":["clone","from","new"],"cache::MapCache":["clone","map"],"debt::Debt":["default"],"debt::DebtHead":["__init"],"debt::Node":["default"],"debt::Slots":["default"],"gen_lock::GenLock":["new"],"gen_lock::Global":["clone","default"],"gen_lock::PrivateUnsharded":["default"],"gen_lock::Shard":["default"],"std::cell::Cell":["__init"],"std::cmp::Ordering":["cmp","partial_cmp"],"std::rc::Rc":["from_ptr"],"std::sync::Arc":["from_ptr"],"strategy::gen_lock::GenLockStrategy":["clone","default"],"strategy::hybrid::HybridProtection":["attempt","from_inner","new"],"strategy::hybrid::HybridStrategy":["clone","default"],"usize":["choose_shard"]},"struct_to_trait":{"<A as access::DynAccess<T>>::A":["access::DynAccess"],"<P as access::Access<T>>::P":["access::Access"],"<S as strategy::CaS<T>>::S":["strategy::CaS"],"<S as strategy::Strategy<T>>::S":["strategy::Strategy"],"ArcSwapAny":["access::Access","std::convert::From","std::default::Default","std::fmt::Debug","std::fmt::Display","std::ops::Drop"],"Guard":["as_raw::AsRaw","as_raw::sealed::Sealed","std::convert::From","std::default::Default","std::fmt::Debug","std::fmt::Display","std::ops::Deref"],"access::Constant":["access::Access","std::clone::Clone","std::cmp::Eq","std::cmp::Ord","std::cmp::PartialEq","std::cmp::PartialOrd","std::fmt::Debug","std::hash::Hash","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq"],"access::ConstantDeref":["std::clone::Clone","std::cmp::Eq","std::cmp::Ord","std::cmp::PartialEq","std::cmp::PartialOrd","std::fmt::Debug","std::hash::Hash","std::marker::Copy","std::marker::StructuralEq","std::marker::StructuralPartialEq","std::ops::Deref"],"access::DirectDeref":["std::fmt::Debug","std::ops::Deref"],"access::DynGuard":["std::ops::Deref"],"access::Map":["access::Access","std::clone::Clone","std::fmt::Debug","std::marker::Copy"],"access::MapGuard":["std::clone::Clone","std::fmt::Debug","std::marker::Copy","std::marker::Send","std::marker::Sync","std::ops::Deref"],"cache::Cache":["cache::Access","std::clone::Clone","std::convert::From","std::fmt::Debug"],"cache::MapCache":["cache::Access","std::clone::Clone","std::fmt::Debug"],"debt::Debt":["std::default::Default"],"debt::DebtHead":["std::ops::Drop"],"debt::Node":["std::default::Default"],"debt::Slots":["std::default::Default"],"gen_lock::GenLock":["std::ops::Drop"],"gen_lock::Global":["gen_lock::LockStorage","std::clone::Clone","std::default::Default","std::marker::Copy"],"gen_lock::PrivateUnsharded":["gen_lock::LockStorage","std::default::Default"],"gen_lock::Shard":["std::borrow::Borrow","std::default::Default"],"std::option::Option":["ref_cnt::RefCnt"],"std::rc::Rc":["ref_cnt::RefCnt"],"std::sync::Arc":["ref_cnt::RefCnt"],"std::sync::RwLock":["strategy::sealed::CaS","strategy::sealed::InnerStrategy"],"strategy::gen_lock::GenLockStrategy":["std::clone::Clone","std::default::Default","std::marker::Copy","strategy::sealed::CaS","strategy::sealed::InnerStrategy"],"strategy::hybrid::HybridProtection":["std::borrow::Borrow","std::ops::Drop","strategy::sealed::Protected"],"strategy::hybrid::HybridStrategy":["std::clone::Clone","std::default::Default","strategy::sealed::CaS","strategy::sealed::InnerStrategy"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::T":["strategy::sealed::Protected"]},"targets":{"<&'a Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["as_raw","Real(LocalPath(\"src/as_raw.rs\"))","as_raw::AsRaw"],"<&'a T as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["as_raw","Real(LocalPath(\"src/as_raw.rs\"))","as_raw::AsRaw"],"<*const T as as_raw::AsRaw<T>>::as_raw":["as_raw","Real(LocalPath(\"src/as_raw.rs\"))","as_raw::AsRaw"],"<*mut T as as_raw::AsRaw<T>>::as_raw":["as_raw","Real(LocalPath(\"src/as_raw.rs\"))","as_raw::AsRaw"],"<A as access::DynAccess<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::DynAccess"],"<ArcSwapAny<T, S> as access::Access<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<ArcSwapAny<T, S> as std::convert::From<T>>::from":["from","Real(LocalPath(\"src/lib.rs\"))","std::convert::From"],"<ArcSwapAny<T, S> as std::default::Default>::default":["default","Real(LocalPath(\"src/lib.rs\"))","std::default::Default"],"<ArcSwapAny<T, S> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"src/lib.rs\"))","std::fmt::Debug"],"<ArcSwapAny<T, S> as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"src/lib.rs\"))","std::fmt::Display"],"<ArcSwapAny<T, S> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"src/lib.rs\"))","std::ops::Drop"],"<ArcSwapAny<std::rc::Rc<T>, S> as access::Access<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<ArcSwapAny<std::sync::Arc<T>, S> as access::Access<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<Guard<T, S> as std::convert::From<T>>::from":["from","Real(LocalPath(\"src/lib.rs\"))","std::convert::From"],"<Guard<T, S> as std::default::Default>::default":["default","Real(LocalPath(\"src/lib.rs\"))","std::default::Default"],"<Guard<T, S> as std::fmt::Debug>::fmt":["fmt","Real(LocalPath(\"src/lib.rs\"))","std::fmt::Debug"],"<Guard<T, S> as std::fmt::Display>::fmt":["fmt","Real(LocalPath(\"src/lib.rs\"))","std::fmt::Display"],"<Guard<T, S> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/lib.rs\"))","std::ops::Deref"],"<Guard<T> as as_raw::AsRaw<<T as ref_cnt::RefCnt>::Base>>::as_raw":["as_raw","Real(LocalPath(\"src/as_raw.rs\"))","as_raw::AsRaw"],"<P as access::Access<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<access::Constant<T> as access::Access<T>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<access::ConstantDeref<T> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/access.rs\"))","std::ops::Deref"],"<access::DirectDeref<std::rc::Rc<T>, S> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/access.rs\"))","std::ops::Deref"],"<access::DirectDeref<std::sync::Arc<T>, S> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/access.rs\"))","std::ops::Deref"],"<access::DynGuard<T> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/access.rs\"))","std::ops::Deref"],"<access::Map<A, T, F> as access::Access<R>>::load":["load","Real(LocalPath(\"src/access.rs\"))","access::Access"],"<access::MapGuard<G, T> as std::ops::Deref>::deref":["deref","Real(LocalPath(\"src/access.rs\"))","std::ops::Deref"],"<cache::Cache<A, T> as cache::Access<<T as std::ops::Deref>::Target>>::load":["load","Real(LocalPath(\"src/cache.rs\"))","cache::Access"],"<cache::Cache<A, T> as std::convert::From<A>>::from":["from","Real(LocalPath(\"src/cache.rs\"))","std::convert::From"],"<cache::MapCache<A, T, F> as cache::Access<U>>::load":["load","Real(LocalPath(\"src/cache.rs\"))","cache::Access"],"<debt::Debt as std::default::Default>::default":["default","Real(LocalPath(\"src/debt.rs\"))","std::default::Default"],"<debt::DebtHead as std::ops::Drop>::drop":["drop","Real(LocalPath(\"src/debt.rs\"))","std::ops::Drop"],"<debt::Node as std::default::Default>::default":["default","Real(LocalPath(\"src/debt.rs\"))","std::default::Default"],"<gen_lock::GenLock<'_> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"src/gen_lock.rs\"))","std::ops::Drop"],"<gen_lock::Global as gen_lock::LockStorage>::choose_shard":["choose_shard","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::Global as gen_lock::LockStorage>::gen_idx":["gen_idx","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::Global as gen_lock::LockStorage>::shards":["shards","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::choose_shard":["choose_shard","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::gen_idx":["gen_idx","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::PrivateUnsharded as gen_lock::LockStorage>::shards":["shards","Real(LocalPath(\"src/gen_lock.rs\"))","gen_lock::LockStorage"],"<gen_lock::Shard as std::borrow::Borrow<[std::sync::atomic::AtomicUsize; _]>>::borrow":["borrow","Real(LocalPath(\"src/gen_lock.rs\"))","std::borrow::Borrow"],"<std::option::Option<T> as ref_cnt::RefCnt>::as_ptr":["as_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::option::Option<T> as ref_cnt::RefCnt>::from_ptr":["from_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::option::Option<T> as ref_cnt::RefCnt>::into_ptr":["into_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::as_ptr":["as_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::from_ptr":["from_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::rc::Rc<T> as ref_cnt::RefCnt>::into_ptr":["into_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::as_ptr":["as_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::from_ptr":["from_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<std::sync::Arc<T> as ref_cnt::RefCnt>::into_ptr":["into_ptr","Real(LocalPath(\"src/ref_cnt.rs\"))","ref_cnt::RefCnt"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::CaS<T>>::compare_and_swap":["compare_and_swap","Real(LocalPath(\"src/strategy/gen_lock.rs\"))","strategy::sealed::CaS"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::load":["load","Real(LocalPath(\"src/strategy/gen_lock.rs\"))","strategy::sealed::InnerStrategy"],"<strategy::gen_lock::GenLockStrategy<L> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["wait_for_readers","Real(LocalPath(\"src/strategy/gen_lock.rs\"))","strategy::sealed::InnerStrategy"],"<strategy::hybrid::HybridProtection<T> as std::borrow::Borrow<T>>::borrow":["borrow","Real(LocalPath(\"src/strategy/hybrid.rs\"))","std::borrow::Borrow"],"<strategy::hybrid::HybridProtection<T> as std::ops::Drop>::drop":["drop","Real(LocalPath(\"src/strategy/hybrid.rs\"))","std::ops::Drop"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::from_inner":["from_inner","Real(LocalPath(\"src/strategy/hybrid.rs\"))","strategy::sealed::Protected"],"<strategy::hybrid::HybridProtection<T> as strategy::sealed::Protected<T>>::into_inner":["into_inner","Real(LocalPath(\"src/strategy/hybrid.rs\"))","strategy::sealed::Protected"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::load":["load","Real(LocalPath(\"src/strategy/hybrid.rs\"))","strategy::sealed::InnerStrategy"],"<strategy::hybrid::HybridStrategy<F> as strategy::sealed::InnerStrategy<T>>::wait_for_readers":["wait_for_readers","Real(LocalPath(\"src/strategy/hybrid.rs\"))","strategy::sealed::InnerStrategy"],"<strategy::hybrid::HybridStrategy<strategy::gen_lock::GenLockStrategy<L>> as strategy::sealed::CaS<T>>::compare_and_swap":["compare_and_swap","Real(LocalPath(\"src/strategy/hybrid.rs\"))","strategy::sealed::CaS"],"ArcSwapAny::<T, S>::compare_and_swap":["compare_and_swap","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::into_inner":["into_inner","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::load":["load","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::load_full":["load_full","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::map":["map","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::new":["new","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::rcu":["rcu","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::store":["store","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::swap":["swap","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<T, S>::with_strategy":["with_strategy","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::empty":["empty","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<std::option::Option<std::sync::Arc<T>>, S>::from_pointee":["from_pointee","Real(LocalPath(\"src/lib.rs\"))",""],"ArcSwapAny::<std::sync::Arc<T>, S>::from_pointee":["from_pointee","Real(LocalPath(\"src/lib.rs\"))",""],"Guard::<T, S>::from_inner":["from_inner","Real(LocalPath(\"src/lib.rs\"))",""],"Guard::<T, S>::into_inner":["into_inner","Real(LocalPath(\"src/lib.rs\"))",""],"access::Map::<A, T, F>::new":["new","Real(LocalPath(\"src/access.rs\"))",""],"cache::Cache::<A, T>::arc_swap":["arc_swap","Real(LocalPath(\"src/cache.rs\"))",""],"cache::Cache::<A, T>::load":["load","Real(LocalPath(\"src/cache.rs\"))",""],"cache::Cache::<A, T>::load_no_revalidate":["load_no_revalidate","Real(LocalPath(\"src/cache.rs\"))",""],"cache::Cache::<A, T>::map":["map","Real(LocalPath(\"src/cache.rs\"))",""],"cache::Cache::<A, T>::new":["new","Real(LocalPath(\"src/cache.rs\"))",""],"cache::Cache::<A, T>::revalidate":["revalidate","Real(LocalPath(\"src/cache.rs\"))",""],"debt::Debt::new":["new","Real(LocalPath(\"src/debt.rs\"))",""],"debt::Debt::pay":["pay","Real(LocalPath(\"src/debt.rs\"))",""],"debt::Debt::pay_all":["pay_all","Real(LocalPath(\"src/debt.rs\"))",""],"debt::Node::get":["get","Real(LocalPath(\"src/debt.rs\"))",""],"debt::traverse":["traverse","Real(LocalPath(\"src/debt.rs\"))",""],"gen_lock::GenLock::<'a>::new":["new","Real(LocalPath(\"src/gen_lock.rs\"))",""],"gen_lock::snapshot":["snapshot","Real(LocalPath(\"src/gen_lock.rs\"))",""],"gen_lock::wait_for_readers":["wait_for_readers","Real(LocalPath(\"src/gen_lock.rs\"))",""],"ptr_eq":["ptr_eq","Real(LocalPath(\"src/lib.rs\"))",""],"ref_cnt::RefCnt::dec":["dec","Real(LocalPath(\"src/ref_cnt.rs\"))",""],"ref_cnt::RefCnt::inc":["inc","Real(LocalPath(\"src/ref_cnt.rs\"))",""],"strategy::hybrid::HybridProtection::<T>::attempt":["attempt","Real(LocalPath(\"src/strategy/hybrid.rs\"))",""],"strategy::hybrid::HybridProtection::<T>::new":["new","Real(LocalPath(\"src/strategy/hybrid.rs\"))",""],"strategy::rw_lock::<impl strategy::sealed::CaS<T> for std::sync::RwLock<()>>::compare_and_swap":["compare_and_swap","Real(LocalPath(\"src/strategy/rw_lock.rs\"))","strategy::sealed::CaS"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::load":["load","Real(LocalPath(\"src/strategy/rw_lock.rs\"))","strategy::sealed::InnerStrategy"],"strategy::rw_lock::<impl strategy::sealed::InnerStrategy<T> for std::sync::RwLock<()>>::wait_for_readers":["wait_for_readers","Real(LocalPath(\"src/strategy/rw_lock.rs\"))","strategy::sealed::InnerStrategy"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::from_inner":["from_inner","Real(LocalPath(\"src/strategy/rw_lock.rs\"))","strategy::sealed::Protected"],"strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::into_inner":["into_inner","Real(LocalPath(\"src/strategy/rw_lock.rs\"))","strategy::sealed::Protected"]},"trait_to_struct":{"access::Access":["<P as access::Access<T>>::P","ArcSwapAny","access::Constant","access::Map"],"access::DynAccess":["<A as access::DynAccess<T>>::A"],"as_raw::AsRaw":["Guard"],"as_raw::sealed::Sealed":["Guard"],"cache::Access":["cache::Cache","cache::MapCache"],"gen_lock::LockStorage":["gen_lock::Global","gen_lock::PrivateUnsharded"],"ref_cnt::RefCnt":["std::option::Option","std::rc::Rc","std::sync::Arc"],"std::borrow::Borrow":["gen_lock::Shard","strategy::hybrid::HybridProtection"],"std::clone::Clone":["access::Constant","access::ConstantDeref","access::Map","access::MapGuard","cache::Cache","cache::MapCache","gen_lock::Global","strategy::gen_lock::GenLockStrategy","strategy::hybrid::HybridStrategy"],"std::cmp::Eq":["access::Constant","access::ConstantDeref"],"std::cmp::Ord":["access::Constant","access::ConstantDeref"],"std::cmp::PartialEq":["access::Constant","access::ConstantDeref"],"std::cmp::PartialOrd":["access::Constant","access::ConstantDeref"],"std::convert::From":["ArcSwapAny","Guard","cache::Cache"],"std::default::Default":["ArcSwapAny","Guard","debt::Debt","debt::Node","debt::Slots","gen_lock::Global","gen_lock::PrivateUnsharded","gen_lock::Shard","strategy::gen_lock::GenLockStrategy","strategy::hybrid::HybridStrategy"],"std::fmt::Debug":["ArcSwapAny","Guard","access::Constant","access::ConstantDeref","access::DirectDeref","access::Map","access::MapGuard","cache::Cache","cache::MapCache"],"std::fmt::Display":["ArcSwapAny","Guard"],"std::hash::Hash":["access::Constant","access::ConstantDeref"],"std::marker::Copy":["access::Constant","access::ConstantDeref","access::Map","access::MapGuard","gen_lock::Global","strategy::gen_lock::GenLockStrategy"],"std::marker::Send":["access::MapGuard"],"std::marker::StructuralEq":["access::Constant","access::ConstantDeref"],"std::marker::StructuralPartialEq":["access::Constant","access::ConstantDeref"],"std::marker::Sync":["access::MapGuard"],"std::ops::Deref":["Guard","access::ConstantDeref","access::DirectDeref","access::DynGuard","access::MapGuard"],"std::ops::Drop":["ArcSwapAny","debt::DebtHead","gen_lock::GenLock","strategy::hybrid::HybridProtection"],"strategy::CaS":["<S as strategy::CaS<T>>::S"],"strategy::Strategy":["<S as strategy::Strategy<T>>::S"],"strategy::sealed::CaS":["std::sync::RwLock","strategy::gen_lock::GenLockStrategy","strategy::hybrid::HybridStrategy"],"strategy::sealed::InnerStrategy":["std::sync::RwLock","strategy::gen_lock::GenLockStrategy","strategy::hybrid::HybridStrategy"],"strategy::sealed::Protected":["strategy::hybrid::HybridProtection","strategy::rw_lock::<impl strategy::sealed::Protected<T> for T>::T"]},"type_to_def_path":{"ArcSwapAny<T, S>":"ArcSwapAny","Guard<T, S>":"Guard","access::Constant<T>":"access::Constant","access::ConstantDeref<T>":"access::ConstantDeref","access::DirectDeref<T, S>":"access::DirectDeref","access::DynGuard<T>":"access::DynGuard","access::Map<A, T, F>":"access::Map","access::MapGuard<G, T>":"access::MapGuard","cache::Cache<A, T>":"cache::Cache","cache::MapCache<A, T, F>":"cache::MapCache","debt::Debt":"debt::Debt","debt::DebtHead":"debt::DebtHead","debt::Node":"debt::Node","debt::Slots":"debt::Slots","gen_lock::GenLock<'a>":"gen_lock::GenLock","gen_lock::Global":"gen_lock::Global","gen_lock::PrivateUnsharded":"gen_lock::PrivateUnsharded","gen_lock::Shard":"gen_lock::Shard","strategy::gen_lock::GenLockStrategy<L>":"strategy::gen_lock::GenLockStrategy","strategy::hybrid::HybridProtection<T>":"strategy::hybrid::HybridProtection","strategy::hybrid::HybridStrategy<F>":"strategy::hybrid::HybridStrategy"}}