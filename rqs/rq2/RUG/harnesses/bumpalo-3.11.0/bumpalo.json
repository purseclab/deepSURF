{"dependencies":{"<&'a Bump as alloc::Alloc>::alloc":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::result::Result"],"<&'a Bump as alloc::Alloc>::dealloc":["Bump","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"<&'a Bump as alloc::Alloc>::realloc":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"<AllocOrInitError<E> as core::clone::Clone>::clone":["AllocOrInitError","alloc::AllocErr","core::marker::Sized"],"<AllocOrInitError<E> as core::cmp::Eq>::assert_receiver_is_total_eq":["AllocOrInitError","alloc::AllocErr","core::marker::Sized"],"<AllocOrInitError<E> as core::cmp::PartialEq>::eq":["AllocOrInitError","alloc::AllocErr","core::marker::Sized"],"<AllocOrInitError<E> as core::convert::From<alloc::AllocErr>>::from":["AllocOrInitError","alloc::AllocErr","core::marker::Sized"],"<AllocOrInitError<E> as core::fmt::Debug>::fmt":["AllocOrInitError","alloc::AllocErr","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<AllocOrInitError<E> as core::fmt::Display>::fmt":["AllocOrInitError","alloc::AllocErr","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<Bump as core::default::Default>::default":["Bump","core::cell::Cell"],"<Bump as core::fmt::Debug>::fmt":["Bump","core::cell::Cell","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<Bump as core::ops::Drop>::drop":["Bump","core::cell::Cell"],"<ChunkFooter as core::fmt::Debug>::fmt":["ChunkFooter","core::alloc::Layout","core::cell::Cell","core::fmt::Formatter","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"<ChunkIter<'a> as core::fmt::Debug>::fmt":["ChunkIter","ChunkRawIter","core::fmt::Formatter","core::marker::PhantomData","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"<ChunkIter<'a> as core::iter::Iterator>::next":["ChunkIter","ChunkRawIter","core::marker::PhantomData","core::marker::Sized","core::option::Option","core::ptr::NonNull"],"<ChunkRawIter<'_> as core::iter::Iterator>::next":["ChunkRawIter","core::marker::PhantomData","core::marker::Sized","core::option::Option","core::ptr::NonNull"],"<ChunkRawIter<'a> as core::fmt::Debug>::fmt":["ChunkRawIter","core::fmt::Formatter","core::marker::PhantomData","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"<NewChunkMemoryDetails as core::clone::Clone>::clone":["NewChunkMemoryDetails"],"<NewChunkMemoryDetails as core::fmt::Debug>::fmt":["NewChunkMemoryDetails","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<alloc::AllocErr as core::clone::Clone>::clone":["alloc::AllocErr"],"<alloc::AllocErr as core::cmp::Eq>::assert_receiver_is_total_eq":["alloc::AllocErr"],"<alloc::AllocErr as core::cmp::PartialEq>::eq":["alloc::AllocErr"],"<alloc::AllocErr as core::fmt::Debug>::fmt":["alloc::AllocErr","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<alloc::AllocErr as core::fmt::Display>::fmt":["alloc::AllocErr","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<alloc::CannotReallocInPlace as core::clone::Clone>::clone":["alloc::CannotReallocInPlace"],"<alloc::CannotReallocInPlace as core::cmp::Eq>::assert_receiver_is_total_eq":["alloc::CannotReallocInPlace"],"<alloc::CannotReallocInPlace as core::cmp::PartialEq>::eq":["alloc::CannotReallocInPlace"],"<alloc::CannotReallocInPlace as core::fmt::Debug>::fmt":["alloc::CannotReallocInPlace","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<alloc::CannotReallocInPlace as core::fmt::Display>::fmt":["alloc::CannotReallocInPlace","core::fmt::Formatter","core::marker::Sized","core::result::Result"],"<alloc::Excess as core::fmt::Debug>::fmt":["alloc::Excess","core::fmt::Formatter","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::array":["core::marker::Sized","core::result::Result"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::padding_needed_for":["core::alloc::Layout"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::repeat":["core::alloc::Layout","core::marker::Sized","core::result::Result"],"AllocOrInitError":["AllocOrInitError","alloc::AllocErr","core::marker::Sized"],"Bump":["Bump","core::cell::Cell"],"Bump::alloc":["Bump","core::cell::Cell","core::marker::Sized"],"Bump::alloc_layout":["Bump","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"Bump::alloc_layout_slow":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::option::Option"],"Bump::alloc_slice_clone":["AllocOrInitError","Bump","alloc::AllocErr","core::cell::Cell","core::clone::Clone","core::marker::Sized"],"Bump::alloc_slice_copy":["Bump","NewChunkMemoryDetails","core::cell::Cell","core::marker::Copy","core::marker::Sized"],"Bump::alloc_slice_fill_clone":["AllocOrInitError","Bump","alloc::AllocErr","core::cell::Cell","core::clone::Clone","core::marker::Sized"],"Bump::alloc_slice_fill_copy":["Bump","NewChunkMemoryDetails","core::cell::Cell","core::marker::Copy","core::marker::Sized"],"Bump::alloc_slice_fill_default":["Bump","core::cell::Cell","core::default::Default","core::marker::Sized"],"Bump::alloc_slice_fill_iter":["Bump","core::cell::Cell","core::iter::ExactSizeIterator","core::iter::IntoIterator","core::marker::Sized"],"Bump::alloc_slice_fill_with":["Bump","core::cell::Cell","core::marker::Sized","core::ops::FnMut"],"Bump::alloc_str":["Bump","core::cell::Cell"],"Bump::alloc_try_with":["Bump","core::cell::Cell","core::marker::Sized","core::ops::FnOnce","core::result::Result"],"Bump::alloc_with":["Bump","core::cell::Cell","core::marker::Sized","core::ops::FnOnce"],"Bump::alloc_with::inner_writer":["core::marker::Sized","core::ops::FnOnce"],"Bump::allocated_bytes":["Bump","core::cell::Cell"],"Bump::allocation_limit":["Bump","core::cell::Cell","core::marker::Sized","core::option::Option"],"Bump::allocation_limit_remaining":["Bump","core::cell::Cell","core::marker::Sized","core::option::Option"],"Bump::chunk_capacity":["Bump","core::cell::Cell"],"Bump::chunk_fits_under_limit":["NewChunkMemoryDetails","core::marker::Sized","core::option::Option"],"Bump::dealloc":["Bump","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"Bump::grow":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"Bump::is_last_allocation":["Bump","core::cell::Cell","core::ptr::NonNull"],"Bump::iter_allocated_chunks":["Bump","ChunkIter","ChunkRawIter","core::cell::Cell","core::marker::PhantomData","core::ptr::NonNull"],"Bump::iter_allocated_chunks_raw":["Bump","ChunkRawIter","core::cell::Cell","core::marker::PhantomData","core::ptr::NonNull"],"Bump::new":["Bump","core::cell::Cell"],"Bump::new_chunk":["NewChunkMemoryDetails","core::alloc::Layout","core::marker::Sized","core::option::Option","core::ptr::NonNull"],"Bump::new_chunk_memory_details":["core::alloc::Layout","core::marker::Sized","core::option::Option"],"Bump::reset":["Bump","core::cell::Cell"],"Bump::set_allocation_limit":["Bump","core::cell::Cell","core::marker::Sized","core::option::Option"],"Bump::shrink":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"Bump::try_alloc":["Bump","core::cell::Cell","core::marker::Sized","core::result::Result"],"Bump::try_alloc_layout":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::result::Result"],"Bump::try_alloc_layout_fast":["Bump","core::alloc::Layout","core::cell::Cell","core::marker::Sized","core::option::Option"],"Bump::try_alloc_try_with":["Bump","core::cell::Cell","core::marker::Sized","core::ops::FnOnce","core::result::Result"],"Bump::try_alloc_with":["Bump","core::cell::Cell","core::marker::Sized","core::ops::FnOnce","core::result::Result"],"Bump::try_alloc_with::inner_writer":["core::marker::Sized","core::ops::FnOnce"],"Bump::try_new":["core::marker::Sized","core::result::Result"],"Bump::try_with_capacity":["core::marker::Sized","core::result::Result"],"Bump::with_capacity":["Bump","core::cell::Cell"],"ChunkFooter":["ChunkFooter","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"ChunkFooter::as_raw_parts":["ChunkFooter","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"ChunkFooter::is_empty":["ChunkFooter","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"ChunkIter":["ChunkIter","ChunkRawIter","core::marker::PhantomData","core::ptr::NonNull"],"ChunkRawIter":["ChunkRawIter","core::marker::PhantomData","core::ptr::NonNull"],"EmptyChunkFooter":["ChunkFooter","EmptyChunkFooter","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"EmptyChunkFooter::get":["ChunkFooter","EmptyChunkFooter","core::alloc::Layout","core::cell::Cell","core::ptr::NonNull"],"NewChunkMemoryDetails":["NewChunkMemoryDetails"],"abs_diff":[],"alloc::Alloc::alloc":["core::alloc::Layout","core::marker::Sized","core::result::Result"],"alloc::Alloc::alloc_array":["core::marker::Sized","core::result::Result"],"alloc::Alloc::alloc_excess":["core::alloc::Layout","core::marker::Sized","core::result::Result"],"alloc::Alloc::alloc_one":["core::marker::Sized","core::result::Result"],"alloc::Alloc::alloc_zeroed":["core::alloc::Layout","core::marker::Sized","core::result::Result"],"alloc::Alloc::dealloc":["core::alloc::Layout","core::ptr::NonNull"],"alloc::Alloc::dealloc_array":["core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::dealloc_one":["core::marker::Sized","core::ptr::NonNull"],"alloc::Alloc::grow_in_place":["core::alloc::Layout","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::realloc":["core::alloc::Layout","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::realloc_array":["core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::realloc_excess":["core::alloc::Layout","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::shrink_in_place":["core::alloc::Layout","core::marker::Sized","core::ptr::NonNull","core::result::Result"],"alloc::Alloc::usable_size":["core::alloc::Layout"],"alloc::AllocErr":["alloc::AllocErr"],"alloc::CannotReallocInPlace":["alloc::CannotReallocInPlace"],"alloc::CannotReallocInPlace::description":["alloc::CannotReallocInPlace"],"alloc::Excess":["alloc::Excess","core::ptr::NonNull"],"alloc::UnstableLayoutMethods::array":["core::marker::Sized","core::result::Result"],"alloc::UnstableLayoutMethods::padding_needed_for":[],"alloc::UnstableLayoutMethods::repeat":["core::marker::Sized","core::result::Result"],"alloc::handle_alloc_error":["core::alloc::Layout"],"alloc::new_layout_err":["core::alloc::LayoutError"],"alloc::size_align":["core::marker::Sized"],"allocation_size_overflow":["core::marker::Sized"],"dealloc_chunk_list":["core::ptr::NonNull"],"layout_from_size_align":["core::alloc::Layout"],"oom":[],"round_down_to":[],"round_up_to":["core::marker::Sized","core::option::Option"]},"glob_path_import":{},"self_to_fn":{"AllocOrInitError":["Clone","Debug","Eq","PartialEq","impl<E: Display> Display for AllocOrInitError<E> {\n    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {\n        match self {\n            AllocOrInitError::Alloc(err) => err.fmt(f),\n            AllocOrInitError::Init(err) => write!(f, \"initialization failed: {}\", err),\n        }\n    }\n}","impl<E> From<AllocErr> for AllocOrInitError<E> {\n    fn from(e: AllocErr) -> Self {\n        Self::Alloc(e)\n    }\n}"],"Bump":["Debug","impl Bump {\n    /// Construct a new arena to bump allocate into.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// # let _ = bump;\n    /// ```\n    pub fn new() -> Bump {\n        Self::with_capacity(0)\n    }\n\n    /// Attempt to construct a new arena to bump allocate into.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::try_new();\n    /// # let _ = bump.unwrap();\n    /// ```\n    pub fn try_new() -> Result<Bump, AllocErr> {\n        Bump::try_with_capacity(0)\n    }\n\n    /// Construct a new arena with the specified byte capacity to bump allocate into.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::with_capacity(100);\n    /// # let _ = bump;\n    /// ```\n    pub fn with_capacity(capacity: usize) -> Bump {\n        Bump::try_with_capacity(capacity).unwrap_or_else(|_| oom())\n    }\n\n    /// Attempt to construct a new arena with the specified byte capacity to bump allocate into.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::try_with_capacity(100);\n    /// # let _ = bump.unwrap();\n    /// ```\n    pub fn try_with_capacity(capacity: usize) -> Result<Self, AllocErr> {\n        if capacity == 0 {\n            return Ok(Bump {\n                current_chunk_footer: Cell::new(EMPTY_CHUNK.get()),\n                allocation_limit: Cell::new(None),\n            });\n        }\n\n        let layout = unsafe { layout_from_size_align(capacity, 1) };\n\n        let chunk_footer = unsafe {\n            Self::new_chunk(\n                Bump::new_chunk_memory_details(None, layout).ok_or(AllocErr)?,\n                layout,\n                EMPTY_CHUNK.get(),\n            )\n            .ok_or(AllocErr)?\n        };\n\n        Ok(Bump {\n            current_chunk_footer: Cell::new(chunk_footer),\n            allocation_limit: Cell::new(None),\n        })\n    }\n\n    /// The allocation limit for this arena in bytes.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::with_capacity(0);\n    ///\n    /// assert_eq!(bump.allocation_limit(), None);\n    ///\n    /// bump.set_allocation_limit(Some(6));\n    ///\n    /// assert_eq!(bump.allocation_limit(), Some(6));\n    ///\n    /// bump.set_allocation_limit(None);\n    ///\n    /// assert_eq!(bump.allocation_limit(), None);\n    /// ```\n    pub fn allocation_limit(&self) -> Option<usize> {\n        self.allocation_limit.get()\n    }\n\n    /// Set the allocation limit in bytes for this arena.\n    ///\n    /// The allocation limit is only enforced when allocating new backing chunks for\n    /// a `Bump`. Updating the allocation limit will not affect existing allocations\n    /// or any future allocations within the `Bump`'s current chunk.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::with_capacity(0);\n    ///\n    /// bump.set_allocation_limit(Some(0));\n    ///\n    /// assert!(bump.try_alloc(5).is_err());\n    /// ```\n    pub fn set_allocation_limit(&self, limit: Option<usize>) {\n        self.allocation_limit.set(limit)\n    }\n\n    /// How much headroom an arena has before it hits its allocation\n    /// limit.\n    fn allocation_limit_remaining(&self) -> Option<usize> {\n        self.allocation_limit.get().and_then(|allocation_limit| {\n            let allocated_bytes = self.allocated_bytes();\n            if allocated_bytes > allocation_limit {\n                None\n            } else {\n                Some(abs_diff(allocation_limit, allocated_bytes))\n            }\n        })\n    }\n\n    /// Whether a request to allocate a new chunk with a given size for a given\n    /// requested layout will fit under the allocation limit set on a `Bump`.\n    fn chunk_fits_under_limit(\n        allocation_limit_remaining: Option<usize>,\n        new_chunk_memory_details: NewChunkMemoryDetails,\n    ) -> bool {\n        allocation_limit_remaining\n            .map(|allocation_limit_left| {\n                allocation_limit_left >= new_chunk_memory_details.new_size_without_footer\n            })\n            .unwrap_or(true)\n    }\n\n    /// Determine the memory details including final size, alignment and\n    /// final size without footer for a new chunk that would be allocated\n    /// to fulfill an allocation request.\n    fn new_chunk_memory_details(\n        new_size_without_footer: Option<usize>,\n        requested_layout: Layout,\n    ) -> Option<NewChunkMemoryDetails> {\n        let mut new_size_without_footer =\n            new_size_without_footer.unwrap_or(DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER);\n\n        // We want to have CHUNK_ALIGN or better alignment\n        let mut align = CHUNK_ALIGN;\n\n        // If we already know we need to fulfill some request,\n        // make sure we allocate at least enough to satisfy it\n        align = align.max(requested_layout.align());\n        let requested_size =\n            round_up_to(requested_layout.size(), align).unwrap_or_else(allocation_size_overflow);\n        new_size_without_footer = new_size_without_footer.max(requested_size);\n\n        // We want our allocations to play nice with the memory allocator,\n        // and waste as little memory as possible.\n        // For small allocations, this means that the entire allocation\n        // including the chunk footer and mallocs internal overhead is\n        // as close to a power of two as we can go without going over.\n        // For larger allocations, we only need to get close to a page\n        // boundary without going over.\n        if new_size_without_footer < PAGE_STRATEGY_CUTOFF {\n            new_size_without_footer =\n                (new_size_without_footer + OVERHEAD).next_power_of_two() - OVERHEAD;\n        } else {\n            new_size_without_footer =\n                round_up_to(new_size_without_footer + OVERHEAD, 0x1000)? - OVERHEAD;\n        }\n\n        debug_assert_eq!(align % CHUNK_ALIGN, 0);\n        debug_assert_eq!(new_size_without_footer % CHUNK_ALIGN, 0);\n        let size = new_size_without_footer\n            .checked_add(FOOTER_SIZE)\n            .unwrap_or_else(allocation_size_overflow);\n\n        Some(NewChunkMemoryDetails {\n            new_size_without_footer,\n            size,\n            align,\n        })\n    }\n\n    /// Allocate a new chunk and return its initialized footer.\n    ///\n    /// If given, `layouts` is a tuple of the current chunk size and the\n    /// layout of the allocation request that triggered us to fall back to\n    /// allocating a new chunk of memory.\n    unsafe fn new_chunk(\n        new_chunk_memory_details: NewChunkMemoryDetails,\n        requested_layout: Layout,\n        prev: NonNull<ChunkFooter>,\n    ) -> Option<NonNull<ChunkFooter>> {\n        let NewChunkMemoryDetails {\n            new_size_without_footer,\n            align,\n            size,\n        } = new_chunk_memory_details;\n\n        let layout = layout_from_size_align(size, align);\n\n        debug_assert!(size >= requested_layout.size());\n\n        let data = alloc(layout);\n        let data = NonNull::new(data)?;\n\n        // The `ChunkFooter` is at the end of the chunk.\n        let footer_ptr = data.as_ptr().add(new_size_without_footer);\n        debug_assert_eq!((data.as_ptr() as usize) % align, 0);\n        debug_assert_eq!(footer_ptr as usize % CHUNK_ALIGN, 0);\n        let footer_ptr = footer_ptr as *mut ChunkFooter;\n\n        // The bump pointer is initialized to the end of the range we will\n        // bump out of.\n        let ptr = Cell::new(NonNull::new_unchecked(footer_ptr as *mut u8));\n\n        // The `allocated_bytes` of a new chunk counts the total size\n        // of the chunks, not how much of the chunks are used.\n        let allocated_bytes = prev.as_ref().allocated_bytes + new_size_without_footer;\n\n        ptr::write(\n            footer_ptr,\n            ChunkFooter {\n                data,\n                layout,\n                prev: Cell::new(prev),\n                ptr,\n                allocated_bytes,\n            },\n        );\n\n        Some(NonNull::new_unchecked(footer_ptr))\n    }\n\n    /// Reset this bump allocator.\n    ///\n    /// Performs mass deallocation on everything allocated in this arena by\n    /// resetting the pointer into the underlying chunk of memory to the start\n    /// of the chunk. Does not run any `Drop` implementations on deallocated\n    /// objects; see [the top-level documentation](struct.Bump.html) for details.\n    ///\n    /// If this arena has allocated multiple chunks to bump allocate into, then\n    /// the excess chunks are returned to the global allocator.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let mut bump = bumpalo::Bump::new();\n    ///\n    /// // Allocate a bunch of things.\n    /// {\n    ///     for i in 0..100 {\n    ///         bump.alloc(i);\n    ///     }\n    /// }\n    ///\n    /// // Reset the arena.\n    /// bump.reset();\n    ///\n    /// // Allocate some new things in the space previously occupied by the\n    /// // original things.\n    /// for j in 200..400 {\n    ///     bump.alloc(j);\n    /// }\n    ///```\n    pub fn reset(&mut self) {\n        // Takes `&mut self` so `self` must be unique and there can't be any\n        // borrows active that would get invalidated by resetting.\n        unsafe {\n            if self.current_chunk_footer.get().as_ref().is_empty() {\n                return;\n            }\n\n            let mut cur_chunk = self.current_chunk_footer.get();\n\n            // Deallocate all chunks except the current one\n            let prev_chunk = cur_chunk.as_ref().prev.replace(EMPTY_CHUNK.get());\n            dealloc_chunk_list(prev_chunk);\n\n            // Reset the bump finger to the end of the chunk.\n            cur_chunk.as_ref().ptr.set(cur_chunk.cast());\n\n            // Reset the allocated size of the chunk.\n            cur_chunk.as_mut().allocated_bytes = cur_chunk.as_ref().layout.size();\n\n            debug_assert!(\n                self.current_chunk_footer\n                    .get()\n                    .as_ref()\n                    .prev\n                    .get()\n                    .as_ref()\n                    .is_empty(),\n                \"We should only have a single chunk\"\n            );\n            debug_assert_eq!(\n                self.current_chunk_footer.get().as_ref().ptr.get(),\n                self.current_chunk_footer.get().cast(),\n                \"Our chunk's bump finger should be reset to the start of its allocation\"\n            );\n        }\n    }\n\n    /// Allocate an object in this `Bump` and return an exclusive reference to\n    /// it.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for `T` fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc(\"hello\");\n    /// assert_eq!(*x, \"hello\");\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc<T>(&self, val: T) -> &mut T {\n        self.alloc_with(|| val)\n    }\n\n    /// Try to allocate an object in this `Bump` and return an exclusive\n    /// reference to it.\n    ///\n    /// ## Errors\n    ///\n    /// Errors if reserving space for `T` fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.try_alloc(\"hello\");\n    /// assert_eq!(x, Ok(&mut \"hello\"));\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn try_alloc<T>(&self, val: T) -> Result<&mut T, AllocErr> {\n        self.try_alloc_with(|| val)\n    }\n\n    /// Pre-allocate space for an object in this `Bump`, initializes it using\n    /// the closure, then returns an exclusive reference to it.\n    ///\n    /// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n    /// discussion on the differences between the `_with` suffixed methods and\n    /// those methods without it, their performance characteristics, and when\n    /// you might or might not choose a `_with` suffixed method.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for `T` fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_with(|| \"hello\");\n    /// assert_eq!(*x, \"hello\");\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_with<F, T>(&self, f: F) -> &mut T\n    where\n        F: FnOnce() -> T,\n    {\n        #[inline(always)]\n        unsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,\n        {\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }\n\n        let layout = Layout::new::<T>();\n\n        unsafe {\n            let p = self.alloc_layout(layout);\n            let p = p.as_ptr() as *mut T;\n            inner_writer(p, f);\n            &mut *p\n        }\n    }\n\n    /// Tries to pre-allocate space for an object in this `Bump`, initializes\n    /// it using the closure, then returns an exclusive reference to it.\n    ///\n    /// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n    /// discussion on the differences between the `_with` suffixed methods and\n    /// those methods without it, their performance characteristics, and when\n    /// you might or might not choose a `_with` suffixed method.\n    ///\n    /// ## Errors\n    ///\n    /// Errors if reserving space for `T` fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.try_alloc_with(|| \"hello\");\n    /// assert_eq!(x, Ok(&mut \"hello\"));\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn try_alloc_with<F, T>(&self, f: F) -> Result<&mut T, AllocErr>\n    where\n        F: FnOnce() -> T,\n    {\n        #[inline(always)]\n        unsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,\n        {\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }\n\n        //SAFETY: Self-contained:\n        // `p` is allocated for `T` and then a `T` is written.\n        let layout = Layout::new::<T>();\n        let p = self.try_alloc_layout(layout)?;\n        let p = p.as_ptr() as *mut T;\n\n        unsafe {\n            inner_writer(p, f);\n            Ok(&mut *p)\n        }\n    }\n\n    /// Pre-allocates space for a [`Result`] in this `Bump`, initializes it using\n    /// the closure, then returns an exclusive reference to its `T` if [`Ok`].\n    ///\n    /// Iff the allocation fails, the closure is not run.\n    ///\n    /// Iff [`Err`], an allocator rewind is *attempted* and the `E` instance is\n    /// moved out of the allocator to be consumed or dropped as normal.\n    ///\n    /// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n    /// discussion on the differences between the `_with` suffixed methods and\n    /// those methods without it, their performance characteristics, and when\n    /// you might or might not choose a `_with` suffixed method.\n    ///\n    /// For caveats specific to fallible initialization, see\n    /// [The `_try_with` Method Suffix](#fallible-initialization-the-_try_with-method-suffix).\n    ///\n    /// [`Result`]: https://doc.rust-lang.org/std/result/enum.Result.html\n    /// [`Ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok\n    /// [`Err`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err\n    ///\n    /// ## Errors\n    ///\n    /// Iff the allocation succeeds but `f` fails, that error is forwarded by value.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for `Result<T, E>` fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_try_with(|| Ok(\"hello\"))?;\n    /// assert_eq!(*x, \"hello\");\n    /// # Result::<_, ()>::Ok(())\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_try_with<F, T, E>(&self, f: F) -> Result<&mut T, E>\n    where\n        F: FnOnce() -> Result<T, E>,\n    {\n        let rewind_footer = self.current_chunk_footer.get();\n        let rewind_ptr = unsafe { rewind_footer.as_ref() }.ptr.get();\n        let mut inner_result_ptr = NonNull::from(self.alloc_with(f));\n        match unsafe { inner_result_ptr.as_mut() } {\n            Ok(t) => Ok(unsafe {\n                //SAFETY:\n                // The `&mut Result<T, E>` returned by `alloc_with` may be\n                // lifetime-limited by `E`, but the derived `&mut T` still has\n                // the same validity as in `alloc_with` since the error variant\n                // is already ruled out here.\n\n                // We could conditionally truncate the allocation here, but\n                // since it grows backwards, it seems unlikely that we'd get\n                // any more than the `Result`'s discriminant this way, if\n                // anything at all.\n                &mut *(t as *mut _)\n            }),\n            Err(e) => unsafe {\n                // If this result was the last allocation in this arena, we can\n                // reclaim its space. In fact, sometimes we can do even better\n                // than simply calling `dealloc` on the result pointer: we can\n                // reclaim any alignment padding we might have added (which\n                // `dealloc` cannot do) if we didn't allocate a new chunk for\n                // this result.\n                if self.is_last_allocation(inner_result_ptr.cast()) {\n                    let current_footer_p = self.current_chunk_footer.get();\n                    let current_ptr = &current_footer_p.as_ref().ptr;\n                    if current_footer_p == rewind_footer {\n                        // It's still the same chunk, so reset the bump pointer\n                        // to its original value upon entry to this method\n                        // (reclaiming any alignment padding we may have\n                        // added).\n                        current_ptr.set(rewind_ptr);\n                    } else {\n                        // We allocated a new chunk for this result.\n                        //\n                        // We know the result is the only allocation in this\n                        // chunk: Any additional allocations since the start of\n                        // this method could only have happened when running\n                        // the initializer function, which is called *after*\n                        // reserving space for this result. Therefore, since we\n                        // already determined via the check above that this\n                        // result was the last allocation, there must not have\n                        // been any other allocations, and this result is the\n                        // only allocation in this chunk.\n                        //\n                        // Because this is the only allocation in this chunk,\n                        // we can reset the chunk's bump finger to the start of\n                        // the chunk.\n                        current_ptr.set(current_footer_p.as_ref().data);\n                    }\n                }\n                //SAFETY:\n                // As we received `E` semantically by value from `f`, we can\n                // just copy that value here as long as we avoid a double-drop\n                // (which can't happen as any specific references to the `E`'s\n                // data in `self` are destroyed when this function returns).\n                //\n                // The order between this and the deallocation doesn't matter\n                // because `Self: !Sync`.\n                Err(ptr::read(e as *const _))\n            },\n        }\n    }\n\n    /// Tries to pre-allocates space for a [`Result`] in this `Bump`,\n    /// initializes it using the closure, then returns an exclusive reference\n    /// to its `T` if all [`Ok`].\n    ///\n    /// Iff the allocation fails, the closure is not run.\n    ///\n    /// Iff the closure returns [`Err`], an allocator rewind is *attempted* and\n    /// the `E` instance is moved out of the allocator to be consumed or dropped\n    /// as normal.\n    ///\n    /// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n    /// discussion on the differences between the `_with` suffixed methods and\n    /// those methods without it, their performance characteristics, and when\n    /// you might or might not choose a `_with` suffixed method.\n    ///\n    /// For caveats specific to fallible initialization, see\n    /// [The `_try_with` Method Suffix](#fallible-initialization-the-_try_with-method-suffix).\n    ///\n    /// [`Result`]: https://doc.rust-lang.org/std/result/enum.Result.html\n    /// [`Ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok\n    /// [`Err`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err\n    ///\n    /// ## Errors\n    ///\n    /// Errors with the [`Alloc`](`AllocOrInitError::Alloc`) variant iff\n    /// reserving space for `Result<T, E>` fails.\n    ///\n    /// Iff the allocation succeeds but `f` fails, that error is forwarded by\n    /// value inside the [`Init`](`AllocOrInitError::Init`) variant.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.try_alloc_try_with(|| Ok(\"hello\"))?;\n    /// assert_eq!(*x, \"hello\");\n    /// # Result::<_, bumpalo::AllocOrInitError<()>>::Ok(())\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn try_alloc_try_with<F, T, E>(&self, f: F) -> Result<&mut T, AllocOrInitError<E>>\n    where\n        F: FnOnce() -> Result<T, E>,\n    {\n        let rewind_footer = self.current_chunk_footer.get();\n        let rewind_ptr = unsafe { rewind_footer.as_ref() }.ptr.get();\n        let mut inner_result_ptr = NonNull::from(self.try_alloc_with(f)?);\n        match unsafe { inner_result_ptr.as_mut() } {\n            Ok(t) => Ok(unsafe {\n                //SAFETY:\n                // The `&mut Result<T, E>` returned by `alloc_with` may be\n                // lifetime-limited by `E`, but the derived `&mut T` still has\n                // the same validity as in `alloc_with` since the error variant\n                // is already ruled out here.\n\n                // We could conditionally truncate the allocation here, but\n                // since it grows backwards, it seems unlikely that we'd get\n                // any more than the `Result`'s discriminant this way, if\n                // anything at all.\n                &mut *(t as *mut _)\n            }),\n            Err(e) => unsafe {\n                // If this result was the last allocation in this arena, we can\n                // reclaim its space. In fact, sometimes we can do even better\n                // than simply calling `dealloc` on the result pointer: we can\n                // reclaim any alignment padding we might have added (which\n                // `dealloc` cannot do) if we didn't allocate a new chunk for\n                // this result.\n                if self.is_last_allocation(inner_result_ptr.cast()) {\n                    let current_footer_p = self.current_chunk_footer.get();\n                    let current_ptr = &current_footer_p.as_ref().ptr;\n                    if current_footer_p == rewind_footer {\n                        // It's still the same chunk, so reset the bump pointer\n                        // to its original value upon entry to this method\n                        // (reclaiming any alignment padding we may have\n                        // added).\n                        current_ptr.set(rewind_ptr);\n                    } else {\n                        // We allocated a new chunk for this result.\n                        //\n                        // We know the result is the only allocation in this\n                        // chunk: Any additional allocations since the start of\n                        // this method could only have happened when running\n                        // the initializer function, which is called *after*\n                        // reserving space for this result. Therefore, since we\n                        // already determined via the check above that this\n                        // result was the last allocation, there must not have\n                        // been any other allocations, and this result is the\n                        // only allocation in this chunk.\n                        //\n                        // Because this is the only allocation in this chunk,\n                        // we can reset the chunk's bump finger to the start of\n                        // the chunk.\n                        current_ptr.set(current_footer_p.as_ref().data);\n                    }\n                }\n                //SAFETY:\n                // As we received `E` semantically by value from `f`, we can\n                // just copy that value here as long as we avoid a double-drop\n                // (which can't happen as any specific references to the `E`'s\n                // data in `self` are destroyed when this function returns).\n                //\n                // The order between this and the deallocation doesn't matter\n                // because `Self: !Sync`.\n                Err(AllocOrInitError::Init(ptr::read(e as *const _)))\n            },\n        }\n    }\n\n    /// `Copy` a slice into this `Bump` and return an exclusive reference to\n    /// the copy.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_slice_copy(&[1, 2, 3]);\n    /// assert_eq!(x, &[1, 2, 3]);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_copy<T>(&self, src: &[T]) -> &mut [T]\n    where\n        T: Copy,\n    {\n        let layout = Layout::for_value(src);\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            ptr::copy_nonoverlapping(src.as_ptr(), dst.as_ptr(), src.len());\n            slice::from_raw_parts_mut(dst.as_ptr(), src.len())\n        }\n    }\n\n    /// `Clone` a slice into this `Bump` and return an exclusive reference to\n    /// the clone. Prefer [`alloc_slice_copy`](#method.alloc_slice_copy) if `T` is `Copy`.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// #[derive(Clone, Debug, Eq, PartialEq)]\n    /// struct Sheep {\n    ///     name: String,\n    /// }\n    ///\n    /// let originals = [\n    ///     Sheep { name: \"Alice\".into() },\n    ///     Sheep { name: \"Bob\".into() },\n    ///     Sheep { name: \"Cathy\".into() },\n    /// ];\n    ///\n    /// let bump = bumpalo::Bump::new();\n    /// let clones = bump.alloc_slice_clone(&originals);\n    /// assert_eq!(originals, clones);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_clone<T>(&self, src: &[T]) -> &mut [T]\n    where\n        T: Clone,\n    {\n        let layout = Layout::for_value(src);\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            for (i, val) in src.iter().cloned().enumerate() {\n                ptr::write(dst.as_ptr().add(i), val);\n            }\n\n            slice::from_raw_parts_mut(dst.as_ptr(), src.len())\n        }\n    }\n\n    /// `Copy` a string slice into this `Bump` and return an exclusive reference to it.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the string fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let hello = bump.alloc_str(\"hello world\");\n    /// assert_eq!(\"hello world\", hello);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_str(&self, src: &str) -> &mut str {\n        let buffer = self.alloc_slice_copy(src.as_bytes());\n        unsafe {\n            // This is OK, because it already came in as str, so it is guaranteed to be utf8\n            str::from_utf8_unchecked_mut(buffer)\n        }\n    }\n\n    /// Allocates a new slice of size `len` into this `Bump` and returns an\n    /// exclusive reference to the copy.\n    ///\n    /// The elements of the slice are initialized using the supplied closure.\n    /// The closure argument is the position in the slice.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_slice_fill_with(5, |i| 5 * (i + 1));\n    /// assert_eq!(x, &[5, 10, 15, 20, 25]);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_fill_with<T, F>(&self, len: usize, mut f: F) -> &mut [T]\n    where\n        F: FnMut(usize) -> T,\n    {\n        let layout = Layout::array::<T>(len).unwrap_or_else(|_| oom());\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            for i in 0..len {\n                ptr::write(dst.as_ptr().add(i), f(i));\n            }\n\n            let result = slice::from_raw_parts_mut(dst.as_ptr(), len);\n            debug_assert_eq!(Layout::for_value(result), layout);\n            result\n        }\n    }\n\n    /// Allocates a new slice of size `len` into this `Bump` and returns an\n    /// exclusive reference to the copy.\n    ///\n    /// All elements of the slice are initialized to `value`.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_slice_fill_copy(5, 42);\n    /// assert_eq!(x, &[42, 42, 42, 42, 42]);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_fill_copy<T: Copy>(&self, len: usize, value: T) -> &mut [T] {\n        self.alloc_slice_fill_with(len, |_| value)\n    }\n\n    /// Allocates a new slice of size `len` slice into this `Bump` and return an\n    /// exclusive reference to the copy.\n    ///\n    /// All elements of the slice are initialized to `value.clone()`.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let s: String = \"Hello Bump!\".to_string();\n    /// let x: &[String] = bump.alloc_slice_fill_clone(2, &s);\n    /// assert_eq!(x.len(), 2);\n    /// assert_eq!(&x[0], &s);\n    /// assert_eq!(&x[1], &s);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_fill_clone<T: Clone>(&self, len: usize, value: &T) -> &mut [T] {\n        self.alloc_slice_fill_with(len, |_| value.clone())\n    }\n\n    /// Allocates a new slice of size `len` slice into this `Bump` and return an\n    /// exclusive reference to the copy.\n    ///\n    /// The elements are initialized using the supplied iterator.\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails, or if the supplied\n    /// iterator returns fewer elements than it promised.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x: &[i32] = bump.alloc_slice_fill_iter([2, 3, 5].iter().cloned().map(|i| i * i));\n    /// assert_eq!(x, [4, 9, 25]);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_fill_iter<T, I>(&self, iter: I) -> &mut [T]\n    where\n        I: IntoIterator<Item = T>,\n        I::IntoIter: ExactSizeIterator,\n    {\n        let mut iter = iter.into_iter();\n        self.alloc_slice_fill_with(iter.len(), |_| {\n            iter.next().expect(\"Iterator supplied too few elements\")\n        })\n    }\n\n    /// Allocates a new slice of size `len` slice into this `Bump` and return an\n    /// exclusive reference to the copy.\n    ///\n    /// All elements of the slice are initialized to [`T::default()`].\n    ///\n    /// [`T::default()`]: https://doc.rust-lang.org/std/default/trait.Default.html#tymethod.default\n    ///\n    /// ## Panics\n    ///\n    /// Panics if reserving space for the slice fails.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let x = bump.alloc_slice_fill_default::<u32>(5);\n    /// assert_eq!(x, &[0, 0, 0, 0, 0]);\n    /// ```\n    #[inline(always)]\n    #[allow(clippy::mut_from_ref)]\n    pub fn alloc_slice_fill_default<T: Default>(&self, len: usize) -> &mut [T] {\n        self.alloc_slice_fill_with(len, |_| T::default())\n    }\n\n    /// Allocate space for an object with the given `Layout`.\n    ///\n    /// The returned pointer points at uninitialized memory, and should be\n    /// initialized with\n    /// [`std::ptr::write`](https://doc.rust-lang.org/std/ptr/fn.write.html).\n    ///\n    /// # Panics\n    ///\n    /// Panics if reserving space matching `layout` fails.\n    #[inline(always)]\n    pub fn alloc_layout(&self, layout: Layout) -> NonNull<u8> {\n        self.try_alloc_layout(layout).unwrap_or_else(|_| oom())\n    }\n\n    /// Attempts to allocate space for an object with the given `Layout` or else returns\n    /// an `Err`.\n    ///\n    /// The returned pointer points at uninitialized memory, and should be\n    /// initialized with\n    /// [`std::ptr::write`](https://doc.rust-lang.org/std/ptr/fn.write.html).\n    ///\n    /// # Errors\n    ///\n    /// Errors if reserving space matching `layout` fails.\n    #[inline(always)]\n    pub fn try_alloc_layout(&self, layout: Layout) -> Result<NonNull<u8>, AllocErr> {\n        if let Some(p) = self.try_alloc_layout_fast(layout) {\n            Ok(p)\n        } else {\n            self.alloc_layout_slow(layout).ok_or(AllocErr)\n        }\n    }\n\n    #[inline(always)]\n    fn try_alloc_layout_fast(&self, layout: Layout) -> Option<NonNull<u8>> {\n        // We don't need to check for ZSTs here since they will automatically\n        // be handled properly: the pointer will be bumped by zero bytes,\n        // modulo alignment. This keeps the fast path optimized for non-ZSTs,\n        // which are much more common.\n        unsafe {\n            let footer = self.current_chunk_footer.get();\n            let footer = footer.as_ref();\n            let ptr = footer.ptr.get().as_ptr();\n            let start = footer.data.as_ptr();\n            debug_assert!(start <= ptr);\n            debug_assert!(ptr as *const u8 <= footer as *const _ as *const u8);\n\n            if (ptr as usize) < layout.size() {\n                return None;\n            }\n\n            let ptr = ptr.wrapping_sub(layout.size());\n            let rem = ptr as usize % layout.align();\n            let aligned_ptr = ptr.wrapping_sub(rem);\n\n            if aligned_ptr >= start {\n                let aligned_ptr = NonNull::new_unchecked(aligned_ptr as *mut u8);\n                footer.ptr.set(aligned_ptr);\n                Some(aligned_ptr)\n            } else {\n                None\n            }\n        }\n    }\n\n    /// Gets the remaining capacity in the current chunk (in bytes).\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// use bumpalo::Bump;\n    ///\n    /// let bump = Bump::with_capacity(100);\n    ///\n    /// let capacity = bump.chunk_capacity();\n    /// assert!(capacity >= 100);\n    /// ```\n    pub fn chunk_capacity(&self) -> usize {\n        let current_footer = self.current_chunk_footer.get();\n        let current_footer = unsafe { current_footer.as_ref() };\n\n        current_footer as *const _ as usize - current_footer.data.as_ptr() as usize\n    }\n\n    /// Slow path allocation for when we need to allocate a new chunk from the\n    /// parent bump set because there isn't enough room in our current chunk.\n    #[inline(never)]\n    fn alloc_layout_slow(&self, layout: Layout) -> Option<NonNull<u8>> {\n        unsafe {\n            let size = layout.size();\n            let allocation_limit_remaining = self.allocation_limit_remaining();\n\n            // Get a new chunk from the global allocator.\n            let current_footer = self.current_chunk_footer.get();\n            let current_layout = current_footer.as_ref().layout;\n\n            // By default, we want our new chunk to be about twice as big\n            // as the previous chunk. If the global allocator refuses it,\n            // we try to divide it by half until it works or the requested\n            // size is smaller than the default footer size.\n            let min_new_chunk_size = layout.size().max(DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER);\n            let mut base_size = (current_layout.size() - FOOTER_SIZE)\n                .checked_mul(2)?\n                .max(min_new_chunk_size);\n            let chunk_memory_details = iter::from_fn(|| {\n                let bypass_min_chunk_size_for_small_limits = match self.allocation_limit() {\n                    Some(limit)\n                        if layout.size() < limit\n                            && base_size >= layout.size()\n                            && limit < DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER\n                            && self.allocated_bytes() == 0 =>\n                    {\n                        true\n                    }\n                    _ => false,\n                };\n\n                if base_size >= min_new_chunk_size || bypass_min_chunk_size_for_small_limits {\n                    let size = base_size;\n                    base_size = base_size / 2;\n                    Bump::new_chunk_memory_details(Some(size), layout)\n                } else {\n                    None\n                }\n            });\n\n            let new_footer = chunk_memory_details\n                .filter_map(|chunk_memory_details| {\n                    if Bump::chunk_fits_under_limit(\n                        allocation_limit_remaining,\n                        chunk_memory_details,\n                    ) {\n                        Bump::new_chunk(chunk_memory_details, layout, current_footer)\n                    } else {\n                        None\n                    }\n                })\n                .next()?;\n\n            debug_assert_eq!(\n                new_footer.as_ref().data.as_ptr() as usize % layout.align(),\n                0\n            );\n\n            // Set the new chunk as our new current chunk.\n            self.current_chunk_footer.set(new_footer);\n\n            let new_footer = new_footer.as_ref();\n\n            // Move the bump ptr finger down to allocate room for `val`. We know\n            // this can't overflow because we successfully allocated a chunk of\n            // at least the requested size.\n            let mut ptr = new_footer.ptr.get().as_ptr().sub(size);\n            // Round the pointer down to the requested alignment.\n            ptr = ptr.sub(ptr as usize % layout.align());\n            debug_assert!(\n                ptr as *const _ <= new_footer,\n                \"{:p} <= {:p}\",\n                ptr,\n                new_footer\n            );\n            let ptr = NonNull::new_unchecked(ptr as *mut u8);\n            new_footer.ptr.set(ptr);\n\n            // Return a pointer to the freshly allocated region in this chunk.\n            Some(ptr)\n        }\n    }\n\n    /// Returns an iterator over each chunk of allocated memory that\n    /// this arena has bump allocated into.\n    ///\n    /// The chunks are returned ordered by allocation time, with the most\n    /// recently allocated chunk being returned first, and the least recently\n    /// allocated chunk being returned last.\n    ///\n    /// The values inside each chunk are also ordered by allocation time, with\n    /// the most recent allocation being earlier in the slice, and the least\n    /// recent allocation being towards the end of the slice.\n    ///\n    /// ## Safety\n    ///\n    /// Because this method takes `&mut self`, we know that the bump arena\n    /// reference is unique and therefore there aren't any active references to\n    /// any of the objects we've allocated in it either. This potential aliasing\n    /// of exclusive references is one common footgun for unsafe code that we\n    /// don't need to worry about here.\n    ///\n    /// However, there could be regions of uninitialized memory used as padding\n    /// between allocations, which is why this iterator has items of type\n    /// `[MaybeUninit<u8>]`, instead of simply `[u8]`.\n    ///\n    /// The only way to guarantee that there is no padding between allocations\n    /// or within allocated objects is if all of these properties hold:\n    ///\n    /// 1. Every object allocated in this arena has the same alignment,\n    ///    and that alignment is at most 16.\n    /// 2. Every object's size is a multiple of its alignment.\n    /// 3. None of the objects allocated in this arena contain any internal\n    ///    padding.\n    ///\n    /// If you want to use this `iter_allocated_chunks` method, it is *your*\n    /// responsibility to ensure that these properties hold before calling\n    /// `MaybeUninit::assume_init` or otherwise reading the returned values.\n    ///\n    /// Finally, you must also ensure that any values allocated into the bump\n    /// arena have not had their `Drop` implementations called on them,\n    /// e.g. after dropping a [`bumpalo::boxed::Box<T>`][crate::boxed::Box].\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let mut bump = bumpalo::Bump::new();\n    ///\n    /// // Allocate a bunch of `i32`s in this bump arena, potentially causing\n    /// // additional memory chunks to be reserved.\n    /// for i in 0..10000 {\n    ///     bump.alloc(i);\n    /// }\n    ///\n    /// // Iterate over each chunk we've bump allocated into. This is safe\n    /// // because we have only allocated `i32`s in this arena, which fulfills\n    /// // the above requirements.\n    /// for ch in bump.iter_allocated_chunks() {\n    ///     println!(\"Used a chunk that is {} bytes long\", ch.len());\n    ///     println!(\"The first byte is {:?}\", unsafe {\n    ///         ch[0].assume_init()\n    ///     });\n    /// }\n    ///\n    /// // Within a chunk, allocations are ordered from most recent to least\n    /// // recent. If we allocated 'a', then 'b', then 'c', when we iterate\n    /// // through the chunk's data, we get them in the order 'c', then 'b',\n    /// // then 'a'.\n    ///\n    /// bump.reset();\n    /// bump.alloc(b'a');\n    /// bump.alloc(b'b');\n    /// bump.alloc(b'c');\n    ///\n    /// assert_eq!(bump.iter_allocated_chunks().count(), 1);\n    /// let chunk = bump.iter_allocated_chunks().nth(0).unwrap();\n    /// assert_eq!(chunk.len(), 3);\n    ///\n    /// // Safe because we've only allocated `u8`s in this arena, which\n    /// // fulfills the above requirements.\n    /// unsafe {\n    ///     assert_eq!(chunk[0].assume_init(), b'c');\n    ///     assert_eq!(chunk[1].assume_init(), b'b');\n    ///     assert_eq!(chunk[2].assume_init(), b'a');\n    /// }\n    /// ```\n    pub fn iter_allocated_chunks(&mut self) -> ChunkIter<'_> {\n        // SAFE: Ensured by mutable borrow of `self`.\n        let raw = unsafe { self.iter_allocated_chunks_raw() };\n        ChunkIter {\n            raw,\n            bump: PhantomData,\n        }\n    }\n\n    /// Returns an iterator over raw pointers to chunks of allocated memory that\n    /// this arena has bump allocated into.\n    ///\n    /// This is an unsafe version of [`iter_allocated_chunks()`](Bump::iter_allocated_chunks),\n    /// with the caller responsible for safe usage of the returned pointers as\n    /// well as ensuring that the iterator is not invalidated by new\n    /// allocations.\n    ///\n    /// ## Safety\n    ///\n    /// Allocations from this arena must not be performed while the returned\n    /// iterator is alive. If reading the chunk data (or casting to a reference)\n    /// the caller must ensure that there exist no mutable references to\n    /// previously allocated data.\n    ///\n    /// In addition, all of the caveats when reading the chunk data from\n    /// [`iter_allocated_chunks()`](Bump::iter_allocated_chunks) still apply.\n    pub unsafe fn iter_allocated_chunks_raw(&self) -> ChunkRawIter<'_> {\n        ChunkRawIter {\n            footer: self.current_chunk_footer.get(),\n            bump: PhantomData,\n        }\n    }\n\n    /// Calculates the number of bytes currently allocated across all chunks in\n    /// this bump arena.\n    ///\n    /// If you allocate types of different alignments or types with\n    /// larger-than-typical alignment in the same arena, some padding\n    /// bytes might get allocated in the bump arena. Note that those padding\n    /// bytes will add to this method's resulting sum, so you cannot rely\n    /// on it only counting the sum of the sizes of the things\n    /// you've allocated in the arena.\n    ///\n    /// ## Example\n    ///\n    /// ```\n    /// let bump = bumpalo::Bump::new();\n    /// let _x = bump.alloc_slice_fill_default::<u32>(5);\n    /// let bytes = bump.allocated_bytes();\n    /// assert!(bytes >= core::mem::size_of::<u32>() * 5);\n    /// ```\n    pub fn allocated_bytes(&self) -> usize {\n        let footer = self.current_chunk_footer.get();\n\n        unsafe { footer.as_ref().allocated_bytes }\n    }\n\n    #[inline]\n    unsafe fn is_last_allocation(&self, ptr: NonNull<u8>) -> bool {\n        let footer = self.current_chunk_footer.get();\n        let footer = footer.as_ref();\n        footer.ptr.get() == ptr\n    }\n\n    #[inline]\n    unsafe fn dealloc(&self, ptr: NonNull<u8>, layout: Layout) {\n        // If the pointer is the last allocation we made, we can reuse the bytes,\n        // otherwise they are simply leaked -- at least until somebody calls reset().\n        if self.is_last_allocation(ptr) {\n            let ptr = NonNull::new_unchecked(ptr.as_ptr().add(layout.size()));\n            self.current_chunk_footer.get().as_ref().ptr.set(ptr);\n        }\n    }\n\n    #[inline]\n    unsafe fn shrink(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<u8>, AllocErr> {\n        let old_size = old_layout.size();\n        let new_size = new_layout.size();\n        let align_is_compatible = old_layout.align() >= new_layout.align();\n\n        if !align_is_compatible {\n            return Err(AllocErr);\n        }\n\n        // This is how much space we would *actually* reclaim while satisfying\n        // the requested alignment.\n        let delta = round_down_to(old_size - new_size, new_layout.align());\n\n        if self.is_last_allocation(ptr)\n                // Only reclaim the excess space (which requires a copy) if it\n                // is worth it: we are actually going to recover \"enough\" space\n                // and we can do a non-overlapping copy.\n                && delta >= old_size / 2\n        {\n            let footer = self.current_chunk_footer.get();\n            let footer = footer.as_ref();\n\n            // NB: new_ptr is aligned, because ptr *has to* be aligned, and we\n            // made sure delta is aligned.\n            let new_ptr = NonNull::new_unchecked(footer.ptr.get().as_ptr().add(delta));\n            footer.ptr.set(new_ptr);\n\n            // NB: we know it is non-overlapping because of the size check\n            // in the `if` condition.\n            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), new_size);\n\n            return Ok(new_ptr);\n        } else {\n            return Ok(ptr);\n        }\n    }\n\n    #[inline]\n    unsafe fn grow(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<u8>, AllocErr> {\n        let old_size = old_layout.size();\n        let new_size = new_layout.size();\n        let align_is_compatible = old_layout.align() >= new_layout.align();\n\n        if align_is_compatible && self.is_last_allocation(ptr) {\n            // Try to allocate the delta size within this same block so we can\n            // reuse the currently allocated space.\n            let delta = new_size - old_size;\n            if let Some(p) =\n                self.try_alloc_layout_fast(layout_from_size_align(delta, old_layout.align()))\n            {\n                ptr::copy(ptr.as_ptr(), p.as_ptr(), old_size);\n                return Ok(p);\n            }\n        }\n\n        // Fallback: do a fresh allocation and copy the existing data into it.\n        let new_ptr = self.try_alloc_layout(new_layout)?;\n        ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), old_size);\n        Ok(new_ptr)\n    }\n}","impl Default for Bump {\n    fn default() -> Bump {\n        Bump::new()\n    }\n}","impl Drop for Bump {\n    fn drop(&mut self) {\n        unsafe {\n            dealloc_chunk_list(self.current_chunk_footer.get());\n        }\n    }\n}","unsafe impl Send for Bump {}"],"ChunkFooter":["Debug","impl ChunkFooter {\n    // Returns the start and length of the currently allocated region of this\n    // chunk.\n    fn as_raw_parts(&self) -> (*const u8, usize) {\n        let data = self.data.as_ptr() as *const u8;\n        let ptr = self.ptr.get().as_ptr() as *const u8;\n        debug_assert!(data <= ptr);\n        debug_assert!(ptr <= self as *const ChunkFooter as *const u8);\n        let len = unsafe { (self as *const ChunkFooter as *const u8).offset_from(ptr) as usize };\n        (ptr, len)\n    }\n\n    /// Is this chunk the last empty chunk?\n    fn is_empty(&self) -> bool {\n        ptr::eq(self, EMPTY_CHUNK.get().as_ptr())\n    }\n}"],"ChunkIter":["Debug","impl<'a> Iterator for ChunkIter<'a> {\n    type Item = &'a [mem::MaybeUninit<u8>];\n    fn next(&mut self) -> Option<&'a [mem::MaybeUninit<u8>]> {\n        unsafe {\n            let (ptr, len) = self.raw.next()?;\n            let slice = slice::from_raw_parts(ptr as *const mem::MaybeUninit<u8>, len);\n            Some(slice)\n        }\n    }\n}","impl<'a> iter::FusedIterator for ChunkIter<'a> {}"],"ChunkRawIter":["Debug","impl Iterator for ChunkRawIter<'_> {\n    type Item = (*mut u8, usize);\n    fn next(&mut self) -> Option<(*mut u8, usize)> {\n        unsafe {\n            let foot = self.footer.as_ref();\n            if foot.is_empty() {\n                return None;\n            }\n            let (ptr, len) = foot.as_raw_parts();\n            self.footer = foot.prev.get();\n            Some((ptr as *mut u8, len))\n        }\n    }\n}","impl iter::FusedIterator for ChunkRawIter<'_> {}"],"EmptyChunkFooter":["impl EmptyChunkFooter {\n    fn get(&'static self) -> NonNull<ChunkFooter> {\n        unsafe { NonNull::new_unchecked(&self.0 as *const ChunkFooter as *mut ChunkFooter) }\n    }\n}","unsafe impl Sync for EmptyChunkFooter {}"],"NewChunkMemoryDetails":["Clone","Copy","Debug"],"alloc::AllocErr":["Clone","Debug","Eq","PartialEq","impl fmt::Display for AllocErr {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        f.write_str(\"memory allocation failed\")\n    }\n}"],"alloc::CannotReallocInPlace":["Clone","Debug","Eq","PartialEq","impl CannotReallocInPlace {\n    pub fn description(&self) -> &str {\n        \"cannot reallocate allocator's memory in place\"\n    }\n}","impl fmt::Display for CannotReallocInPlace {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        write!(f, \"{}\", self.description())\n    }\n}"],"alloc::Excess":["Debug"],"core::alloc::Layout":["impl UnstableLayoutMethods for Layout {\n    fn padding_needed_for(&self, align: usize) -> usize {\n        let len = self.size();\n\n        // Rounded up value is:\n        //   len_rounded_up = (len + align - 1) & !(align - 1);\n        // and then we return the padding difference: `len_rounded_up - len`.\n        //\n        // We use modular arithmetic throughout:\n        //\n        // 1. align is guaranteed to be > 0, so align - 1 is always\n        //    valid.\n        //\n        // 2. `len + align - 1` can overflow by at most `align - 1`,\n        //    so the &-mask with `!(align - 1)` will ensure that in the\n        //    case of overflow, `len_rounded_up` will itself be 0.\n        //    Thus the returned padding, when added to `len`, yields 0,\n        //    which trivially satisfies the alignment `align`.\n        //\n        // (Of course, attempts to allocate blocks of memory whose\n        // size and padding overflow in the above manner should cause\n        // the allocator to yield an error anyway.)\n\n        let len_rounded_up = len.wrapping_add(align).wrapping_sub(1) & !align.wrapping_sub(1);\n        len_rounded_up.wrapping_sub(len)\n    }\n\n    fn repeat(&self, n: usize) -> Result<(Layout, usize), LayoutErr> {\n        let padded_size = self\n            .size()\n            .checked_add(self.padding_needed_for(self.align()))\n            .ok_or_else(new_layout_err)?;\n        let alloc_size = padded_size.checked_mul(n).ok_or_else(new_layout_err)?;\n\n        unsafe {\n            // self.align is already known to be valid and alloc_size has been\n            // padded already.\n            Ok((\n                Layout::from_size_align_unchecked(alloc_size, self.align()),\n                padded_size,\n            ))\n        }\n    }\n\n    fn array<T>(n: usize) -> Result<Layout, LayoutErr> {\n        Layout::new::<T>().repeat(n).map(|(k, offs)| {\n            debug_assert!(offs == mem::size_of::<T>());\n            k\n        })\n    }\n}"]},"single_path_import":{"alloc::AllocErr":"AllocErr","core::alloc::Layout":"alloc::Layout","core::alloc::LayoutErr":"alloc::LayoutErr"},"srcs":{"<&'a Bump as alloc::Alloc>::alloc":["#[inline(always)]\nunsafe fn alloc(&mut self, layout: Layout) -> Result<NonNull<u8>, AllocErr>{\n        self.try_alloc_layout(layout)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<&'a Bump as alloc::Alloc>::dealloc":["#[inline]\nunsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout){\n        Bump::dealloc(self, ptr, layout)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<&'a Bump as alloc::Alloc>::realloc":["#[inline]\nunsafe fn realloc(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<NonNull<u8>, AllocErr>{\n        let old_size = layout.size();\n\n        if old_size == 0 {\n            return self.try_alloc_layout(layout);\n        }\n\n        let new_layout = layout_from_size_align(new_size, layout.align());\n        if new_size <= old_size {\n            self.shrink(ptr, layout, new_layout)\n        } else {\n            self.grow(ptr, layout, new_layout)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<AllocOrInitError<E> as core::convert::From<alloc::AllocErr>>::from":["fn from(e: AllocErr) -> Self{\n        Self::Alloc(e)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<AllocOrInitError<E> as core::fmt::Display>::fmt":["fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result{\n        match self {\n            AllocOrInitError::Alloc(err) => err.fmt(f),\n            AllocOrInitError::Init(err) => write!(f, \"initialization failed: {}\", err),\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Bump as core::default::Default>::default":["fn default() -> Bump{\n        Bump::new()\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<Bump as core::ops::Drop>::drop":["fn drop(&mut self){\n        unsafe {\n            dealloc_chunk_list(self.current_chunk_footer.get());\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ChunkIter<'a> as core::iter::Iterator>::next":["fn next(&mut self) -> Option<&'a [mem::MaybeUninit<u8>]>{\n        unsafe {\n            let (ptr, len) = self.raw.next()?;\n            let slice = slice::from_raw_parts(ptr as *const mem::MaybeUninit<u8>, len);\n            Some(slice)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<ChunkRawIter<'_> as core::iter::Iterator>::next":["fn next(&mut self) -> Option<(*mut u8, usize)>{\n        unsafe {\n            let foot = self.footer.as_ref();\n            if foot.is_empty() {\n                return None;\n            }\n            let (ptr, len) = foot.as_raw_parts();\n            self.footer = foot.prev.get();\n            Some((ptr as *mut u8, len))\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"<alloc::AllocErr as core::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result{\n        f.write_str(\"memory allocation failed\")\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"<alloc::CannotReallocInPlace as core::fmt::Display>::fmt":["fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result{\n        write!(f, \"{}\", self.description())\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::array":["fn array<T>(n: usize) -> Result<Layout, LayoutErr>{\n        Layout::new::<T>().repeat(n).map(|(k, offs)| {\n            debug_assert!(offs == mem::size_of::<T>());\n            k\n        })\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::padding_needed_for":["fn padding_needed_for(&self, align: usize) -> usize{\n        let len = self.size();\n\n        // Rounded up value is:\n        //   len_rounded_up = (len + align - 1) & !(align - 1);\n        // and then we return the padding difference: `len_rounded_up - len`.\n        //\n        // We use modular arithmetic throughout:\n        //\n        // 1. align is guaranteed to be > 0, so align - 1 is always\n        //    valid.\n        //\n        // 2. `len + align - 1` can overflow by at most `align - 1`,\n        //    so the &-mask with `!(align - 1)` will ensure that in the\n        //    case of overflow, `len_rounded_up` will itself be 0.\n        //    Thus the returned padding, when added to `len`, yields 0,\n        //    which trivially satisfies the alignment `align`.\n        //\n        // (Of course, attempts to allocate blocks of memory whose\n        // size and padding overflow in the above manner should cause\n        // the allocator to yield an error anyway.)\n\n        let len_rounded_up = len.wrapping_add(align).wrapping_sub(1) & !align.wrapping_sub(1);\n        len_rounded_up.wrapping_sub(len)\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::repeat":["fn repeat(&self, n: usize) -> Result<(Layout, usize), LayoutErr>{\n        let padded_size = self\n            .size()\n            .checked_add(self.padding_needed_for(self.align()))\n            .ok_or_else(new_layout_err)?;\n        let alloc_size = padded_size.checked_mul(n).ok_or_else(new_layout_err)?;\n\n        unsafe {\n            // self.align is already known to be valid and alloc_size has been\n            // padded already.\n            Ok((\n                Layout::from_size_align_unchecked(alloc_size, self.align()),\n                padded_size,\n            ))\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"AllocOrInitError":["/// An error returned from [`Bump::try_alloc_try_with`].\npub enum AllocOrInitError<E> {\n    /// Indicates that the initial allocation failed.\n    Alloc(AllocErr),\n    /// Indicates that the initializer failed with the contained error after\n    /// allocation.\n    ///\n    /// It is possible but not guaranteed that the allocated memory has been\n    /// released back to the allocator at this point.\n    Init(E),\n}","Real(LocalPath(\"src/lib.rs\"))"],"Bump":["/// An arena to bump allocate into.\n///\n/// ## No `Drop`s\n///\n/// Objects that are bump-allocated will never have their [`Drop`] implementation\n/// called &mdash; unless you do it manually yourself. This makes it relatively\n/// easy to leak memory or other resources.\n///\n/// If you have a type which internally manages\n///\n/// * an allocation from the global heap (e.g. [`Vec<T>`]),\n/// * open file descriptors (e.g. [`std::fs::File`]), or\n/// * any other resource that must be cleaned up (e.g. an `mmap`)\n///\n/// and relies on its `Drop` implementation to clean up the internal resource,\n/// then if you allocate that type with a `Bump`, you need to find a new way to\n/// clean up after it yourself.\n///\n/// Potential solutions are:\n///\n/// * Using [`bumpalo::boxed::Box::new_in`] instead of [`Bump::alloc`], that\n///   will drop wrapped values similarly to [`std::boxed::Box`]. Note that this\n///   requires enabling the `\"boxed\"` Cargo feature for this crate. **This is\n///   often the easiest solution.**\n///\n/// * Calling [`drop_in_place`][drop_in_place] or using\n///   [`std::mem::ManuallyDrop`][manuallydrop] to manually drop these types.\n///\n/// * Using [`bumpalo::collections::Vec`] instead of [`std::vec::Vec`].\n///\n/// * Avoiding allocating these problematic types within a `Bump`.\n///\n/// Note that not calling `Drop` is memory safe! Destructors are never\n/// guaranteed to run in Rust, you can't rely on them for enforcing memory\n/// safety.\n///\n/// [`Drop`]: https://doc.rust-lang.org/std/ops/trait.Drop.html\n/// [`Vec<T>`]: https://doc.rust-lang.org/std/vec/struct.Vec.html\n/// [`std::fs::File`]: https://doc.rust-lang.org/std/fs/struct.File.html\n/// [drop_in_place]: https://doc.rust-lang.org/std/ptr/fn.drop_in_place.html\n/// [manuallydrop]: https://doc.rust-lang.org/std/mem/struct.ManuallyDrop.html\n/// [`bumpalo::collections::Vec`]: collections/vec/struct.Vec.html\n/// [`std::vec::Vec`]: https://doc.rust-lang.org/std/vec/struct.Vec.html\n/// [`bumpalo::boxed::Box::new_in`]: boxed/struct.Box.html#method.new_in\n/// [`std::boxed::Box`]: https://doc.rust-lang.org/std/boxed/struct.Box.html\n///\n/// ## Example\n///\n/// ```\n/// use bumpalo::Bump;\n///\n/// // Create a new bump arena.\n/// let bump = Bump::new();\n///\n/// // Allocate values into the arena.\n/// let forty_two = bump.alloc(42);\n/// assert_eq!(*forty_two, 42);\n///\n/// // Mutable references are returned from allocation.\n/// let mut s = bump.alloc(\"bumpalo\");\n/// *s = \"the bump allocator; and also is a buffalo\";\n/// ```\n///\n/// ## Allocation Methods Come in Many Flavors\n///\n/// There are various allocation methods on `Bump`, the simplest being\n/// [`alloc`][Bump::alloc]. The others exist to satisfy some combination of\n/// fallible allocation and initialization. The allocation methods are\n/// summarized in the following table:\n///\n/// <table>\n///   <thead>\n///     <tr>\n///       <th></th>\n///       <th>Infallible Allocation</th>\n///       <th>Fallible Allocation</th>\n///     </tr>\n///   </thead>\n///     <tr>\n///       <th>By Value</th>\n///       <td><a href=\"#method.alloc\"><code>alloc</code></a></td>\n///       <td><a href=\"#method.try_alloc\"><code>try_alloc</code></a></td>\n///     </tr>\n///     <tr>\n///       <th>Infallible Initializer Function</th>\n///       <td><a href=\"#method.alloc_with\"><code>alloc_with</code></a></td>\n///       <td><a href=\"#method.try_alloc_with\"><code>try_alloc_with</code></a></td>\n///     </tr>\n///     <tr>\n///       <th>Fallible Initializer Function</th>\n///       <td><a href=\"#method.alloc_try_with\"><code>alloc_try_with</code></a></td>\n///       <td><a href=\"#method.try_alloc_try_with\"><code>try_alloc_try_with</code></a></td>\n///     </tr>\n///   <tbody>\n///   </tbody>\n/// </table>\n///\n/// ### Fallible Allocation: The `try_alloc_` Method Prefix\n///\n/// These allocation methods let you recover from out-of-memory (OOM)\n/// scenarioes, rather than raising a panic on OOM.\n///\n/// ```\n/// use bumpalo::Bump;\n///\n/// let bump = Bump::new();\n///\n/// match bump.try_alloc(MyStruct {\n///     // ...\n/// }) {\n///     Ok(my_struct) => {\n///         // Allocation succeeded.\n///     }\n///     Err(e) => {\n///         // Out of memory.\n///     }\n/// }\n///\n/// struct MyStruct {\n///     // ...\n/// }\n/// ```\n///\n/// ### Initializer Functions: The `_with` Method Suffix\n///\n/// Calling one of the generic `…alloc(x)` methods is essentially equivalent to\n/// the matching [`…alloc_with(|| x)`](?search=alloc_with). However if you use\n/// `…alloc_with`, then the closure will not be invoked until after allocating\n/// space for storing `x` on the heap.\n///\n/// This can be useful in certain edge-cases related to compiler optimizations.\n/// When evaluating for example `bump.alloc(x)`, semantically `x` is first put\n/// on the stack and then moved onto the heap. In some cases, the compiler is\n/// able to optimize this into constructing `x` directly on the heap, however\n/// in many cases it does not.\n///\n/// The `…alloc_with` functions try to help the compiler be smarter. In most\n/// cases doing for example `bump.try_alloc_with(|| x)` on release mode will be\n/// enough to help the compiler realize that this optimization is valid and\n/// to construct `x` directly onto the heap.\n///\n/// #### Warning\n///\n/// These functions critically depend on compiler optimizations to achieve their\n/// desired effect. This means that it is not an effective tool when compiling\n/// without optimizations on.\n///\n/// Even when optimizations are on, these functions do not **guarantee** that\n/// the value is constructed on the heap. To the best of our knowledge no such\n/// guarantee can be made in stable Rust as of 1.54.\n///\n/// ### Fallible Initialization: The `_try_with` Method Suffix\n///\n/// The generic [`…alloc_try_with(|| x)`](?search=_try_with) methods behave\n/// like the purely `_with` suffixed methods explained above. However, they\n/// allow for fallible initialization by accepting a closure that returns a\n/// [`Result`] and will attempt to undo the initial allocation if this closure\n/// returns [`Err`].\n///\n/// #### Warning\n///\n/// If the inner closure returns [`Ok`], space for the entire [`Result`] remains\n/// allocated inside `self`. This can be a problem especially if the [`Err`]\n/// variant is larger, but even otherwise there may be overhead for the\n/// [`Result`]'s discriminant.\n///\n/// <p><details><summary>Undoing the allocation in the <code>Err</code> case\n/// always fails if <code>f</code> successfully made any additional allocations\n/// in <code>self</code>.</summary>\n///\n/// For example, the following will always leak also space for the [`Result`]\n/// into this `Bump`, even though the inner reference isn't kept and the [`Err`]\n/// payload is returned semantically by value:\n///\n/// ```rust\n/// let bump = bumpalo::Bump::new();\n///\n/// let r: Result<&mut [u8; 1000], ()> = bump.alloc_try_with(|| {\n///     let _ = bump.alloc(0_u8);\n///     Err(())\n/// });\n///\n/// assert!(r.is_err());\n/// ```\n///\n///</details></p>\n///\n/// Since [`Err`] payloads are first placed on the heap and then moved to the\n/// stack, `bump.…alloc_try_with(|| x)?` is likely to execute more slowly than\n/// the matching `bump.…alloc(x?)` in case of initialization failure. If this\n/// happens frequently, using the plain un-suffixed method may perform better.\n///\n/// [`Result`]: https://doc.rust-lang.org/std/result/enum.Result.html\n/// [`Ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok\n/// [`Err`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err\n///\n/// ### `Bump` Allocation Limits\n///\n/// `bumpalo` supports setting a limit on the maximum bytes of memory that can\n/// be allocated for use in a particular `Bump` arena. This limit can be set and removed with\n/// [`set_allocation_limit`][Bump::set_allocation_limit].\n/// The allocation limit is only enforced when allocating new backing chunks for\n/// a `Bump`. Updating the allocation limit will not affect existing allocations\n/// or any future allocations within the `Bump`'s current chunk.\n///\n/// #### Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n///\n/// assert_eq!(bump.allocation_limit(), None);\n/// bump.set_allocation_limit(Some(0));\n///\n/// assert!(bump.try_alloc(5).is_err());\n///\n/// bump.set_allocation_limit(Some(6));\n///\n/// assert_eq!(bump.allocation_limit(), Some(6));\n///\n/// bump.set_allocation_limit(None);\n///\n/// assert_eq!(bump.allocation_limit(), None);\n/// ```\n///\n/// #### Warning\n///\n/// Because of backwards compatibility, allocations that fail\n/// due to allocation limits will not present differently than\n/// errors due to resource exhaustion.\npub struct Bump {\n    // The current chunk we are bump allocating within.\n    current_chunk_footer: Cell<NonNull<ChunkFooter>>,\n    allocation_limit: Cell<Option<usize>>,\n}","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc":["/// Allocate an object in this `Bump` and return an exclusive reference to\n/// it.\n///\n/// ## Panics\n///\n/// Panics if reserving space for `T` fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc(\"hello\");\n/// assert_eq!(*x, \"hello\");\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc<T>(&self, val: T) -> &mut T{\n        self.alloc_with(|| val)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_layout":["/// Allocate space for an object with the given `Layout`.\n///\n/// The returned pointer points at uninitialized memory, and should be\n/// initialized with\n/// [`std::ptr::write`](https://doc.rust-lang.org/std/ptr/fn.write.html).\n///\n/// # Panics\n///\n/// Panics if reserving space matching `layout` fails.\n#[inline(always)]\npub fn alloc_layout(&self, layout: Layout) -> NonNull<u8>{\n        self.try_alloc_layout(layout).unwrap_or_else(|_| oom())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_layout_slow":["/// Slow path allocation for when we need to allocate a new chunk from the\n/// parent bump set because there isn't enough room in our current chunk.\n#[inline(never)]\nfn alloc_layout_slow(&self, layout: Layout) -> Option<NonNull<u8>>{\n        unsafe {\n            let size = layout.size();\n            let allocation_limit_remaining = self.allocation_limit_remaining();\n\n            // Get a new chunk from the global allocator.\n            let current_footer = self.current_chunk_footer.get();\n            let current_layout = current_footer.as_ref().layout;\n\n            // By default, we want our new chunk to be about twice as big\n            // as the previous chunk. If the global allocator refuses it,\n            // we try to divide it by half until it works or the requested\n            // size is smaller than the default footer size.\n            let min_new_chunk_size = layout.size().max(DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER);\n            let mut base_size = (current_layout.size() - FOOTER_SIZE)\n                .checked_mul(2)?\n                .max(min_new_chunk_size);\n            let chunk_memory_details = iter::from_fn(|| {\n                let bypass_min_chunk_size_for_small_limits = match self.allocation_limit() {\n                    Some(limit)\n                        if layout.size() < limit\n                            && base_size >= layout.size()\n                            && limit < DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER\n                            && self.allocated_bytes() == 0 =>\n                    {\n                        true\n                    }\n                    _ => false,\n                };\n\n                if base_size >= min_new_chunk_size || bypass_min_chunk_size_for_small_limits {\n                    let size = base_size;\n                    base_size = base_size / 2;\n                    Bump::new_chunk_memory_details(Some(size), layout)\n                } else {\n                    None\n                }\n            });\n\n            let new_footer = chunk_memory_details\n                .filter_map(|chunk_memory_details| {\n                    if Bump::chunk_fits_under_limit(\n                        allocation_limit_remaining,\n                        chunk_memory_details,\n                    ) {\n                        Bump::new_chunk(chunk_memory_details, layout, current_footer)\n                    } else {\n                        None\n                    }\n                })\n                .next()?;\n\n            debug_assert_eq!(\n                new_footer.as_ref().data.as_ptr() as usize % layout.align(),\n                0\n            );\n\n            // Set the new chunk as our new current chunk.\n            self.current_chunk_footer.set(new_footer);\n\n            let new_footer = new_footer.as_ref();\n\n            // Move the bump ptr finger down to allocate room for `val`. We know\n            // this can't overflow because we successfully allocated a chunk of\n            // at least the requested size.\n            let mut ptr = new_footer.ptr.get().as_ptr().sub(size);\n            // Round the pointer down to the requested alignment.\n            ptr = ptr.sub(ptr as usize % layout.align());\n            debug_assert!(\n                ptr as *const _ <= new_footer,\n                \"{:p} <= {:p}\",\n                ptr,\n                new_footer\n            );\n            let ptr = NonNull::new_unchecked(ptr as *mut u8);\n            new_footer.ptr.set(ptr);\n\n            // Return a pointer to the freshly allocated region in this chunk.\n            Some(ptr)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_clone":["/// `Clone` a slice into this `Bump` and return an exclusive reference to\n/// the clone. Prefer [`alloc_slice_copy`](#method.alloc_slice_copy) if `T` is `Copy`.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// #[derive(Clone, Debug, Eq, PartialEq)]\n/// struct Sheep {\n///     name: String,\n/// }\n///\n/// let originals = [\n///     Sheep { name: \"Alice\".into() },\n///     Sheep { name: \"Bob\".into() },\n///     Sheep { name: \"Cathy\".into() },\n/// ];\n///\n/// let bump = bumpalo::Bump::new();\n/// let clones = bump.alloc_slice_clone(&originals);\n/// assert_eq!(originals, clones);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_clone<T>(&self, src: &[T]) -> &mut [T]\n    where\n        T: Clone,{\n        let layout = Layout::for_value(src);\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            for (i, val) in src.iter().cloned().enumerate() {\n                ptr::write(dst.as_ptr().add(i), val);\n            }\n\n            slice::from_raw_parts_mut(dst.as_ptr(), src.len())\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_copy":["/// `Copy` a slice into this `Bump` and return an exclusive reference to\n/// the copy.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_slice_copy(&[1, 2, 3]);\n/// assert_eq!(x, &[1, 2, 3]);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_copy<T>(&self, src: &[T]) -> &mut [T]\n    where\n        T: Copy,{\n        let layout = Layout::for_value(src);\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            ptr::copy_nonoverlapping(src.as_ptr(), dst.as_ptr(), src.len());\n            slice::from_raw_parts_mut(dst.as_ptr(), src.len())\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_fill_clone":["/// Allocates a new slice of size `len` slice into this `Bump` and return an\n/// exclusive reference to the copy.\n///\n/// All elements of the slice are initialized to `value.clone()`.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let s: String = \"Hello Bump!\".to_string();\n/// let x: &[String] = bump.alloc_slice_fill_clone(2, &s);\n/// assert_eq!(x.len(), 2);\n/// assert_eq!(&x[0], &s);\n/// assert_eq!(&x[1], &s);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_fill_clone<T: Clone>(&self, len: usize, value: &T) -> &mut [T]{\n        self.alloc_slice_fill_with(len, |_| value.clone())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_fill_copy":["/// Allocates a new slice of size `len` into this `Bump` and returns an\n/// exclusive reference to the copy.\n///\n/// All elements of the slice are initialized to `value`.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_slice_fill_copy(5, 42);\n/// assert_eq!(x, &[42, 42, 42, 42, 42]);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_fill_copy<T: Copy>(&self, len: usize, value: T) -> &mut [T]{\n        self.alloc_slice_fill_with(len, |_| value)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_fill_default":["/// Allocates a new slice of size `len` slice into this `Bump` and return an\n/// exclusive reference to the copy.\n///\n/// All elements of the slice are initialized to [`T::default()`].\n///\n/// [`T::default()`]: https://doc.rust-lang.org/std/default/trait.Default.html#tymethod.default\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_slice_fill_default::<u32>(5);\n/// assert_eq!(x, &[0, 0, 0, 0, 0]);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_fill_default<T: Default>(&self, len: usize) -> &mut [T]{\n        self.alloc_slice_fill_with(len, |_| T::default())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_fill_iter":["/// Allocates a new slice of size `len` slice into this `Bump` and return an\n/// exclusive reference to the copy.\n///\n/// The elements are initialized using the supplied iterator.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails, or if the supplied\n/// iterator returns fewer elements than it promised.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x: &[i32] = bump.alloc_slice_fill_iter([2, 3, 5].iter().cloned().map(|i| i * i));\n/// assert_eq!(x, [4, 9, 25]);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_fill_iter<T, I>(&self, iter: I) -> &mut [T]\n    where\n        I: IntoIterator<Item = T>,\n        I::IntoIter: ExactSizeIterator,{\n        let mut iter = iter.into_iter();\n        self.alloc_slice_fill_with(iter.len(), |_| {\n            iter.next().expect(\"Iterator supplied too few elements\")\n        })\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_slice_fill_with":["/// Allocates a new slice of size `len` into this `Bump` and returns an\n/// exclusive reference to the copy.\n///\n/// The elements of the slice are initialized using the supplied closure.\n/// The closure argument is the position in the slice.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the slice fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_slice_fill_with(5, |i| 5 * (i + 1));\n/// assert_eq!(x, &[5, 10, 15, 20, 25]);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_slice_fill_with<T, F>(&self, len: usize, mut f: F) -> &mut [T]\n    where\n        F: FnMut(usize) -> T,{\n        let layout = Layout::array::<T>(len).unwrap_or_else(|_| oom());\n        let dst = self.alloc_layout(layout).cast::<T>();\n\n        unsafe {\n            for i in 0..len {\n                ptr::write(dst.as_ptr().add(i), f(i));\n            }\n\n            let result = slice::from_raw_parts_mut(dst.as_ptr(), len);\n            debug_assert_eq!(Layout::for_value(result), layout);\n            result\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_str":["/// `Copy` a string slice into this `Bump` and return an exclusive reference to it.\n///\n/// ## Panics\n///\n/// Panics if reserving space for the string fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let hello = bump.alloc_str(\"hello world\");\n/// assert_eq!(\"hello world\", hello);\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_str(&self, src: &str) -> &mut str{\n        let buffer = self.alloc_slice_copy(src.as_bytes());\n        unsafe {\n            // This is OK, because it already came in as str, so it is guaranteed to be utf8\n            str::from_utf8_unchecked_mut(buffer)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_try_with":["/// Pre-allocates space for a [`Result`] in this `Bump`, initializes it using\n/// the closure, then returns an exclusive reference to its `T` if [`Ok`].\n///\n/// Iff the allocation fails, the closure is not run.\n///\n/// Iff [`Err`], an allocator rewind is *attempted* and the `E` instance is\n/// moved out of the allocator to be consumed or dropped as normal.\n///\n/// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n/// discussion on the differences between the `_with` suffixed methods and\n/// those methods without it, their performance characteristics, and when\n/// you might or might not choose a `_with` suffixed method.\n///\n/// For caveats specific to fallible initialization, see\n/// [The `_try_with` Method Suffix](#fallible-initialization-the-_try_with-method-suffix).\n///\n/// [`Result`]: https://doc.rust-lang.org/std/result/enum.Result.html\n/// [`Ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok\n/// [`Err`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err\n///\n/// ## Errors\n///\n/// Iff the allocation succeeds but `f` fails, that error is forwarded by value.\n///\n/// ## Panics\n///\n/// Panics if reserving space for `Result<T, E>` fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_try_with(|| Ok(\"hello\"))?;\n/// assert_eq!(*x, \"hello\");\n/// # Result::<_, ()>::Ok(())\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_try_with<F, T, E>(&self, f: F) -> Result<&mut T, E>\n    where\n        F: FnOnce() -> Result<T, E>,{\n        let rewind_footer = self.current_chunk_footer.get();\n        let rewind_ptr = unsafe { rewind_footer.as_ref() }.ptr.get();\n        let mut inner_result_ptr = NonNull::from(self.alloc_with(f));\n        match unsafe { inner_result_ptr.as_mut() } {\n            Ok(t) => Ok(unsafe {\n                //SAFETY:\n                // The `&mut Result<T, E>` returned by `alloc_with` may be\n                // lifetime-limited by `E`, but the derived `&mut T` still has\n                // the same validity as in `alloc_with` since the error variant\n                // is already ruled out here.\n\n                // We could conditionally truncate the allocation here, but\n                // since it grows backwards, it seems unlikely that we'd get\n                // any more than the `Result`'s discriminant this way, if\n                // anything at all.\n                &mut *(t as *mut _)\n            }),\n            Err(e) => unsafe {\n                // If this result was the last allocation in this arena, we can\n                // reclaim its space. In fact, sometimes we can do even better\n                // than simply calling `dealloc` on the result pointer: we can\n                // reclaim any alignment padding we might have added (which\n                // `dealloc` cannot do) if we didn't allocate a new chunk for\n                // this result.\n                if self.is_last_allocation(inner_result_ptr.cast()) {\n                    let current_footer_p = self.current_chunk_footer.get();\n                    let current_ptr = &current_footer_p.as_ref().ptr;\n                    if current_footer_p == rewind_footer {\n                        // It's still the same chunk, so reset the bump pointer\n                        // to its original value upon entry to this method\n                        // (reclaiming any alignment padding we may have\n                        // added).\n                        current_ptr.set(rewind_ptr);\n                    } else {\n                        // We allocated a new chunk for this result.\n                        //\n                        // We know the result is the only allocation in this\n                        // chunk: Any additional allocations since the start of\n                        // this method could only have happened when running\n                        // the initializer function, which is called *after*\n                        // reserving space for this result. Therefore, since we\n                        // already determined via the check above that this\n                        // result was the last allocation, there must not have\n                        // been any other allocations, and this result is the\n                        // only allocation in this chunk.\n                        //\n                        // Because this is the only allocation in this chunk,\n                        // we can reset the chunk's bump finger to the start of\n                        // the chunk.\n                        current_ptr.set(current_footer_p.as_ref().data);\n                    }\n                }\n                //SAFETY:\n                // As we received `E` semantically by value from `f`, we can\n                // just copy that value here as long as we avoid a double-drop\n                // (which can't happen as any specific references to the `E`'s\n                // data in `self` are destroyed when this function returns).\n                //\n                // The order between this and the deallocation doesn't matter\n                // because `Self: !Sync`.\n                Err(ptr::read(e as *const _))\n            },\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_with":["/// Pre-allocate space for an object in this `Bump`, initializes it using\n/// the closure, then returns an exclusive reference to it.\n///\n/// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n/// discussion on the differences between the `_with` suffixed methods and\n/// those methods without it, their performance characteristics, and when\n/// you might or might not choose a `_with` suffixed method.\n///\n/// ## Panics\n///\n/// Panics if reserving space for `T` fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.alloc_with(|| \"hello\");\n/// assert_eq!(*x, \"hello\");\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn alloc_with<F, T>(&self, f: F) -> &mut T\n    where\n        F: FnOnce() -> T,{\n        #[inline(always)]\n        unsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,\n        {\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }\n\n        let layout = Layout::new::<T>();\n\n        unsafe {\n            let p = self.alloc_layout(layout);\n            let p = p.as_ptr() as *mut T;\n            inner_writer(p, f);\n            &mut *p\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::alloc_with::inner_writer":["#[inline(always)]\nunsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,{\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::allocated_bytes":["/// Calculates the number of bytes currently allocated across all chunks in\n/// this bump arena.\n///\n/// If you allocate types of different alignments or types with\n/// larger-than-typical alignment in the same arena, some padding\n/// bytes might get allocated in the bump arena. Note that those padding\n/// bytes will add to this method's resulting sum, so you cannot rely\n/// on it only counting the sum of the sizes of the things\n/// you've allocated in the arena.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let _x = bump.alloc_slice_fill_default::<u32>(5);\n/// let bytes = bump.allocated_bytes();\n/// assert!(bytes >= core::mem::size_of::<u32>() * 5);\n/// ```\npub fn allocated_bytes(&self) -> usize{\n        let footer = self.current_chunk_footer.get();\n\n        unsafe { footer.as_ref().allocated_bytes }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::allocation_limit":["/// The allocation limit for this arena in bytes.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::with_capacity(0);\n///\n/// assert_eq!(bump.allocation_limit(), None);\n///\n/// bump.set_allocation_limit(Some(6));\n///\n/// assert_eq!(bump.allocation_limit(), Some(6));\n///\n/// bump.set_allocation_limit(None);\n///\n/// assert_eq!(bump.allocation_limit(), None);\n/// ```\npub fn allocation_limit(&self) -> Option<usize>{\n        self.allocation_limit.get()\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::allocation_limit_remaining":["/// How much headroom an arena has before it hits its allocation\n/// limit.\nfn allocation_limit_remaining(&self) -> Option<usize>{\n        self.allocation_limit.get().and_then(|allocation_limit| {\n            let allocated_bytes = self.allocated_bytes();\n            if allocated_bytes > allocation_limit {\n                None\n            } else {\n                Some(abs_diff(allocation_limit, allocated_bytes))\n            }\n        })\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::chunk_capacity":["/// Gets the remaining capacity in the current chunk (in bytes).\n///\n/// ## Example\n///\n/// ```\n/// use bumpalo::Bump;\n///\n/// let bump = Bump::with_capacity(100);\n///\n/// let capacity = bump.chunk_capacity();\n/// assert!(capacity >= 100);\n/// ```\npub fn chunk_capacity(&self) -> usize{\n        let current_footer = self.current_chunk_footer.get();\n        let current_footer = unsafe { current_footer.as_ref() };\n\n        current_footer as *const _ as usize - current_footer.data.as_ptr() as usize\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::chunk_fits_under_limit":["/// Whether a request to allocate a new chunk with a given size for a given\n/// requested layout will fit under the allocation limit set on a `Bump`.\nfn chunk_fits_under_limit(\n        allocation_limit_remaining: Option<usize>,\n        new_chunk_memory_details: NewChunkMemoryDetails,\n    ) -> bool{\n        allocation_limit_remaining\n            .map(|allocation_limit_left| {\n                allocation_limit_left >= new_chunk_memory_details.new_size_without_footer\n            })\n            .unwrap_or(true)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::dealloc":["#[inline]\nunsafe fn dealloc(&self, ptr: NonNull<u8>, layout: Layout){\n        // If the pointer is the last allocation we made, we can reuse the bytes,\n        // otherwise they are simply leaked -- at least until somebody calls reset().\n        if self.is_last_allocation(ptr) {\n            let ptr = NonNull::new_unchecked(ptr.as_ptr().add(layout.size()));\n            self.current_chunk_footer.get().as_ref().ptr.set(ptr);\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::grow":["#[inline]\nunsafe fn grow(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<u8>, AllocErr>{\n        let old_size = old_layout.size();\n        let new_size = new_layout.size();\n        let align_is_compatible = old_layout.align() >= new_layout.align();\n\n        if align_is_compatible && self.is_last_allocation(ptr) {\n            // Try to allocate the delta size within this same block so we can\n            // reuse the currently allocated space.\n            let delta = new_size - old_size;\n            if let Some(p) =\n                self.try_alloc_layout_fast(layout_from_size_align(delta, old_layout.align()))\n            {\n                ptr::copy(ptr.as_ptr(), p.as_ptr(), old_size);\n                return Ok(p);\n            }\n        }\n\n        // Fallback: do a fresh allocation and copy the existing data into it.\n        let new_ptr = self.try_alloc_layout(new_layout)?;\n        ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), old_size);\n        Ok(new_ptr)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::is_last_allocation":["#[inline]\nunsafe fn is_last_allocation(&self, ptr: NonNull<u8>) -> bool{\n        let footer = self.current_chunk_footer.get();\n        let footer = footer.as_ref();\n        footer.ptr.get() == ptr\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::iter_allocated_chunks":["/// Returns an iterator over each chunk of allocated memory that\n/// this arena has bump allocated into.\n///\n/// The chunks are returned ordered by allocation time, with the most\n/// recently allocated chunk being returned first, and the least recently\n/// allocated chunk being returned last.\n///\n/// The values inside each chunk are also ordered by allocation time, with\n/// the most recent allocation being earlier in the slice, and the least\n/// recent allocation being towards the end of the slice.\n///\n/// ## Safety\n///\n/// Because this method takes `&mut self`, we know that the bump arena\n/// reference is unique and therefore there aren't any active references to\n/// any of the objects we've allocated in it either. This potential aliasing\n/// of exclusive references is one common footgun for unsafe code that we\n/// don't need to worry about here.\n///\n/// However, there could be regions of uninitialized memory used as padding\n/// between allocations, which is why this iterator has items of type\n/// `[MaybeUninit<u8>]`, instead of simply `[u8]`.\n///\n/// The only way to guarantee that there is no padding between allocations\n/// or within allocated objects is if all of these properties hold:\n///\n/// 1. Every object allocated in this arena has the same alignment,\n///    and that alignment is at most 16.\n/// 2. Every object's size is a multiple of its alignment.\n/// 3. None of the objects allocated in this arena contain any internal\n///    padding.\n///\n/// If you want to use this `iter_allocated_chunks` method, it is *your*\n/// responsibility to ensure that these properties hold before calling\n/// `MaybeUninit::assume_init` or otherwise reading the returned values.\n///\n/// Finally, you must also ensure that any values allocated into the bump\n/// arena have not had their `Drop` implementations called on them,\n/// e.g. after dropping a [`bumpalo::boxed::Box<T>`][crate::boxed::Box].\n///\n/// ## Example\n///\n/// ```\n/// let mut bump = bumpalo::Bump::new();\n///\n/// // Allocate a bunch of `i32`s in this bump arena, potentially causing\n/// // additional memory chunks to be reserved.\n/// for i in 0..10000 {\n///     bump.alloc(i);\n/// }\n///\n/// // Iterate over each chunk we've bump allocated into. This is safe\n/// // because we have only allocated `i32`s in this arena, which fulfills\n/// // the above requirements.\n/// for ch in bump.iter_allocated_chunks() {\n///     println!(\"Used a chunk that is {} bytes long\", ch.len());\n///     println!(\"The first byte is {:?}\", unsafe {\n///         ch[0].assume_init()\n///     });\n/// }\n///\n/// // Within a chunk, allocations are ordered from most recent to least\n/// // recent. If we allocated 'a', then 'b', then 'c', when we iterate\n/// // through the chunk's data, we get them in the order 'c', then 'b',\n/// // then 'a'.\n///\n/// bump.reset();\n/// bump.alloc(b'a');\n/// bump.alloc(b'b');\n/// bump.alloc(b'c');\n///\n/// assert_eq!(bump.iter_allocated_chunks().count(), 1);\n/// let chunk = bump.iter_allocated_chunks().nth(0).unwrap();\n/// assert_eq!(chunk.len(), 3);\n///\n/// // Safe because we've only allocated `u8`s in this arena, which\n/// // fulfills the above requirements.\n/// unsafe {\n///     assert_eq!(chunk[0].assume_init(), b'c');\n///     assert_eq!(chunk[1].assume_init(), b'b');\n///     assert_eq!(chunk[2].assume_init(), b'a');\n/// }\n/// ```\npub fn iter_allocated_chunks(&mut self) -> ChunkIter<'_>{\n        // SAFE: Ensured by mutable borrow of `self`.\n        let raw = unsafe { self.iter_allocated_chunks_raw() };\n        ChunkIter {\n            raw,\n            bump: PhantomData,\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::iter_allocated_chunks_raw":["/// Returns an iterator over raw pointers to chunks of allocated memory that\n/// this arena has bump allocated into.\n///\n/// This is an unsafe version of [`iter_allocated_chunks()`](Bump::iter_allocated_chunks),\n/// with the caller responsible for safe usage of the returned pointers as\n/// well as ensuring that the iterator is not invalidated by new\n/// allocations.\n///\n/// ## Safety\n///\n/// Allocations from this arena must not be performed while the returned\n/// iterator is alive. If reading the chunk data (or casting to a reference)\n/// the caller must ensure that there exist no mutable references to\n/// previously allocated data.\n///\n/// In addition, all of the caveats when reading the chunk data from\n/// [`iter_allocated_chunks()`](Bump::iter_allocated_chunks) still apply.\npub unsafe fn iter_allocated_chunks_raw(&self) -> ChunkRawIter<'_>{\n        ChunkRawIter {\n            footer: self.current_chunk_footer.get(),\n            bump: PhantomData,\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::new":["/// Construct a new arena to bump allocate into.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// # let _ = bump;\n/// ```\npub fn new() -> Bump{\n        Self::with_capacity(0)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::new_chunk":["/// Allocate a new chunk and return its initialized footer.\n///\n/// If given, `layouts` is a tuple of the current chunk size and the\n/// layout of the allocation request that triggered us to fall back to\n/// allocating a new chunk of memory.\nunsafe fn new_chunk(\n        new_chunk_memory_details: NewChunkMemoryDetails,\n        requested_layout: Layout,\n        prev: NonNull<ChunkFooter>,\n    ) -> Option<NonNull<ChunkFooter>>{\n        let NewChunkMemoryDetails {\n            new_size_without_footer,\n            align,\n            size,\n        } = new_chunk_memory_details;\n\n        let layout = layout_from_size_align(size, align);\n\n        debug_assert!(size >= requested_layout.size());\n\n        let data = alloc(layout);\n        let data = NonNull::new(data)?;\n\n        // The `ChunkFooter` is at the end of the chunk.\n        let footer_ptr = data.as_ptr().add(new_size_without_footer);\n        debug_assert_eq!((data.as_ptr() as usize) % align, 0);\n        debug_assert_eq!(footer_ptr as usize % CHUNK_ALIGN, 0);\n        let footer_ptr = footer_ptr as *mut ChunkFooter;\n\n        // The bump pointer is initialized to the end of the range we will\n        // bump out of.\n        let ptr = Cell::new(NonNull::new_unchecked(footer_ptr as *mut u8));\n\n        // The `allocated_bytes` of a new chunk counts the total size\n        // of the chunks, not how much of the chunks are used.\n        let allocated_bytes = prev.as_ref().allocated_bytes + new_size_without_footer;\n\n        ptr::write(\n            footer_ptr,\n            ChunkFooter {\n                data,\n                layout,\n                prev: Cell::new(prev),\n                ptr,\n                allocated_bytes,\n            },\n        );\n\n        Some(NonNull::new_unchecked(footer_ptr))\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::new_chunk_memory_details":["/// Determine the memory details including final size, alignment and\n/// final size without footer for a new chunk that would be allocated\n/// to fulfill an allocation request.\nfn new_chunk_memory_details(\n        new_size_without_footer: Option<usize>,\n        requested_layout: Layout,\n    ) -> Option<NewChunkMemoryDetails>{\n        let mut new_size_without_footer =\n            new_size_without_footer.unwrap_or(DEFAULT_CHUNK_SIZE_WITHOUT_FOOTER);\n\n        // We want to have CHUNK_ALIGN or better alignment\n        let mut align = CHUNK_ALIGN;\n\n        // If we already know we need to fulfill some request,\n        // make sure we allocate at least enough to satisfy it\n        align = align.max(requested_layout.align());\n        let requested_size =\n            round_up_to(requested_layout.size(), align).unwrap_or_else(allocation_size_overflow);\n        new_size_without_footer = new_size_without_footer.max(requested_size);\n\n        // We want our allocations to play nice with the memory allocator,\n        // and waste as little memory as possible.\n        // For small allocations, this means that the entire allocation\n        // including the chunk footer and mallocs internal overhead is\n        // as close to a power of two as we can go without going over.\n        // For larger allocations, we only need to get close to a page\n        // boundary without going over.\n        if new_size_without_footer < PAGE_STRATEGY_CUTOFF {\n            new_size_without_footer =\n                (new_size_without_footer + OVERHEAD).next_power_of_two() - OVERHEAD;\n        } else {\n            new_size_without_footer =\n                round_up_to(new_size_without_footer + OVERHEAD, 0x1000)? - OVERHEAD;\n        }\n\n        debug_assert_eq!(align % CHUNK_ALIGN, 0);\n        debug_assert_eq!(new_size_without_footer % CHUNK_ALIGN, 0);\n        let size = new_size_without_footer\n            .checked_add(FOOTER_SIZE)\n            .unwrap_or_else(allocation_size_overflow);\n\n        Some(NewChunkMemoryDetails {\n            new_size_without_footer,\n            size,\n            align,\n        })\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::reset":["/// Reset this bump allocator.\n///\n/// Performs mass deallocation on everything allocated in this arena by\n/// resetting the pointer into the underlying chunk of memory to the start\n/// of the chunk. Does not run any `Drop` implementations on deallocated\n/// objects; see [the top-level documentation](struct.Bump.html) for details.\n///\n/// If this arena has allocated multiple chunks to bump allocate into, then\n/// the excess chunks are returned to the global allocator.\n///\n/// ## Example\n///\n/// ```\n/// let mut bump = bumpalo::Bump::new();\n///\n/// // Allocate a bunch of things.\n/// {\n///     for i in 0..100 {\n///         bump.alloc(i);\n///     }\n/// }\n///\n/// // Reset the arena.\n/// bump.reset();\n///\n/// // Allocate some new things in the space previously occupied by the\n/// // original things.\n/// for j in 200..400 {\n///     bump.alloc(j);\n/// }\n///```\npub fn reset(&mut self){\n        // Takes `&mut self` so `self` must be unique and there can't be any\n        // borrows active that would get invalidated by resetting.\n        unsafe {\n            if self.current_chunk_footer.get().as_ref().is_empty() {\n                return;\n            }\n\n            let mut cur_chunk = self.current_chunk_footer.get();\n\n            // Deallocate all chunks except the current one\n            let prev_chunk = cur_chunk.as_ref().prev.replace(EMPTY_CHUNK.get());\n            dealloc_chunk_list(prev_chunk);\n\n            // Reset the bump finger to the end of the chunk.\n            cur_chunk.as_ref().ptr.set(cur_chunk.cast());\n\n            // Reset the allocated size of the chunk.\n            cur_chunk.as_mut().allocated_bytes = cur_chunk.as_ref().layout.size();\n\n            debug_assert!(\n                self.current_chunk_footer\n                    .get()\n                    .as_ref()\n                    .prev\n                    .get()\n                    .as_ref()\n                    .is_empty(),\n                \"We should only have a single chunk\"\n            );\n            debug_assert_eq!(\n                self.current_chunk_footer.get().as_ref().ptr.get(),\n                self.current_chunk_footer.get().cast(),\n                \"Our chunk's bump finger should be reset to the start of its allocation\"\n            );\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::set_allocation_limit":["/// Set the allocation limit in bytes for this arena.\n///\n/// The allocation limit is only enforced when allocating new backing chunks for\n/// a `Bump`. Updating the allocation limit will not affect existing allocations\n/// or any future allocations within the `Bump`'s current chunk.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::with_capacity(0);\n///\n/// bump.set_allocation_limit(Some(0));\n///\n/// assert!(bump.try_alloc(5).is_err());\n/// ```\npub fn set_allocation_limit(&self, limit: Option<usize>){\n        self.allocation_limit.set(limit)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::shrink":["#[inline]\nunsafe fn shrink(\n        &self,\n        ptr: NonNull<u8>,\n        old_layout: Layout,\n        new_layout: Layout,\n    ) -> Result<NonNull<u8>, AllocErr>{\n        let old_size = old_layout.size();\n        let new_size = new_layout.size();\n        let align_is_compatible = old_layout.align() >= new_layout.align();\n\n        if !align_is_compatible {\n            return Err(AllocErr);\n        }\n\n        // This is how much space we would *actually* reclaim while satisfying\n        // the requested alignment.\n        let delta = round_down_to(old_size - new_size, new_layout.align());\n\n        if self.is_last_allocation(ptr)\n                // Only reclaim the excess space (which requires a copy) if it\n                // is worth it: we are actually going to recover \"enough\" space\n                // and we can do a non-overlapping copy.\n                && delta >= old_size / 2\n        {\n            let footer = self.current_chunk_footer.get();\n            let footer = footer.as_ref();\n\n            // NB: new_ptr is aligned, because ptr *has to* be aligned, and we\n            // made sure delta is aligned.\n            let new_ptr = NonNull::new_unchecked(footer.ptr.get().as_ptr().add(delta));\n            footer.ptr.set(new_ptr);\n\n            // NB: we know it is non-overlapping because of the size check\n            // in the `if` condition.\n            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), new_size);\n\n            return Ok(new_ptr);\n        } else {\n            return Ok(ptr);\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc":["/// Try to allocate an object in this `Bump` and return an exclusive\n/// reference to it.\n///\n/// ## Errors\n///\n/// Errors if reserving space for `T` fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.try_alloc(\"hello\");\n/// assert_eq!(x, Ok(&mut \"hello\"));\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn try_alloc<T>(&self, val: T) -> Result<&mut T, AllocErr>{\n        self.try_alloc_with(|| val)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc_layout":["/// Attempts to allocate space for an object with the given `Layout` or else returns\n/// an `Err`.\n///\n/// The returned pointer points at uninitialized memory, and should be\n/// initialized with\n/// [`std::ptr::write`](https://doc.rust-lang.org/std/ptr/fn.write.html).\n///\n/// # Errors\n///\n/// Errors if reserving space matching `layout` fails.\n#[inline(always)]\npub fn try_alloc_layout(&self, layout: Layout) -> Result<NonNull<u8>, AllocErr>{\n        if let Some(p) = self.try_alloc_layout_fast(layout) {\n            Ok(p)\n        } else {\n            self.alloc_layout_slow(layout).ok_or(AllocErr)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc_layout_fast":["#[inline(always)]\nfn try_alloc_layout_fast(&self, layout: Layout) -> Option<NonNull<u8>>{\n        // We don't need to check for ZSTs here since they will automatically\n        // be handled properly: the pointer will be bumped by zero bytes,\n        // modulo alignment. This keeps the fast path optimized for non-ZSTs,\n        // which are much more common.\n        unsafe {\n            let footer = self.current_chunk_footer.get();\n            let footer = footer.as_ref();\n            let ptr = footer.ptr.get().as_ptr();\n            let start = footer.data.as_ptr();\n            debug_assert!(start <= ptr);\n            debug_assert!(ptr as *const u8 <= footer as *const _ as *const u8);\n\n            if (ptr as usize) < layout.size() {\n                return None;\n            }\n\n            let ptr = ptr.wrapping_sub(layout.size());\n            let rem = ptr as usize % layout.align();\n            let aligned_ptr = ptr.wrapping_sub(rem);\n\n            if aligned_ptr >= start {\n                let aligned_ptr = NonNull::new_unchecked(aligned_ptr as *mut u8);\n                footer.ptr.set(aligned_ptr);\n                Some(aligned_ptr)\n            } else {\n                None\n            }\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc_try_with":["/// Tries to pre-allocates space for a [`Result`] in this `Bump`,\n/// initializes it using the closure, then returns an exclusive reference\n/// to its `T` if all [`Ok`].\n///\n/// Iff the allocation fails, the closure is not run.\n///\n/// Iff the closure returns [`Err`], an allocator rewind is *attempted* and\n/// the `E` instance is moved out of the allocator to be consumed or dropped\n/// as normal.\n///\n/// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n/// discussion on the differences between the `_with` suffixed methods and\n/// those methods without it, their performance characteristics, and when\n/// you might or might not choose a `_with` suffixed method.\n///\n/// For caveats specific to fallible initialization, see\n/// [The `_try_with` Method Suffix](#fallible-initialization-the-_try_with-method-suffix).\n///\n/// [`Result`]: https://doc.rust-lang.org/std/result/enum.Result.html\n/// [`Ok`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Ok\n/// [`Err`]: https://doc.rust-lang.org/std/result/enum.Result.html#variant.Err\n///\n/// ## Errors\n///\n/// Errors with the [`Alloc`](`AllocOrInitError::Alloc`) variant iff\n/// reserving space for `Result<T, E>` fails.\n///\n/// Iff the allocation succeeds but `f` fails, that error is forwarded by\n/// value inside the [`Init`](`AllocOrInitError::Init`) variant.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.try_alloc_try_with(|| Ok(\"hello\"))?;\n/// assert_eq!(*x, \"hello\");\n/// # Result::<_, bumpalo::AllocOrInitError<()>>::Ok(())\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn try_alloc_try_with<F, T, E>(&self, f: F) -> Result<&mut T, AllocOrInitError<E>>\n    where\n        F: FnOnce() -> Result<T, E>,{\n        let rewind_footer = self.current_chunk_footer.get();\n        let rewind_ptr = unsafe { rewind_footer.as_ref() }.ptr.get();\n        let mut inner_result_ptr = NonNull::from(self.try_alloc_with(f)?);\n        match unsafe { inner_result_ptr.as_mut() } {\n            Ok(t) => Ok(unsafe {\n                //SAFETY:\n                // The `&mut Result<T, E>` returned by `alloc_with` may be\n                // lifetime-limited by `E`, but the derived `&mut T` still has\n                // the same validity as in `alloc_with` since the error variant\n                // is already ruled out here.\n\n                // We could conditionally truncate the allocation here, but\n                // since it grows backwards, it seems unlikely that we'd get\n                // any more than the `Result`'s discriminant this way, if\n                // anything at all.\n                &mut *(t as *mut _)\n            }),\n            Err(e) => unsafe {\n                // If this result was the last allocation in this arena, we can\n                // reclaim its space. In fact, sometimes we can do even better\n                // than simply calling `dealloc` on the result pointer: we can\n                // reclaim any alignment padding we might have added (which\n                // `dealloc` cannot do) if we didn't allocate a new chunk for\n                // this result.\n                if self.is_last_allocation(inner_result_ptr.cast()) {\n                    let current_footer_p = self.current_chunk_footer.get();\n                    let current_ptr = &current_footer_p.as_ref().ptr;\n                    if current_footer_p == rewind_footer {\n                        // It's still the same chunk, so reset the bump pointer\n                        // to its original value upon entry to this method\n                        // (reclaiming any alignment padding we may have\n                        // added).\n                        current_ptr.set(rewind_ptr);\n                    } else {\n                        // We allocated a new chunk for this result.\n                        //\n                        // We know the result is the only allocation in this\n                        // chunk: Any additional allocations since the start of\n                        // this method could only have happened when running\n                        // the initializer function, which is called *after*\n                        // reserving space for this result. Therefore, since we\n                        // already determined via the check above that this\n                        // result was the last allocation, there must not have\n                        // been any other allocations, and this result is the\n                        // only allocation in this chunk.\n                        //\n                        // Because this is the only allocation in this chunk,\n                        // we can reset the chunk's bump finger to the start of\n                        // the chunk.\n                        current_ptr.set(current_footer_p.as_ref().data);\n                    }\n                }\n                //SAFETY:\n                // As we received `E` semantically by value from `f`, we can\n                // just copy that value here as long as we avoid a double-drop\n                // (which can't happen as any specific references to the `E`'s\n                // data in `self` are destroyed when this function returns).\n                //\n                // The order between this and the deallocation doesn't matter\n                // because `Self: !Sync`.\n                Err(AllocOrInitError::Init(ptr::read(e as *const _)))\n            },\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc_with":["/// Tries to pre-allocate space for an object in this `Bump`, initializes\n/// it using the closure, then returns an exclusive reference to it.\n///\n/// See [The `_with` Method Suffix](#initializer-functions-the-_with-method-suffix) for a\n/// discussion on the differences between the `_with` suffixed methods and\n/// those methods without it, their performance characteristics, and when\n/// you might or might not choose a `_with` suffixed method.\n///\n/// ## Errors\n///\n/// Errors if reserving space for `T` fails.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::new();\n/// let x = bump.try_alloc_with(|| \"hello\");\n/// assert_eq!(x, Ok(&mut \"hello\"));\n/// ```\n#[inline(always)]\n#[allow(clippy::mut_from_ref)]\npub fn try_alloc_with<F, T>(&self, f: F) -> Result<&mut T, AllocErr>\n    where\n        F: FnOnce() -> T,{\n        #[inline(always)]\n        unsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,\n        {\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }\n\n        //SAFETY: Self-contained:\n        // `p` is allocated for `T` and then a `T` is written.\n        let layout = Layout::new::<T>();\n        let p = self.try_alloc_layout(layout)?;\n        let p = p.as_ptr() as *mut T;\n\n        unsafe {\n            inner_writer(p, f);\n            Ok(&mut *p)\n        }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_alloc_with::inner_writer":["#[inline(always)]\nunsafe fn inner_writer<T, F>(ptr: *mut T, f: F)\n        where\n            F: FnOnce() -> T,{\n            // This function is translated as:\n            // - allocate space for a T on the stack\n            // - call f() with the return value being put onto this stack space\n            // - memcpy from the stack to the heap\n            //\n            // Ideally we want LLVM to always realize that doing a stack\n            // allocation is unnecessary and optimize the code so it writes\n            // directly into the heap instead. It seems we get it to realize\n            // this most consistently if we put this critical line into it's\n            // own function instead of inlining it into the surrounding code.\n            ptr::write(ptr, f())\n        }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_new":["/// Attempt to construct a new arena to bump allocate into.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::try_new();\n/// # let _ = bump.unwrap();\n/// ```\npub fn try_new() -> Result<Bump, AllocErr>{\n        Bump::try_with_capacity(0)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::try_with_capacity":["/// Attempt to construct a new arena with the specified byte capacity to bump allocate into.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::try_with_capacity(100);\n/// # let _ = bump.unwrap();\n/// ```\npub fn try_with_capacity(capacity: usize) -> Result<Self, AllocErr>{\n        if capacity == 0 {\n            return Ok(Bump {\n                current_chunk_footer: Cell::new(EMPTY_CHUNK.get()),\n                allocation_limit: Cell::new(None),\n            });\n        }\n\n        let layout = unsafe { layout_from_size_align(capacity, 1) };\n\n        let chunk_footer = unsafe {\n            Self::new_chunk(\n                Bump::new_chunk_memory_details(None, layout).ok_or(AllocErr)?,\n                layout,\n                EMPTY_CHUNK.get(),\n            )\n            .ok_or(AllocErr)?\n        };\n\n        Ok(Bump {\n            current_chunk_footer: Cell::new(chunk_footer),\n            allocation_limit: Cell::new(None),\n        })\n    }","Real(LocalPath(\"src/lib.rs\"))"],"Bump::with_capacity":["/// Construct a new arena with the specified byte capacity to bump allocate into.\n///\n/// ## Example\n///\n/// ```\n/// let bump = bumpalo::Bump::with_capacity(100);\n/// # let _ = bump;\n/// ```\npub fn with_capacity(capacity: usize) -> Bump{\n        Bump::try_with_capacity(capacity).unwrap_or_else(|_| oom())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ChunkFooter":["#[repr(C)]\nstruct ChunkFooter {\n    // Pointer to the start of this chunk allocation. This footer is always at\n    // the end of the chunk.\n    data: NonNull<u8>,\n\n    // The layout of this chunk's allocation.\n    layout: Layout,\n\n    // Link to the previous chunk.\n    //\n    // Note that the last node in the `prev` linked list is the canonical empty\n    // chunk, whose `prev` link points to itself.\n    prev: Cell<NonNull<ChunkFooter>>,\n\n    // Bump allocation finger that is always in the range `self.data..=self`.\n    ptr: Cell<NonNull<u8>>,\n\n    // The bytes allocated in all chunks so far, the canonical empty chunk has\n    // a size of 0 and for all other chunks, `allocated_bytes` will be\n    // the allocated_bytes of the current chunk plus the allocated bytes\n    // of the `prev` chunk.\n    allocated_bytes: usize,\n}","Real(LocalPath(\"src/lib.rs\"))"],"ChunkFooter::as_raw_parts":["fn as_raw_parts(&self) -> (*const u8, usize){\n        let data = self.data.as_ptr() as *const u8;\n        let ptr = self.ptr.get().as_ptr() as *const u8;\n        debug_assert!(data <= ptr);\n        debug_assert!(ptr <= self as *const ChunkFooter as *const u8);\n        let len = unsafe { (self as *const ChunkFooter as *const u8).offset_from(ptr) as usize };\n        (ptr, len)\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ChunkFooter::is_empty":["/// Is this chunk the last empty chunk?\nfn is_empty(&self) -> bool{\n        ptr::eq(self, EMPTY_CHUNK.get().as_ptr())\n    }","Real(LocalPath(\"src/lib.rs\"))"],"ChunkIter":["/// An iterator over each chunk of allocated memory that\n/// an arena has bump allocated into.\n///\n/// The chunks are returned ordered by allocation time, with the most recently\n/// allocated chunk being returned first.\n///\n/// The values inside each chunk are also ordered by allocation time, with the most\n/// recent allocation being earlier in the slice.\n///\n/// This struct is created by the [`iter_allocated_chunks`] method on\n/// [`Bump`]. See that function for a safety description regarding reading from the returned items.\n///\n/// [`Bump`]: struct.Bump.html\n/// [`iter_allocated_chunks`]: struct.Bump.html#method.iter_allocated_chunks\npub struct ChunkIter<'a> {\n    raw: ChunkRawIter<'a>,\n    bump: PhantomData<&'a mut Bump>,\n}","Real(LocalPath(\"src/lib.rs\"))"],"ChunkRawIter":["/// An iterator over raw pointers to chunks of allocated memory that this\n/// arena has bump allocated into.\n///\n/// See [`ChunkIter`] for details regarding the returned chunks.\n///\n/// This struct is created by the [`iter_allocated_chunks_raw`] method on\n/// [`Bump`]. See that function for a safety description regarding reading from\n/// the returned items.\n///\n/// [`Bump`]: struct.Bump.html\n/// [`iter_allocated_chunks_raw`]: struct.Bump.html#method.iter_allocated_chunks_raw\npub struct ChunkRawIter<'a> {\n    footer: NonNull<ChunkFooter>,\n    bump: PhantomData<&'a Bump>,\n}","Real(LocalPath(\"src/lib.rs\"))"],"EmptyChunkFooter":["/// A wrapper type for the canonical, statically allocated empty chunk.\n///\n/// For the canonical empty chunk to be `static`, its type must be `Sync`, which\n/// is the purpose of this wrapper type. This is safe because the empty chunk is\n/// immutable and never actually modified.\n#[repr(transparent)]\nstruct EmptyChunkFooter(ChunkFooter);","Real(LocalPath(\"src/lib.rs\"))"],"EmptyChunkFooter::get":["fn get(&'static self) -> NonNull<ChunkFooter>{\n        unsafe { NonNull::new_unchecked(&self.0 as *const ChunkFooter as *mut ChunkFooter) }\n    }","Real(LocalPath(\"src/lib.rs\"))"],"NewChunkMemoryDetails":["/// The memory size and alignment details for a potential new chunk\n/// allocation.\nstruct NewChunkMemoryDetails {\n    new_size_without_footer: usize,\n    align: usize,\n    size: usize,\n}","Real(LocalPath(\"src/lib.rs\"))"],"abs_diff":["fn abs_diff(a: usize, b: usize) -> usize{\n    usize::max(a, b) - usize::min(a, b)\n}","Real(LocalPath(\"src/lib.rs\"))"],"alloc::Alloc":["/// An implementation of `Alloc` can allocate, reallocate, and\n/// deallocate arbitrary blocks of data described via `Layout`.\n///\n/// Some of the methods require that a memory block be *currently\n/// allocated* via an allocator. This means that:\n///\n/// * the starting address for that memory block was previously\n///   returned by a previous call to an allocation method (`alloc`,\n///   `alloc_zeroed`, `alloc_excess`, `alloc_one`, `alloc_array`) or\n///   reallocation method (`realloc`, `realloc_excess`, or\n///   `realloc_array`), and\n///\n/// * the memory block has not been subsequently deallocated, where\n///   blocks are deallocated either by being passed to a deallocation\n///   method (`dealloc`, `dealloc_one`, `dealloc_array`) or by being\n///   passed to a reallocation method (see above) that returns `Ok`.\n///\n/// A note regarding zero-sized types and zero-sized layouts: many\n/// methods in the `Alloc` trait state that allocation requests\n/// must be non-zero size, or else undefined behavior can result.\n///\n/// * However, some higher-level allocation methods (`alloc_one`,\n///   `alloc_array`) are well-defined on zero-sized types and can\n///   optionally support them: it is left up to the implementor\n///   whether to return `Err`, or to return `Ok` with some pointer.\n///\n/// * If an `Alloc` implementation chooses to return `Ok` in this\n///   case (i.e. the pointer denotes a zero-sized inaccessible block)\n///   then that returned pointer must be considered \"currently\n///   allocated\". On such an allocator, *all* methods that take\n///   currently-allocated pointers as inputs must accept these\n///   zero-sized pointers, *without* causing undefined behavior.\n///\n/// * In other words, if a zero-sized pointer can flow out of an\n///   allocator, then that allocator must likewise accept that pointer\n///   flowing back into its deallocation and reallocation methods.\n///\n/// Some of the methods require that a layout *fit* a memory block.\n/// What it means for a layout to \"fit\" a memory block means (or\n/// equivalently, for a memory block to \"fit\" a layout) is that the\n/// following two conditions must hold:\n///\n/// 1. The block's starting address must be aligned to `layout.align()`.\n///\n/// 2. The block's size must fall in the range `[use_min, use_max]`, where:\n///\n///    * `use_min` is `self.usable_size(layout).0`, and\n///\n///    * `use_max` is the capacity that was (or would have been)\n///      returned when (if) the block was allocated via a call to\n///      `alloc_excess` or `realloc_excess`.\n///\n/// Note that:\n///\n///  * the size of the layout most recently used to allocate the block\n///    is guaranteed to be in the range `[use_min, use_max]`, and\n///\n///  * a lower-bound on `use_max` can be safely approximated by a call to\n///    `usable_size`.\n///\n///  * if a layout `k` fits a memory block (denoted by `ptr`)\n///    currently allocated via an allocator `a`, then it is legal to\n///    use that layout to deallocate it, i.e. `a.dealloc(ptr, k);`.\n///\n/// # Unsafety\n///\n/// The `Alloc` trait is an `unsafe` trait for a number of reasons, and\n/// implementors must ensure that they adhere to these contracts:\n///\n/// * Pointers returned from allocation functions must point to valid memory and\n///   retain their validity until at least the instance of `Alloc` is dropped\n///   itself.\n///\n/// * `Layout` queries and calculations in general must be correct. Callers of\n///   this trait are allowed to rely on the contracts defined on each method,\n///   and implementors must ensure such contracts remain true.\n///\n/// Note that this list may get tweaked over time as clarifications are made in\n/// the future.\npub unsafe trait Alloc {\n    // (Note: some existing allocators have unspecified but well-defined\n    // behavior in response to a zero size allocation request ;\n    // e.g. in C, `malloc` of 0 will either return a null pointer or a\n    // unique pointer, but will not have arbitrary undefined\n    // behavior.\n    // However in jemalloc for example,\n    // `mallocx(0)` is documented as undefined behavior.)\n\n    /// Returns a pointer meeting the size and alignment guarantees of\n    /// `layout`.\n    ///\n    /// If this method returns an `Ok(addr)`, then the `addr` returned\n    /// will be non-null address pointing to a block of storage\n    /// suitable for holding an instance of `layout`.\n    ///\n    /// The returned block of storage may or may not have its contents\n    /// initialized. (Extension subtraits might restrict this\n    /// behavior, e.g. to ensure initialization to particular sets of\n    /// bit patterns.)\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure that `layout` has non-zero size.\n    ///\n    /// (Extension subtraits might provide more specific bounds on\n    /// behavior, e.g. guarantee a sentinel address or a null pointer\n    /// in response to a zero-size allocation request.)\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `layout` does not meet allocator's size or alignment\n    /// constraints.\n    ///\n    /// Implementations are encouraged to return `Err` on memory\n    /// exhaustion rather than panicking or aborting, but this is not\n    /// a strict requirement. (Specifically: it is *legal* to\n    /// implement this trait atop an underlying native allocation\n    /// library that aborts on memory exhaustion.)\n    ///\n    /// Clients wishing to abort computation in response to an\n    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn alloc(&mut self, layout: Layout) -> Result<NonNull<u8>, AllocErr>;\n\n    /// Deallocate the memory referenced by `ptr`.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure all of the following:\n    ///\n    /// * `ptr` must denote a block of memory currently allocated via\n    ///   this allocator,\n    ///\n    /// * `layout` must *fit* that block of memory,\n    ///\n    /// * In addition to fitting the block of memory `layout`, the\n    ///   alignment of the `layout` must match the alignment used\n    ///   to allocate that block of memory.\n    unsafe fn dealloc(&mut self, ptr: NonNull<u8>, layout: Layout);\n\n    // == ALLOCATOR-SPECIFIC QUANTITIES AND LIMITS ==\n    // usable_size\n\n    /// Returns bounds on the guaranteed usable size of a successful\n    /// allocation created with the specified `layout`.\n    ///\n    /// In particular, if one has a memory block allocated via a given\n    /// allocator `a` and layout `k` where `a.usable_size(k)` returns\n    /// `(l, u)`, then one can pass that block to `a.dealloc()` with a\n    /// layout in the size range [l, u].\n    ///\n    /// (All implementors of `usable_size` must ensure that\n    /// `l <= k.size() <= u`)\n    ///\n    /// Both the lower- and upper-bounds (`l` and `u` respectively)\n    /// are provided, because an allocator based on size classes could\n    /// misbehave if one attempts to deallocate a block without\n    /// providing a correct value for its size (i.e., one within the\n    /// range `[l, u]`).\n    ///\n    /// Clients who wish to make use of excess capacity are encouraged\n    /// to use the `alloc_excess` and `realloc_excess` instead, as\n    /// this method is constrained to report conservative values that\n    /// serve as valid bounds for *all possible* allocation method\n    /// calls.\n    ///\n    /// However, for clients that do not wish to track the capacity\n    /// returned by `alloc_excess` locally, this method is likely to\n    /// produce useful results.\n    #[inline]\n    fn usable_size(&self, layout: &Layout) -> (usize, usize) {\n        (layout.size(), layout.size())\n    }\n\n    // == METHODS FOR MEMORY REUSE ==\n    // realloc. alloc_excess, realloc_excess\n\n    /// Returns a pointer suitable for holding data described by\n    /// a new layout with `layout`’s alignment and a size given\n    /// by `new_size`. To\n    /// accomplish this, this may extend or shrink the allocation\n    /// referenced by `ptr` to fit the new layout.\n    ///\n    /// If this returns `Ok`, then ownership of the memory block\n    /// referenced by `ptr` has been transferred to this\n    /// allocator. The memory may or may not have been freed, and\n    /// should be considered unusable (unless of course it was\n    /// transferred back to the caller again via the return value of\n    /// this method).\n    ///\n    /// If this method returns `Err`, then ownership of the memory\n    /// block has not been transferred to this allocator, and the\n    /// contents of the memory block are unaltered.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure all of the following:\n    ///\n    /// * `ptr` must be currently allocated via this allocator,\n    ///\n    /// * `layout` must *fit* the `ptr` (see above). (The `new_size`\n    ///   argument need not fit it.)\n    ///\n    /// * `new_size` must be greater than zero.\n    ///\n    /// * `new_size`, when rounded up to the nearest multiple of `layout.align()`,\n    ///   must not overflow (i.e. the rounded value must be less than `usize::MAX`).\n    ///\n    /// (Extension subtraits might provide more specific bounds on\n    /// behavior, e.g. guarantee a sentinel address or a null pointer\n    /// in response to a zero-size allocation request.)\n    ///\n    /// # Errors\n    ///\n    /// Returns `Err` only if the new layout\n    /// does not meet the allocator's size\n    /// and alignment constraints of the allocator, or if reallocation\n    /// otherwise fails.\n    ///\n    /// Implementations are encouraged to return `Err` on memory\n    /// exhaustion rather than panicking or aborting, but this is not\n    /// a strict requirement. (Specifically: it is *legal* to\n    /// implement this trait atop an underlying native allocation\n    /// library that aborts on memory exhaustion.)\n    ///\n    /// Clients wishing to abort computation in response to a\n    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn realloc(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<NonNull<u8>, AllocErr> {\n        let old_size = layout.size();\n\n        if new_size >= old_size {\n            if let Ok(()) = self.grow_in_place(ptr, layout, new_size) {\n                return Ok(ptr);\n            }\n        } else if new_size < old_size {\n            if let Ok(()) = self.shrink_in_place(ptr, layout, new_size) {\n                return Ok(ptr);\n            }\n        }\n\n        // otherwise, fall back on alloc + copy + dealloc.\n        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n        let result = self.alloc(new_layout);\n        if let Ok(new_ptr) = result {\n            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), cmp::min(old_size, new_size));\n            self.dealloc(ptr, layout);\n        }\n        result\n    }\n\n    /// Behaves like `alloc`, but also ensures that the contents\n    /// are set to zero before being returned.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe for the same reasons that `alloc` is.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `layout` does not meet allocator's size or alignment\n    /// constraints, just as in `alloc`.\n    ///\n    /// Clients wishing to abort computation in response to an\n    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn alloc_zeroed(&mut self, layout: Layout) -> Result<NonNull<u8>, AllocErr> {\n        let size = layout.size();\n        let p = self.alloc(layout);\n        if let Ok(p) = p {\n            ptr::write_bytes(p.as_ptr(), 0, size);\n        }\n        p\n    }\n\n    /// Behaves like `alloc`, but also returns the whole size of\n    /// the returned block. For some `layout` inputs, like arrays, this\n    /// may include extra storage usable for additional data.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe for the same reasons that `alloc` is.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `layout` does not meet allocator's size or alignment\n    /// constraints, just as in `alloc`.\n    ///\n    /// Clients wishing to abort computation in response to an\n    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn alloc_excess(&mut self, layout: Layout) -> Result<Excess, AllocErr> {\n        let usable_size = self.usable_size(&layout);\n        self.alloc(layout).map(|p| Excess(p, usable_size.1))\n    }\n\n    /// Behaves like `realloc`, but also returns the whole size of\n    /// the returned block. For some `layout` inputs, like arrays, this\n    /// may include extra storage usable for additional data.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe for the same reasons that `realloc` is.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `layout` does not meet allocator's size or alignment\n    /// constraints, just as in `realloc`.\n    ///\n    /// Clients wishing to abort computation in response to a\n    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn realloc_excess(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<Excess, AllocErr> {\n        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n        let usable_size = self.usable_size(&new_layout);\n        self.realloc(ptr, layout, new_size)\n            .map(|p| Excess(p, usable_size.1))\n    }\n\n    /// Attempts to extend the allocation referenced by `ptr` to fit `new_size`.\n    ///\n    /// If this returns `Ok`, then the allocator has asserted that the\n    /// memory block referenced by `ptr` now fits `new_size`, and thus can\n    /// be used to carry data of a layout of that size and same alignment as\n    /// `layout`. (The allocator is allowed to\n    /// expend effort to accomplish this, such as extending the memory block to\n    /// include successor blocks, or virtual memory tricks.)\n    ///\n    /// Regardless of what this method returns, ownership of the\n    /// memory block referenced by `ptr` has not been transferred, and\n    /// the contents of the memory block are unaltered.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure all of the following:\n    ///\n    /// * `ptr` must be currently allocated via this allocator,\n    ///\n    /// * `layout` must *fit* the `ptr` (see above); note the\n    ///   `new_size` argument need not fit it,\n    ///\n    /// * `new_size` must not be less than `layout.size()`,\n    ///\n    /// # Errors\n    ///\n    /// Returns `Err(CannotReallocInPlace)` when the allocator is\n    /// unable to assert that the memory block referenced by `ptr`\n    /// could fit `layout`.\n    ///\n    /// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n    /// function; clients are expected either to be able to recover from\n    /// `grow_in_place` failures without aborting, or to fall back on\n    /// another reallocation method before resorting to an abort.\n    unsafe fn grow_in_place(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<(), CannotReallocInPlace> {\n        let _ = ptr; // this default implementation doesn't care about the actual address.\n        debug_assert!(new_size >= layout.size());\n        let (_l, u) = self.usable_size(&layout);\n        // _l <= layout.size()                       [guaranteed by usable_size()]\n        //       layout.size() <= new_layout.size()  [required by this method]\n        if new_size <= u {\n            Ok(())\n        } else {\n            Err(CannotReallocInPlace)\n        }\n    }\n\n    /// Attempts to shrink the allocation referenced by `ptr` to fit `new_size`.\n    ///\n    /// If this returns `Ok`, then the allocator has asserted that the\n    /// memory block referenced by `ptr` now fits `new_size`, and\n    /// thus can only be used to carry data of that smaller\n    /// layout. (The allocator is allowed to take advantage of this,\n    /// carving off portions of the block for reuse elsewhere.) The\n    /// truncated contents of the block within the smaller layout are\n    /// unaltered, and ownership of block has not been transferred.\n    ///\n    /// If this returns `Err`, then the memory block is considered to\n    /// still represent the original (larger) `layout`. None of the\n    /// block has been carved off for reuse elsewhere, ownership of\n    /// the memory block has not been transferred, and the contents of\n    /// the memory block are unaltered.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure all of the following:\n    ///\n    /// * `ptr` must be currently allocated via this allocator,\n    ///\n    /// * `layout` must *fit* the `ptr` (see above); note the\n    ///   `new_size` argument need not fit it,\n    ///\n    /// * `new_size` must not be greater than `layout.size()`\n    ///   (and must be greater than zero),\n    ///\n    /// # Errors\n    ///\n    /// Returns `Err(CannotReallocInPlace)` when the allocator is\n    /// unable to assert that the memory block referenced by `ptr`\n    /// could fit `layout`.\n    ///\n    /// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n    /// function; clients are expected either to be able to recover from\n    /// `shrink_in_place` failures without aborting, or to fall back\n    /// on another reallocation method before resorting to an abort.\n    unsafe fn shrink_in_place(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<(), CannotReallocInPlace> {\n        let _ = ptr; // this default implementation doesn't care about the actual address.\n        debug_assert!(new_size <= layout.size());\n        let (l, _u) = self.usable_size(&layout);\n        //                      layout.size() <= _u  [guaranteed by usable_size()]\n        // new_layout.size() <= layout.size()        [required by this method]\n        if l <= new_size {\n            Ok(())\n        } else {\n            Err(CannotReallocInPlace)\n        }\n    }\n\n    // == COMMON USAGE PATTERNS ==\n    // alloc_one, dealloc_one, alloc_array, realloc_array. dealloc_array\n\n    /// Allocates a block suitable for holding an instance of `T`.\n    ///\n    /// Captures a common usage pattern for allocators.\n    ///\n    /// The returned block is suitable for passing to the\n    /// `alloc`/`realloc` methods of this allocator.\n    ///\n    /// Note to implementors: If this returns `Ok(ptr)`, then `ptr`\n    /// must be considered \"currently allocated\" and must be\n    /// acceptable input to methods such as `realloc` or `dealloc`,\n    /// *even if* `T` is a zero-sized type. In other words, if your\n    /// `Alloc` implementation overrides this method in a manner\n    /// that can return a zero-sized `ptr`, then all reallocation and\n    /// deallocation methods need to be similarly overridden to accept\n    /// such values as input.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `T` does not meet allocator's size or alignment constraints.\n    ///\n    /// For zero-sized `T`, may return either of `Ok` or `Err`, but\n    /// will *not* yield undefined behavior.\n    ///\n    /// Clients wishing to abort computation in response to an\n    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    fn alloc_one<T>(&mut self) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,\n    {\n        let k = Layout::new::<T>();\n        if k.size() > 0 {\n            unsafe { self.alloc(k).map(|p| p.cast()) }\n        } else {\n            Err(AllocErr)\n        }\n    }\n\n    /// Deallocates a block suitable for holding an instance of `T`.\n    ///\n    /// The given block must have been produced by this allocator,\n    /// and must be suitable for storing a `T` (in terms of alignment\n    /// as well as minimum and maximum size); otherwise yields\n    /// undefined behavior.\n    ///\n    /// Captures a common usage pattern for allocators.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure both:\n    ///\n    /// * `ptr` must denote a block of memory currently allocated via this allocator\n    ///\n    /// * the layout of `T` must *fit* that block of memory.\n    unsafe fn dealloc_one<T>(&mut self, ptr: NonNull<T>)\n    where\n        Self: Sized,\n    {\n        let k = Layout::new::<T>();\n        if k.size() > 0 {\n            self.dealloc(ptr.cast(), k);\n        }\n    }\n\n    /// Allocates a block suitable for holding `n` instances of `T`.\n    ///\n    /// Captures a common usage pattern for allocators.\n    ///\n    /// The returned block is suitable for passing to the\n    /// `alloc`/`realloc` methods of this allocator.\n    ///\n    /// Note to implementors: If this returns `Ok(ptr)`, then `ptr`\n    /// must be considered \"currently allocated\" and must be\n    /// acceptable input to methods such as `realloc` or `dealloc`,\n    /// *even if* `T` is a zero-sized type. In other words, if your\n    /// `Alloc` implementation overrides this method in a manner\n    /// that can return a zero-sized `ptr`, then all reallocation and\n    /// deallocation methods need to be similarly overridden to accept\n    /// such values as input.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `[T; n]` does not meet allocator's size or alignment\n    /// constraints.\n    ///\n    /// For zero-sized `T` or `n == 0`, may return either of `Ok` or\n    /// `Err`, but will *not* yield undefined behavior.\n    ///\n    /// Always returns `Err` on arithmetic overflow.\n    ///\n    /// Clients wishing to abort computation in response to an\n    /// allocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    fn alloc_array<T>(&mut self, n: usize) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,\n    {\n        match Layout::array::<T>(n) {\n            Ok(layout) if layout.size() > 0 => unsafe { self.alloc(layout).map(|p| p.cast()) },\n            _ => Err(AllocErr),\n        }\n    }\n\n    /// Reallocates a block previously suitable for holding `n_old`\n    /// instances of `T`, returning a block suitable for holding\n    /// `n_new` instances of `T`.\n    ///\n    /// Captures a common usage pattern for allocators.\n    ///\n    /// The returned block is suitable for passing to the\n    /// `alloc`/`realloc` methods of this allocator.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure all of the following:\n    ///\n    /// * `ptr` must be currently allocated via this allocator,\n    ///\n    /// * the layout of `[T; n_old]` must *fit* that block of memory.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either memory is exhausted or\n    /// `[T; n_new]` does not meet allocator's size or alignment\n    /// constraints.\n    ///\n    /// For zero-sized `T` or `n_new == 0`, may return either of `Ok` or\n    /// `Err`, but will *not* yield undefined behavior.\n    ///\n    /// Always returns `Err` on arithmetic overflow.\n    ///\n    /// Clients wishing to abort computation in response to a\n    /// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n    /// rather than directly invoking `panic!` or similar.\n    ///\n    /// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\n    unsafe fn realloc_array<T>(\n        &mut self,\n        ptr: NonNull<T>,\n        n_old: usize,\n        n_new: usize,\n    ) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,\n    {\n        match (Layout::array::<T>(n_old), Layout::array::<T>(n_new)) {\n            (Ok(ref k_old), Ok(ref k_new)) if k_old.size() > 0 && k_new.size() > 0 => {\n                debug_assert!(k_old.align() == k_new.align());\n                self.realloc(ptr.cast(), k_old.clone(), k_new.size())\n                    .map(NonNull::cast)\n            }\n            _ => Err(AllocErr),\n        }\n    }\n\n    /// Deallocates a block suitable for holding `n` instances of `T`.\n    ///\n    /// Captures a common usage pattern for allocators.\n    ///\n    /// # Safety\n    ///\n    /// This function is unsafe because undefined behavior can result\n    /// if the caller does not ensure both:\n    ///\n    /// * `ptr` must denote a block of memory currently allocated via this allocator\n    ///\n    /// * the layout of `[T; n]` must *fit* that block of memory.\n    ///\n    /// # Errors\n    ///\n    /// Returning `Err` indicates that either `[T; n]` or the given\n    /// memory block does not meet allocator's size or alignment\n    /// constraints.\n    ///\n    /// Always returns `Err` on arithmetic overflow.\n    unsafe fn dealloc_array<T>(&mut self, ptr: NonNull<T>, n: usize) -> Result<(), AllocErr>\n    where\n        Self: Sized,\n    {\n        match Layout::array::<T>(n) {\n            Ok(k) if k.size() > 0 => {\n                self.dealloc(ptr.cast(), k);\n                Ok(())\n            }\n            _ => Err(AllocErr),\n        }\n    }\n}","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::alloc_array":["/// Allocates a block suitable for holding `n` instances of `T`.\n///\n/// Captures a common usage pattern for allocators.\n///\n/// The returned block is suitable for passing to the\n/// `alloc`/`realloc` methods of this allocator.\n///\n/// Note to implementors: If this returns `Ok(ptr)`, then `ptr`\n/// must be considered \"currently allocated\" and must be\n/// acceptable input to methods such as `realloc` or `dealloc`,\n/// *even if* `T` is a zero-sized type. In other words, if your\n/// `Alloc` implementation overrides this method in a manner\n/// that can return a zero-sized `ptr`, then all reallocation and\n/// deallocation methods need to be similarly overridden to accept\n/// such values as input.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `[T; n]` does not meet allocator's size or alignment\n/// constraints.\n///\n/// For zero-sized `T` or `n == 0`, may return either of `Ok` or\n/// `Err`, but will *not* yield undefined behavior.\n///\n/// Always returns `Err` on arithmetic overflow.\n///\n/// Clients wishing to abort computation in response to an\n/// allocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nfn alloc_array<T>(&mut self, n: usize) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,{\n        match Layout::array::<T>(n) {\n            Ok(layout) if layout.size() > 0 => unsafe { self.alloc(layout).map(|p| p.cast()) },\n            _ => Err(AllocErr),\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::alloc_excess":["/// Behaves like `alloc`, but also returns the whole size of\n/// the returned block. For some `layout` inputs, like arrays, this\n/// may include extra storage usable for additional data.\n///\n/// # Safety\n///\n/// This function is unsafe for the same reasons that `alloc` is.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `layout` does not meet allocator's size or alignment\n/// constraints, just as in `alloc`.\n///\n/// Clients wishing to abort computation in response to an\n/// allocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nunsafe fn alloc_excess(&mut self, layout: Layout) -> Result<Excess, AllocErr>{\n        let usable_size = self.usable_size(&layout);\n        self.alloc(layout).map(|p| Excess(p, usable_size.1))\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::alloc_one":["/// Allocates a block suitable for holding an instance of `T`.\n///\n/// Captures a common usage pattern for allocators.\n///\n/// The returned block is suitable for passing to the\n/// `alloc`/`realloc` methods of this allocator.\n///\n/// Note to implementors: If this returns `Ok(ptr)`, then `ptr`\n/// must be considered \"currently allocated\" and must be\n/// acceptable input to methods such as `realloc` or `dealloc`,\n/// *even if* `T` is a zero-sized type. In other words, if your\n/// `Alloc` implementation overrides this method in a manner\n/// that can return a zero-sized `ptr`, then all reallocation and\n/// deallocation methods need to be similarly overridden to accept\n/// such values as input.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `T` does not meet allocator's size or alignment constraints.\n///\n/// For zero-sized `T`, may return either of `Ok` or `Err`, but\n/// will *not* yield undefined behavior.\n///\n/// Clients wishing to abort computation in response to an\n/// allocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nfn alloc_one<T>(&mut self) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,{\n        let k = Layout::new::<T>();\n        if k.size() > 0 {\n            unsafe { self.alloc(k).map(|p| p.cast()) }\n        } else {\n            Err(AllocErr)\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::alloc_zeroed":["/// Behaves like `alloc`, but also ensures that the contents\n/// are set to zero before being returned.\n///\n/// # Safety\n///\n/// This function is unsafe for the same reasons that `alloc` is.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `layout` does not meet allocator's size or alignment\n/// constraints, just as in `alloc`.\n///\n/// Clients wishing to abort computation in response to an\n/// allocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nunsafe fn alloc_zeroed(&mut self, layout: Layout) -> Result<NonNull<u8>, AllocErr>{\n        let size = layout.size();\n        let p = self.alloc(layout);\n        if let Ok(p) = p {\n            ptr::write_bytes(p.as_ptr(), 0, size);\n        }\n        p\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::dealloc_array":["/// Deallocates a block suitable for holding `n` instances of `T`.\n///\n/// Captures a common usage pattern for allocators.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure both:\n///\n/// * `ptr` must denote a block of memory currently allocated via this allocator\n///\n/// * the layout of `[T; n]` must *fit* that block of memory.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either `[T; n]` or the given\n/// memory block does not meet allocator's size or alignment\n/// constraints.\n///\n/// Always returns `Err` on arithmetic overflow.\nunsafe fn dealloc_array<T>(&mut self, ptr: NonNull<T>, n: usize) -> Result<(), AllocErr>\n    where\n        Self: Sized,{\n        match Layout::array::<T>(n) {\n            Ok(k) if k.size() > 0 => {\n                self.dealloc(ptr.cast(), k);\n                Ok(())\n            }\n            _ => Err(AllocErr),\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::dealloc_one":["/// Deallocates a block suitable for holding an instance of `T`.\n///\n/// The given block must have been produced by this allocator,\n/// and must be suitable for storing a `T` (in terms of alignment\n/// as well as minimum and maximum size); otherwise yields\n/// undefined behavior.\n///\n/// Captures a common usage pattern for allocators.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure both:\n///\n/// * `ptr` must denote a block of memory currently allocated via this allocator\n///\n/// * the layout of `T` must *fit* that block of memory.\nunsafe fn dealloc_one<T>(&mut self, ptr: NonNull<T>)\n    where\n        Self: Sized,{\n        let k = Layout::new::<T>();\n        if k.size() > 0 {\n            self.dealloc(ptr.cast(), k);\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::grow_in_place":["/// Attempts to extend the allocation referenced by `ptr` to fit `new_size`.\n///\n/// If this returns `Ok`, then the allocator has asserted that the\n/// memory block referenced by `ptr` now fits `new_size`, and thus can\n/// be used to carry data of a layout of that size and same alignment as\n/// `layout`. (The allocator is allowed to\n/// expend effort to accomplish this, such as extending the memory block to\n/// include successor blocks, or virtual memory tricks.)\n///\n/// Regardless of what this method returns, ownership of the\n/// memory block referenced by `ptr` has not been transferred, and\n/// the contents of the memory block are unaltered.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure all of the following:\n///\n/// * `ptr` must be currently allocated via this allocator,\n///\n/// * `layout` must *fit* the `ptr` (see above); note the\n///   `new_size` argument need not fit it,\n///\n/// * `new_size` must not be less than `layout.size()`,\n///\n/// # Errors\n///\n/// Returns `Err(CannotReallocInPlace)` when the allocator is\n/// unable to assert that the memory block referenced by `ptr`\n/// could fit `layout`.\n///\n/// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n/// function; clients are expected either to be able to recover from\n/// `grow_in_place` failures without aborting, or to fall back on\n/// another reallocation method before resorting to an abort.\nunsafe fn grow_in_place(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<(), CannotReallocInPlace>{\n        let _ = ptr; // this default implementation doesn't care about the actual address.\n        debug_assert!(new_size >= layout.size());\n        let (_l, u) = self.usable_size(&layout);\n        // _l <= layout.size()                       [guaranteed by usable_size()]\n        //       layout.size() <= new_layout.size()  [required by this method]\n        if new_size <= u {\n            Ok(())\n        } else {\n            Err(CannotReallocInPlace)\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::realloc":["/// Returns a pointer suitable for holding data described by\n/// a new layout with `layout`’s alignment and a size given\n/// by `new_size`. To\n/// accomplish this, this may extend or shrink the allocation\n/// referenced by `ptr` to fit the new layout.\n///\n/// If this returns `Ok`, then ownership of the memory block\n/// referenced by `ptr` has been transferred to this\n/// allocator. The memory may or may not have been freed, and\n/// should be considered unusable (unless of course it was\n/// transferred back to the caller again via the return value of\n/// this method).\n///\n/// If this method returns `Err`, then ownership of the memory\n/// block has not been transferred to this allocator, and the\n/// contents of the memory block are unaltered.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure all of the following:\n///\n/// * `ptr` must be currently allocated via this allocator,\n///\n/// * `layout` must *fit* the `ptr` (see above). (The `new_size`\n///   argument need not fit it.)\n///\n/// * `new_size` must be greater than zero.\n///\n/// * `new_size`, when rounded up to the nearest multiple of `layout.align()`,\n///   must not overflow (i.e. the rounded value must be less than `usize::MAX`).\n///\n/// (Extension subtraits might provide more specific bounds on\n/// behavior, e.g. guarantee a sentinel address or a null pointer\n/// in response to a zero-size allocation request.)\n///\n/// # Errors\n///\n/// Returns `Err` only if the new layout\n/// does not meet the allocator's size\n/// and alignment constraints of the allocator, or if reallocation\n/// otherwise fails.\n///\n/// Implementations are encouraged to return `Err` on memory\n/// exhaustion rather than panicking or aborting, but this is not\n/// a strict requirement. (Specifically: it is *legal* to\n/// implement this trait atop an underlying native allocation\n/// library that aborts on memory exhaustion.)\n///\n/// Clients wishing to abort computation in response to a\n/// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nunsafe fn realloc(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<NonNull<u8>, AllocErr>{\n        let old_size = layout.size();\n\n        if new_size >= old_size {\n            if let Ok(()) = self.grow_in_place(ptr, layout, new_size) {\n                return Ok(ptr);\n            }\n        } else if new_size < old_size {\n            if let Ok(()) = self.shrink_in_place(ptr, layout, new_size) {\n                return Ok(ptr);\n            }\n        }\n\n        // otherwise, fall back on alloc + copy + dealloc.\n        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n        let result = self.alloc(new_layout);\n        if let Ok(new_ptr) = result {\n            ptr::copy_nonoverlapping(ptr.as_ptr(), new_ptr.as_ptr(), cmp::min(old_size, new_size));\n            self.dealloc(ptr, layout);\n        }\n        result\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::realloc_array":["/// Reallocates a block previously suitable for holding `n_old`\n/// instances of `T`, returning a block suitable for holding\n/// `n_new` instances of `T`.\n///\n/// Captures a common usage pattern for allocators.\n///\n/// The returned block is suitable for passing to the\n/// `alloc`/`realloc` methods of this allocator.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure all of the following:\n///\n/// * `ptr` must be currently allocated via this allocator,\n///\n/// * the layout of `[T; n_old]` must *fit* that block of memory.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `[T; n_new]` does not meet allocator's size or alignment\n/// constraints.\n///\n/// For zero-sized `T` or `n_new == 0`, may return either of `Ok` or\n/// `Err`, but will *not* yield undefined behavior.\n///\n/// Always returns `Err` on arithmetic overflow.\n///\n/// Clients wishing to abort computation in response to a\n/// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nunsafe fn realloc_array<T>(\n        &mut self,\n        ptr: NonNull<T>,\n        n_old: usize,\n        n_new: usize,\n    ) -> Result<NonNull<T>, AllocErr>\n    where\n        Self: Sized,{\n        match (Layout::array::<T>(n_old), Layout::array::<T>(n_new)) {\n            (Ok(ref k_old), Ok(ref k_new)) if k_old.size() > 0 && k_new.size() > 0 => {\n                debug_assert!(k_old.align() == k_new.align());\n                self.realloc(ptr.cast(), k_old.clone(), k_new.size())\n                    .map(NonNull::cast)\n            }\n            _ => Err(AllocErr),\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::realloc_excess":["/// Behaves like `realloc`, but also returns the whole size of\n/// the returned block. For some `layout` inputs, like arrays, this\n/// may include extra storage usable for additional data.\n///\n/// # Safety\n///\n/// This function is unsafe for the same reasons that `realloc` is.\n///\n/// # Errors\n///\n/// Returning `Err` indicates that either memory is exhausted or\n/// `layout` does not meet allocator's size or alignment\n/// constraints, just as in `realloc`.\n///\n/// Clients wishing to abort computation in response to a\n/// reallocation error are encouraged to call the [`handle_alloc_error`] function,\n/// rather than directly invoking `panic!` or similar.\n///\n/// [`handle_alloc_error`]: ../../alloc/alloc/fn.handle_alloc_error.html\nunsafe fn realloc_excess(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<Excess, AllocErr>{\n        let new_layout = Layout::from_size_align_unchecked(new_size, layout.align());\n        let usable_size = self.usable_size(&new_layout);\n        self.realloc(ptr, layout, new_size)\n            .map(|p| Excess(p, usable_size.1))\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::shrink_in_place":["/// Attempts to shrink the allocation referenced by `ptr` to fit `new_size`.\n///\n/// If this returns `Ok`, then the allocator has asserted that the\n/// memory block referenced by `ptr` now fits `new_size`, and\n/// thus can only be used to carry data of that smaller\n/// layout. (The allocator is allowed to take advantage of this,\n/// carving off portions of the block for reuse elsewhere.) The\n/// truncated contents of the block within the smaller layout are\n/// unaltered, and ownership of block has not been transferred.\n///\n/// If this returns `Err`, then the memory block is considered to\n/// still represent the original (larger) `layout`. None of the\n/// block has been carved off for reuse elsewhere, ownership of\n/// the memory block has not been transferred, and the contents of\n/// the memory block are unaltered.\n///\n/// # Safety\n///\n/// This function is unsafe because undefined behavior can result\n/// if the caller does not ensure all of the following:\n///\n/// * `ptr` must be currently allocated via this allocator,\n///\n/// * `layout` must *fit* the `ptr` (see above); note the\n///   `new_size` argument need not fit it,\n///\n/// * `new_size` must not be greater than `layout.size()`\n///   (and must be greater than zero),\n///\n/// # Errors\n///\n/// Returns `Err(CannotReallocInPlace)` when the allocator is\n/// unable to assert that the memory block referenced by `ptr`\n/// could fit `layout`.\n///\n/// Note that one cannot pass `CannotReallocInPlace` to the `handle_alloc_error`\n/// function; clients are expected either to be able to recover from\n/// `shrink_in_place` failures without aborting, or to fall back\n/// on another reallocation method before resorting to an abort.\nunsafe fn shrink_in_place(\n        &mut self,\n        ptr: NonNull<u8>,\n        layout: Layout,\n        new_size: usize,\n    ) -> Result<(), CannotReallocInPlace>{\n        let _ = ptr; // this default implementation doesn't care about the actual address.\n        debug_assert!(new_size <= layout.size());\n        let (l, _u) = self.usable_size(&layout);\n        //                      layout.size() <= _u  [guaranteed by usable_size()]\n        // new_layout.size() <= layout.size()        [required by this method]\n        if l <= new_size {\n            Ok(())\n        } else {\n            Err(CannotReallocInPlace)\n        }\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Alloc::usable_size":["/// Returns bounds on the guaranteed usable size of a successful\n/// allocation created with the specified `layout`.\n///\n/// In particular, if one has a memory block allocated via a given\n/// allocator `a` and layout `k` where `a.usable_size(k)` returns\n/// `(l, u)`, then one can pass that block to `a.dealloc()` with a\n/// layout in the size range [l, u].\n///\n/// (All implementors of `usable_size` must ensure that\n/// `l <= k.size() <= u`)\n///\n/// Both the lower- and upper-bounds (`l` and `u` respectively)\n/// are provided, because an allocator based on size classes could\n/// misbehave if one attempts to deallocate a block without\n/// providing a correct value for its size (i.e., one within the\n/// range `[l, u]`).\n///\n/// Clients who wish to make use of excess capacity are encouraged\n/// to use the `alloc_excess` and `realloc_excess` instead, as\n/// this method is constrained to report conservative values that\n/// serve as valid bounds for *all possible* allocation method\n/// calls.\n///\n/// However, for clients that do not wish to track the capacity\n/// returned by `alloc_excess` locally, this method is likely to\n/// produce useful results.\n#[inline]\nfn usable_size(&self, layout: &Layout) -> (usize, usize){\n        (layout.size(), layout.size())\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::AllocErr":["/// The `AllocErr` error indicates an allocation failure\n/// that may be due to resource exhaustion or to\n/// something wrong when combining the given input arguments with this\n/// allocator.\npub struct AllocErr;","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::CannotReallocInPlace":["/// The `CannotReallocInPlace` error is used when `grow_in_place` or\n/// `shrink_in_place` were unable to reuse the given memory block for\n/// a requested layout.\npub struct CannotReallocInPlace;","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::CannotReallocInPlace::description":["pub fn description(&self) -> &str{\n        \"cannot reallocate allocator's memory in place\"\n    }","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::Excess":["/// Represents the combination of a starting address and\n/// a total capacity of the returned block.\npub struct Excess(pub NonNull<u8>, pub usize);","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::UnstableLayoutMethods":["pub trait UnstableLayoutMethods {\n    fn padding_needed_for(&self, align: usize) -> usize;\n    fn repeat(&self, n: usize) -> Result<(Layout, usize), LayoutErr>;\n    fn array<T>(n: usize) -> Result<Layout, LayoutErr>;\n}","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::handle_alloc_error":["pub fn handle_alloc_error(layout: Layout) -> !{\n    panic!(\"encountered allocation error: {:?}\", layout)\n}","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::new_layout_err":["fn new_layout_err() -> LayoutErr{\n    Layout::from_size_align(1, 3).unwrap_err()\n}","Real(LocalPath(\"src/alloc.rs\"))"],"alloc::size_align":["fn size_align<T>() -> (usize, usize){\n    (mem::size_of::<T>(), mem::align_of::<T>())\n}","Real(LocalPath(\"src/alloc.rs\"))"],"allocation_size_overflow":["#[inline(never)]\nfn allocation_size_overflow<T>() -> T{\n    panic!(\"requested allocation size overflowed\")\n}","Real(LocalPath(\"src/lib.rs\"))"],"dealloc_chunk_list":["#[inline]\nunsafe fn dealloc_chunk_list(mut footer: NonNull<ChunkFooter>){\n    while !footer.as_ref().is_empty() {\n        let f = footer;\n        footer = f.as_ref().prev.get();\n        dealloc(f.as_ref().data.as_ptr(), f.as_ref().layout);\n    }\n}","Real(LocalPath(\"src/lib.rs\"))"],"layout_from_size_align":["/// Wrapper around `Layout::from_size_align` that adds debug assertions.\n#[inline]\nunsafe fn layout_from_size_align(size: usize, align: usize) -> Layout{\n    if cfg!(debug_assertions) {\n        Layout::from_size_align(size, align).unwrap()\n    } else {\n        Layout::from_size_align_unchecked(size, align)\n    }\n}","Real(LocalPath(\"src/lib.rs\"))"],"oom":["#[inline(never)]\n#[cold]\nfn oom() -> !{\n    panic!(\"out of memory\")\n}","Real(LocalPath(\"src/lib.rs\"))"],"round_down_to":["#[inline]\npub(crate) fn round_down_to(n: usize, divisor: usize) -> usize{\n    debug_assert!(divisor > 0);\n    debug_assert!(divisor.is_power_of_two());\n    n & !(divisor - 1)\n}","Real(LocalPath(\"src/lib.rs\"))"],"round_up_to":["#[inline]\npub(crate) fn round_up_to(n: usize, divisor: usize) -> Option<usize>{\n    debug_assert!(divisor > 0);\n    debug_assert!(divisor.is_power_of_two());\n    Some(n.checked_add(divisor - 1)? & !(divisor - 1))\n}","Real(LocalPath(\"src/lib.rs\"))"]},"struct_constructor":{"!":["handle_alloc_error","oom"],"&mut [T]":["alloc_slice_clone","alloc_slice_copy","alloc_slice_fill_clone","alloc_slice_fill_copy","alloc_slice_fill_default","alloc_slice_fill_iter","alloc_slice_fill_with"],"&mut str":["alloc_str"],"&str":["description"],"(*const u8, usize)":["as_raw_parts"],"(usize, usize)":["size_align","usable_size"],"AllocOrInitError":["clone","from"],"Bump":["default","new","with_capacity"],"ChunkIter":["iter_allocated_chunks"],"ChunkRawIter":["iter_allocated_chunks_raw"],"NewChunkMemoryDetails":["clone"],"alloc::AllocErr":["clone"],"alloc::CannotReallocInPlace":["clone"],"bool":["chunk_fits_under_limit","eq","is_empty","is_last_allocation"],"core::alloc::Layout":["layout_from_size_align"],"core::alloc::LayoutError":["new_layout_err"],"core::option::Option":["alloc_layout_slow","allocation_limit","allocation_limit_remaining","new_chunk","next","round_up_to","try_alloc_layout_fast"],"core::ptr::NonNull":["alloc_layout","get"],"core::result::Result":["alloc","alloc_array","alloc_excess","alloc_one","alloc_try_with","alloc_zeroed","array","dealloc_array","fmt","grow","grow_in_place","realloc","realloc_array","realloc_excess","repeat","shrink","shrink_in_place","try_alloc","try_alloc_layout","try_alloc_try_with","try_alloc_with","try_new","try_with_capacity"],"usize":["allocated_bytes","chunk_capacity"]},"struct_to_trait":{"AllocOrInitError":["core::clone::Clone","core::cmp::Eq","core::cmp::PartialEq","core::convert::From","core::fmt::Debug","core::fmt::Display","core::marker::StructuralEq","core::marker::StructuralPartialEq"],"Bump":["core::default::Default","core::fmt::Debug","core::marker::Send","core::ops::Drop"],"ChunkFooter":["core::fmt::Debug"],"ChunkIter":["core::fmt::Debug","core::iter::FusedIterator","core::iter::Iterator"],"ChunkRawIter":["core::fmt::Debug","core::iter::FusedIterator","core::iter::Iterator"],"EmptyChunkFooter":["core::marker::Sync"],"NewChunkMemoryDetails":["core::clone::Clone","core::fmt::Debug","core::marker::Copy"],"alloc::AllocErr":["core::clone::Clone","core::cmp::Eq","core::cmp::PartialEq","core::fmt::Debug","core::fmt::Display","core::marker::StructuralEq","core::marker::StructuralPartialEq"],"alloc::CannotReallocInPlace":["core::clone::Clone","core::cmp::Eq","core::cmp::PartialEq","core::fmt::Debug","core::fmt::Display","core::marker::StructuralEq","core::marker::StructuralPartialEq"],"alloc::Excess":["core::fmt::Debug"],"core::alloc::Layout":["alloc::UnstableLayoutMethods"]},"targets":{"<&'a Bump as alloc::Alloc>::alloc":["alloc","Real(LocalPath(\"src/lib.rs\"))","alloc::Alloc"],"<&'a Bump as alloc::Alloc>::dealloc":["dealloc","Real(LocalPath(\"src/lib.rs\"))","alloc::Alloc"],"<&'a Bump as alloc::Alloc>::realloc":["realloc","Real(LocalPath(\"src/lib.rs\"))","alloc::Alloc"],"<AllocOrInitError<E> as core::convert::From<alloc::AllocErr>>::from":["from","Real(LocalPath(\"src/lib.rs\"))","core::convert::From"],"<AllocOrInitError<E> as core::fmt::Display>::fmt":["fmt","Real(LocalPath(\"src/lib.rs\"))","core::fmt::Display"],"<Bump as core::default::Default>::default":["default","Real(LocalPath(\"src/lib.rs\"))","core::default::Default"],"<Bump as core::ops::Drop>::drop":["drop","Real(LocalPath(\"src/lib.rs\"))","core::ops::Drop"],"<ChunkIter<'a> as core::iter::Iterator>::next":["next","Real(LocalPath(\"src/lib.rs\"))","core::iter::Iterator"],"<ChunkRawIter<'_> as core::iter::Iterator>::next":["next","Real(LocalPath(\"src/lib.rs\"))","core::iter::Iterator"],"<alloc::AllocErr as core::fmt::Display>::fmt":["fmt","Real(LocalPath(\"src/alloc.rs\"))","core::fmt::Display"],"<alloc::CannotReallocInPlace as core::fmt::Display>::fmt":["fmt","Real(LocalPath(\"src/alloc.rs\"))","core::fmt::Display"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::array":["array","Real(LocalPath(\"src/alloc.rs\"))","alloc::UnstableLayoutMethods"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::padding_needed_for":["padding_needed_for","Real(LocalPath(\"src/alloc.rs\"))","alloc::UnstableLayoutMethods"],"<core::alloc::Layout as alloc::UnstableLayoutMethods>::repeat":["repeat","Real(LocalPath(\"src/alloc.rs\"))","alloc::UnstableLayoutMethods"],"Bump::alloc":["alloc","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_layout":["alloc_layout","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_layout_slow":["alloc_layout_slow","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_clone":["alloc_slice_clone","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_copy":["alloc_slice_copy","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_fill_clone":["alloc_slice_fill_clone","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_fill_copy":["alloc_slice_fill_copy","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_fill_default":["alloc_slice_fill_default","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_fill_iter":["alloc_slice_fill_iter","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_slice_fill_with":["alloc_slice_fill_with","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_str":["alloc_str","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_try_with":["alloc_try_with","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_with":["alloc_with","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::alloc_with::inner_writer":["inner_writer","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::allocated_bytes":["allocated_bytes","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::allocation_limit":["allocation_limit","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::allocation_limit_remaining":["allocation_limit_remaining","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::chunk_capacity":["chunk_capacity","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::chunk_fits_under_limit":["chunk_fits_under_limit","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::dealloc":["dealloc","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::grow":["grow","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::is_last_allocation":["is_last_allocation","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::iter_allocated_chunks":["iter_allocated_chunks","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::iter_allocated_chunks_raw":["iter_allocated_chunks_raw","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::new":["new","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::new_chunk":["new_chunk","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::new_chunk_memory_details":["new_chunk_memory_details","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::reset":["reset","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::set_allocation_limit":["set_allocation_limit","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::shrink":["shrink","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc":["try_alloc","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc_layout":["try_alloc_layout","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc_layout_fast":["try_alloc_layout_fast","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc_try_with":["try_alloc_try_with","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc_with":["try_alloc_with","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_alloc_with::inner_writer":["inner_writer","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_new":["try_new","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::try_with_capacity":["try_with_capacity","Real(LocalPath(\"src/lib.rs\"))",""],"Bump::with_capacity":["with_capacity","Real(LocalPath(\"src/lib.rs\"))",""],"ChunkFooter::as_raw_parts":["as_raw_parts","Real(LocalPath(\"src/lib.rs\"))",""],"ChunkFooter::is_empty":["is_empty","Real(LocalPath(\"src/lib.rs\"))",""],"EmptyChunkFooter::get":["get","Real(LocalPath(\"src/lib.rs\"))",""],"abs_diff":["abs_diff","Real(LocalPath(\"src/lib.rs\"))",""],"alloc::Alloc::alloc_array":["alloc_array","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::alloc_excess":["alloc_excess","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::alloc_one":["alloc_one","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::alloc_zeroed":["alloc_zeroed","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::dealloc_array":["dealloc_array","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::dealloc_one":["dealloc_one","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::grow_in_place":["grow_in_place","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::realloc":["realloc","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::realloc_array":["realloc_array","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::realloc_excess":["realloc_excess","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::shrink_in_place":["shrink_in_place","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::Alloc::usable_size":["usable_size","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::CannotReallocInPlace::description":["description","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::handle_alloc_error":["handle_alloc_error","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::new_layout_err":["new_layout_err","Real(LocalPath(\"src/alloc.rs\"))",""],"alloc::size_align":["size_align","Real(LocalPath(\"src/alloc.rs\"))",""],"allocation_size_overflow":["allocation_size_overflow","Real(LocalPath(\"src/lib.rs\"))",""],"dealloc_chunk_list":["dealloc_chunk_list","Real(LocalPath(\"src/lib.rs\"))",""],"layout_from_size_align":["layout_from_size_align","Real(LocalPath(\"src/lib.rs\"))",""],"oom":["oom","Real(LocalPath(\"src/lib.rs\"))",""],"round_down_to":["round_down_to","Real(LocalPath(\"src/lib.rs\"))",""],"round_up_to":["round_up_to","Real(LocalPath(\"src/lib.rs\"))",""]},"trait_to_struct":{"alloc::UnstableLayoutMethods":["core::alloc::Layout"],"core::clone::Clone":["AllocOrInitError","NewChunkMemoryDetails","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::cmp::Eq":["AllocOrInitError","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::cmp::PartialEq":["AllocOrInitError","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::convert::From":["AllocOrInitError"],"core::default::Default":["Bump"],"core::fmt::Debug":["AllocOrInitError","Bump","ChunkFooter","ChunkIter","ChunkRawIter","NewChunkMemoryDetails","alloc::AllocErr","alloc::CannotReallocInPlace","alloc::Excess"],"core::fmt::Display":["AllocOrInitError","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::iter::FusedIterator":["ChunkIter","ChunkRawIter"],"core::iter::Iterator":["ChunkIter","ChunkRawIter"],"core::marker::Copy":["NewChunkMemoryDetails"],"core::marker::Send":["Bump"],"core::marker::StructuralEq":["AllocOrInitError","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::marker::StructuralPartialEq":["AllocOrInitError","alloc::AllocErr","alloc::CannotReallocInPlace"],"core::marker::Sync":["EmptyChunkFooter"],"core::ops::Drop":["Bump"]},"type_to_def_path":{"AllocOrInitError<E>":"AllocOrInitError","Bump":"Bump","ChunkFooter":"ChunkFooter","ChunkIter<'a>":"ChunkIter","ChunkRawIter<'a>":"ChunkRawIter","EmptyChunkFooter":"EmptyChunkFooter","NewChunkMemoryDetails":"NewChunkMemoryDetails","alloc::AllocErr":"alloc::AllocErr","alloc::CannotReallocInPlace":"alloc::CannotReallocInPlace","alloc::Excess":"alloc::Excess"}}